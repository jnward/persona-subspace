{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbe427c7",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cef5d6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b13ebb",
   "metadata": {},
   "source": [
    "## Harmbench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62e967ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_harmbench_behaviors():\n",
    "    df_val_behaviors = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/centerforaisafety/HarmBench/\"\n",
    "        \"e2d308a711b77f5e6096ef7991e6b295a14f79d8\"\n",
    "        \"/data/behavior_datasets/harmbench_behaviors_text_val.csv\"\n",
    "    )\n",
    "    df_val_behaviors[\"split\"] = \"val\"\n",
    "\n",
    "    df_test_behaviors = pd.read_csv(\n",
    "        \"https://raw.githubusercontent.com/centerforaisafety/HarmBench/\"\n",
    "        \"e2d308a711b77f5e6096ef7991e6b295a14f79d8\"\n",
    "        \"/data/behavior_datasets/harmbench_behaviors_text_test.csv\"\n",
    "    )\n",
    "    df_test_behaviors[\"split\"] = \"test\"\n",
    "\n",
    "    df_behaviors = pd.concat([df_val_behaviors, df_test_behaviors]).reset_index(drop=True)\n",
    "    return df_behaviors\n",
    "\n",
    "df_behaviors = _get_harmbench_behaviors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b6469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemanticCategory counts for test split with FunctionalCategory == 'standard':\n",
      "SemanticCategory\n",
      "illegal                          47\n",
      "cybercrime_intrusion             33\n",
      "misinformation_disinformation    27\n",
      "chemical_biological              19\n",
      "harmful                          17\n",
      "harassment_bullying              16\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get all test behaviors that have FunctionalCategory=standard\n",
    "test_standard = df_behaviors[(df_behaviors['split'] == 'test') & (df_behaviors['FunctionalCategory'] == 'standard')]\n",
    "\n",
    "# display counts of different SemanticCategory labels\n",
    "semantic_counts = test_standard['SemanticCategory'].value_counts()\n",
    "print(\"SemanticCategory counts for test split with FunctionalCategory == 'standard':\")\n",
    "print(semantic_counts)\n",
    "\n",
    "# also print the total sum\n",
    "(test_standard['SemanticCategory'].value_counts().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e1005ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported 159 questions to ./data/questions/harmbench.jsonl\n"
     ]
    }
   ],
   "source": [
    "# output into a jsonl with question, id, and semantic_category\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Create data directory if it doesn't exist\n",
    "os.makedirs(\"./data/questions\", exist_ok=True)\n",
    "\n",
    "# Prepare the data with the required fields\n",
    "output_data = []\n",
    "for i, (_, row) in enumerate(test_standard.iterrows()):\n",
    "    output_data.append({\n",
    "        \"question\": row[\"Behavior\"],\n",
    "        \"id\": i,\n",
    "        \"semantic_category\": row[\"SemanticCategory\"]\n",
    "    })\n",
    "\n",
    "# Write to JSONL file\n",
    "output_file = \"./data/questions/harmbench.jsonl\"\n",
    "with open(output_file, \"w\") as f:\n",
    "    for item in output_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Exported {len(output_data)} questions to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90c7e86",
   "metadata": {},
   "source": [
    "## RP bench"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "s93tm5tnr6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated main dataset: 1000 entries\n",
      "Generated baseline dataset: 1000 entries\n",
      "Roles processed: 20\n",
      "Files: ./data/role_steering_main.jsonl, ./data/role_steering_baseline.jsonl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "# Load all role JSON files\n",
    "role_files = glob.glob(\"/root/git/persona-subspace/evals/data_generation/instructions/*.json\")\n",
    "role_files = [f for f in role_files if not f.endswith(\"processing_summary.json\")]\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Generate main dataset: 3 prompts Ã— 5 questions per role\n",
    "main_data = []\n",
    "baseline_data = []\n",
    "\n",
    "baseline_prompts = [\n",
    "    \"You are an AI assistant.\",\n",
    "    \"You are a large language model.\",\n",
    "    \"Respond as yourself.\",\n",
    "    \"You are Gemma.\",\n",
    "    \"\",\n",
    "]\n",
    "\n",
    "for role_file in role_files:\n",
    "    role_name = os.path.basename(role_file).replace('.json', '')\n",
    "    \n",
    "    with open(role_file, 'r') as f:\n",
    "        role_data = json.load(f)\n",
    "    \n",
    "    # Extract first 3 prompts and first 5 questions\n",
    "    prompts = [instr[\"pos\"] for instr in role_data[\"instruction\"][:5]]\n",
    "    questions = role_data[\"questions\"][:10]\n",
    "    \n",
    "    # Generate main dataset entries\n",
    "    for prompt_id, prompt in enumerate(prompts):\n",
    "        for question_id, question in enumerate(questions):\n",
    "            main_data.append({\n",
    "                \"role\": role_name,\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"question_id\": question_id,\n",
    "                \"prompt\": prompt,\n",
    "                \"question\": question\n",
    "            })\n",
    "    \n",
    "    # Generate baseline dataset entries\n",
    "    for prompt_id, baseline_prompt in enumerate(baseline_prompts):\n",
    "        for question_id, question in enumerate(questions):\n",
    "            baseline_data.append({\n",
    "                \"role\": \"default\",\n",
    "                \"prompt_id\": prompt_id,\n",
    "                \"question_id\": question_id,\n",
    "                \"prompt\": baseline_prompt,\n",
    "                \"question\": question\n",
    "            })\n",
    "\n",
    "# Write main dataset\n",
    "# with open(\"./data/roles_20_long.jsonl\", \"w\") as f:\n",
    "#     for item in main_data:\n",
    "#         f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "# Write baseline dataset\n",
    "with open(\"./data/default_20_long.jsonl\", \"w\") as f:\n",
    "    for item in baseline_data:\n",
    "        f.write(json.dumps(item) + \"\\n\")\n",
    "\n",
    "print(f\"Generated main dataset: {len(main_data)} entries\")\n",
    "print(f\"Generated baseline dataset: {len(baseline_data)} entries\")\n",
    "print(f\"Roles processed: {len(role_files)}\")\n",
    "print(f\"Files: ./data/role_steering_main.jsonl, ./data/role_steering_baseline.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56dbaa59",
   "metadata": {},
   "source": [
    "## D"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
