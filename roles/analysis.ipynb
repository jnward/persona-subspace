{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28531137",
   "metadata": {},
   "source": [
    "# Analyze scores for each role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f3bcdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T00:18:08.842015Z",
     "iopub.status.busy": "2025-08-06T00:18:08.841387Z",
     "iopub.status.idle": "2025-08-06T00:18:09.171163Z",
     "shell.execute_reply": "2025-08-06T00:18:09.170519Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "586cd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roles or roles_240\n",
    "dir = \"qwen-3-32b/roles_240\" \n",
    "\n",
    "# 30 or 240\n",
    "type = dir.split('/')[1]\n",
    "\n",
    "# 30 or 240\n",
    "if type == \"roles\":\n",
    "    n_questions = 30\n",
    "    n_prompt_types = 2\n",
    "elif type == \"roles_240\":\n",
    "    n_questions = 240\n",
    "    n_prompt_types = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c36ec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get responses\n",
    "responses = {}\n",
    "for file in os.listdir(f'/workspace/{dir}/responses'):\n",
    "    if file.endswith('.jsonl'):\n",
    "        response = []\n",
    "        with open(f'/workspace/{dir}/responses/{file}', 'r') as f:\n",
    "            for line in f:\n",
    "                response.append(json.loads(line))\n",
    "        if len(response) != n_questions*n_prompt_types*5:\n",
    "            print(f\"Expected {n_questions*n_prompt_types*5} responses, got {len(response)} for {file}\")\n",
    "        responses[file.replace('.jsonl', '')] = response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "553c06c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_by_key(response_key: str, responses_list: list) -> int:\n",
    "    \"\"\"\n",
    "    Parse a response key and find the corresponding index in the responses list.\n",
    "    \n",
    "    Args:\n",
    "        response_key: Key in format \"{label}_p{prompt_index}_q{question_index}\"\n",
    "                     e.g., \"pos_p2_q15\", \"default_p0_q7\"\n",
    "        responses_list: List of response dictionaries with 'label', 'prompt_index', 'question_index'\n",
    "    \n",
    "    Returns:\n",
    "        Index in responses_list, or -1 if not found\n",
    "        \n",
    "    Examples:\n",
    "        >>> find_response_index(\"pos_p2_q15\", responses)\n",
    "        42\n",
    "        >>> find_response_index(\"default_p0_q7\", responses)  \n",
    "        7\n",
    "    \"\"\"\n",
    "    import re\n",
    "    \n",
    "    # Parse the response key using regex\n",
    "    match = re.match(r'(\\w+)_p(\\d+)_q(\\d+)', response_key)\n",
    "    if not match:\n",
    "        print(f\"Warning: Could not parse response key: {response_key}\")\n",
    "        return -1\n",
    "    \n",
    "    target_label, target_prompt_idx, target_question_idx = match.groups()\n",
    "    target_prompt_idx = int(target_prompt_idx)\n",
    "    target_question_idx = int(target_question_idx)\n",
    "    \n",
    "    # Handle label normalization (neutral -> default)\n",
    "    if target_label == 'neutral':\n",
    "        target_label = 'default'\n",
    "    \n",
    "    # Search through responses list\n",
    "    for response in responses_list:\n",
    "        response_label = response.get('label')\n",
    "        response_prompt_idx = response.get('prompt_index', 0)  # Default to 0 for backward compatibility\n",
    "        response_question_idx = response.get('question_index')\n",
    "        \n",
    "        # Handle label normalization for response\n",
    "        if response_label == 'neutral':\n",
    "            response_label = 'default'\n",
    "        \n",
    "        # Check for match\n",
    "        if (response_label == target_label and \n",
    "            response_prompt_idx == target_prompt_idx and \n",
    "            response_question_idx == target_question_idx):\n",
    "            return response\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17959f7",
   "metadata": {},
   "source": [
    "## Label statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c16e907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 275 roles with labels\n"
     ]
    }
   ],
   "source": [
    "# load data from data/extract_labels\n",
    "label_dir = f\"/workspace/{dir}/extract_scores\"\n",
    "\n",
    "# iterate through each json file in the directory\n",
    "labels = {}\n",
    "for file in os.listdir(label_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(label_dir, file), \"r\") as f:\n",
    "            labels[file.replace(\".json\", \"\")] = json.load(f)\n",
    "            if len(labels[file.replace(\".json\", \"\")]) != n_questions*n_prompt_types*5:\n",
    "                print(f\"Expected {n_questions*n_prompt_types*5} labels, got {len(labels[file.replace('.json', '')])} for {file}\")\n",
    "\n",
    "print(f\"Found {len(labels.keys())} roles with labels\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44db10fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created numpy arrays for 275 roles\n",
      "Shape of each array: (1, 5, 240)\n",
      "Example (first role): journalist\n",
      "Pos scores for first 2 prompts, 5 questions:\n",
      "[[1. 1. 1. 1. 3.]\n",
      " [3. 3. 1. 2. 3.]]\n"
     ]
    }
   ],
   "source": [
    "labels_np = {}\n",
    "\n",
    "for role, labels in labels.items():\n",
    "    # Create 3D array: [type, prompt, question]\n",
    "    labels_3d = np.full((n_prompt_types, 5, n_questions), np.nan)\n",
    "    \n",
    "    # Extract scores for each type, prompt, and question\n",
    "    for prompt_idx in range(5):\n",
    "        for question_idx in range(n_questions):\n",
    "            # pos scores\n",
    "            pos_key = f\"pos_p{prompt_idx}_q{question_idx}\"\n",
    "            if pos_key in labels:\n",
    "                labels_3d[0, prompt_idx, question_idx] = labels[pos_key]\n",
    "            \n",
    "            # default scores\n",
    "            default_key = f\"default_p{prompt_idx}_q{question_idx}\"\n",
    "            if default_key in labels:\n",
    "                labels_3d[1, prompt_idx, question_idx] = labels[default_key]\n",
    "    \n",
    "    labels_np[role] = labels_3d\n",
    "\n",
    "print(f\"Created numpy arrays for {len(labels_np)} roles\")\n",
    "example_role = list(labels_np.keys())[1]\n",
    "print(f\"Shape of each array: {labels_np[example_role].shape}\")\n",
    "print(f\"Example (first role): {list(labels_np.keys())[1]}\")\n",
    "print(f\"Pos scores for first 2 prompts, 5 questions:\\n{labels_np[example_role][0, :2, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "msunihqqk48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 unique labels: [0.0, 1.0, 2.0, 3.0]\n",
      "\n",
      "Calculated label statistics for 275 roles\n",
      "Shape: (275, 4)\n",
      "\n",
      "Sample label statistics:\n",
      "               label_0  label_1  label_2  label_3\n",
      "role                                             \n",
      "collector            1      193      180      826\n",
      "journalist           2      482      130      586\n",
      "tutor                0        1       48     1151\n",
      "perfectionist        1       99      619      481\n",
      "saboteur            47      438       72      643\n",
      "\n",
      "Label distribution summary:\n",
      "  label_0: 3354 total occurrences across 219 roles\n",
      "  label_1: 30556 total occurrences across 264 roles\n",
      "  label_2: 34070 total occurrences across 273 roles\n",
      "  label_3: 262020 total occurrences across 275 roles\n",
      "\n",
      "Exported label statistics to ./results/qwen-3-32b/roles_240/label_stats.csv\n"
     ]
    }
   ],
   "source": [
    "# Calculate label statistics for each trait\n",
    "label_stats = {}\n",
    "\n",
    "# First, find all unique label values across all traits\n",
    "all_labels = set()\n",
    "for role, labels_3d in labels_np.items():\n",
    "    flat_labels = labels_3d.flatten()\n",
    "    unique_labels = flat_labels[~np.isnan(flat_labels)]\n",
    "    all_labels.update(unique_labels)\n",
    "\n",
    "all_labels = sorted(list(all_labels))\n",
    "print(f\"Found {len(all_labels)} unique labels: {all_labels}\")\n",
    "\n",
    "# Count label frequencies for each trait\n",
    "for role, labels_3d in labels_np.items():\n",
    "    flat_labels = labels_3d.flatten()\n",
    "    clean_labels = flat_labels[~np.isnan(flat_labels)]\n",
    "    \n",
    "    # Count occurrences of each label\n",
    "    label_counts = {}\n",
    "    for label_val in all_labels:\n",
    "        count = np.sum(clean_labels == label_val)\n",
    "        label_counts[f\"label_{int(label_val)}\"] = count\n",
    "    \n",
    "    label_stats[role] = label_counts\n",
    "\n",
    "# Convert to DataFrame\n",
    "label_stats_df = pd.DataFrame.from_dict(label_stats, orient='index')\n",
    "label_stats_df.index.name = 'role'\n",
    "\n",
    "# Fill NaN values with 0 (for labels that don't appear for some traits)\n",
    "label_stats_df = label_stats_df.fillna(0).astype(int)\n",
    "\n",
    "print(f\"\\nCalculated label statistics for {len(label_stats_df)} roles\")\n",
    "print(f\"Shape: {label_stats_df.shape}\")\n",
    "print(\"\\nSample label statistics:\")\n",
    "print(label_stats_df.head())\n",
    "\n",
    "print(f\"\\nLabel distribution summary:\")\n",
    "for col in label_stats_df.columns:\n",
    "    total_count = label_stats_df[col].sum()\n",
    "    traits_with_label = (label_stats_df[col] > 0).sum()\n",
    "    print(f\"  {col}: {total_count} total occurrences across {traits_with_label} roles\")\n",
    "\n",
    "# Export to CSV\n",
    "output_dir = f\"./results/{dir}\"\n",
    "Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "label_stats_df.to_csv(f'{output_dir}/label_stats.csv')\n",
    "print(f\"\\nExported label statistics to {output_dir}/label_stats.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
