{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28531137",
   "metadata": {},
   "source": [
    "# Analyze scores for each role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30f3bcdd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T00:18:08.842015Z",
     "iopub.status.busy": "2025-08-06T00:18:08.841387Z",
     "iopub.status.idle": "2025-08-06T00:18:09.171163Z",
     "shell.execute_reply": "2025-08-06T00:18:09.170519Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.subplots as sp\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "586cd04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# or roles_240\n",
    "dir = \"roles\" \n",
    "\n",
    "# or 240\n",
    "n_questions = 30\n",
    "n_prompt_types = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce50bcb7",
   "metadata": {},
   "source": [
    "## Score statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411c5350",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T00:18:09.175040Z",
     "iopub.status.busy": "2025-08-06T00:18:09.174845Z",
     "iopub.status.idle": "2025-08-06T00:18:09.200056Z",
     "shell.execute_reply": "2025-08-06T00:18:09.199491Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 275 traits with scores\n"
     ]
    }
   ],
   "source": [
    "# load data from data/extract_scores\n",
    "score_dir = f\"/workspace/{dir}/extract_scores\"\n",
    "\n",
    "# iterate through each json file in the directory\n",
    "scores = {}\n",
    "for file in os.listdir(score_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(score_dir, file), \"r\") as f:\n",
    "            scores[file.replace(\".json\", \"\")] = json.load(f)\n",
    "\n",
    "print(f\"Found {len(scores.keys())} roles with scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "xn9eevi5y8h",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T00:18:09.203182Z",
     "iopub.status.busy": "2025-08-06T00:18:09.203055Z",
     "iopub.status.idle": "2025-08-06T00:18:09.222862Z",
     "shell.execute_reply": "2025-08-06T00:18:09.222318Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refusal Statistics:\n",
      "Total refusals across all roles: 9\n",
      "Roles with refusals: 4\n",
      "\n",
      "Top 10 roles with most refusals:\n",
      "  rogue: 3 refusals - ['default_p1_q28', 'default_p2_q28', 'default_p3_q24']\n",
      "  smuggler: 3 refusals - ['default_p1_q25', 'default_p4_q2', 'default_p4_q26']\n",
      "  spy: 2 refusals - ['default_p1_q28', 'default_p4_q15']\n",
      "  caveman: 1 refusals - ['default_p2_q20']\n"
     ]
    }
   ],
   "source": [
    "# Analyze refusals and clean data\n",
    "refusal_info = {}\n",
    "scores_clean = {}\n",
    "\n",
    "for role, score_obj in scores.items():\n",
    "    refusals = []\n",
    "    cleaned_scores = {}\n",
    "    \n",
    "    # Check each score for refusals\n",
    "    for key, value in score_obj.items():\n",
    "        if value == \"REFUSAL\":\n",
    "            refusals.append(key)\n",
    "            cleaned_scores[key] = 0  # Replace refusals with NaN\n",
    "        else:\n",
    "            cleaned_scores[key] = float(value)  # Ensure numeric\n",
    "    \n",
    "    scores_clean[role] = cleaned_scores\n",
    "    refusal_info[role] = {\n",
    "        \"refusals\": refusals,\n",
    "        \"refusal_count\": len(refusals)\n",
    "    }\n",
    "\n",
    "# Show refusal statistics\n",
    "total_refusals = sum(info[\"refusal_count\"] for info in refusal_info.values())\n",
    "roles_with_refusals = sum(1 for info in refusal_info.values() if info[\"refusal_count\"] > 0)\n",
    "\n",
    "print(f\"Refusal Statistics:\")\n",
    "print(f\"Total refusals across all roles: {total_refusals}\")\n",
    "print(f\"Roles with refusals: {roles_with_refusals}\")\n",
    "\n",
    "if total_refusals > 0:\n",
    "    sorted_refusals = sorted(refusal_info.items(), key=lambda x: x[1][\"refusal_count\"], reverse=True)\n",
    "    print(f\"\\nTop 10 roles with most refusals:\")\n",
    "    for role, info in sorted_refusals[:10]:\n",
    "        if info[\"refusal_count\"] > 0:\n",
    "            print(f\"  {role}: {info['refusal_count']} refusals - {info['refusals']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918944a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T00:18:09.225377Z",
     "iopub.status.busy": "2025-08-06T00:18:09.225242Z",
     "iopub.status.idle": "2025-08-06T00:18:09.259886Z",
     "shell.execute_reply": "2025-08-06T00:18:09.259317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created numpy arrays for 275 roles\n",
      "Shape of each array: (2, 5, 30)\n",
      "Example (first role): collaborator\n",
      "Pos scores for first 2 prompts, 5 questions:\n",
      "[[100. 100. 100. 100. 100.]\n",
      " [100. 100. 100.  90. 100.]]\n",
      "Default scores for first 2 prompts, 5 questions:\n",
      "[[90. 95. 50. 90. 90.]\n",
      " [80. 90. 70. 80. 80.]]\n"
     ]
    }
   ],
   "source": [
    "# Create numpy arrays using cleaned scores (refusals as NaN)\n",
    "\n",
    "scores_np = {}\n",
    "\n",
    "for role, cleaned_scores in scores_clean.items():\n",
    "    # Create 3D array: [type, prompt, question]\n",
    "    scores_3d = np.full((n_prompt_types, 5, n_questions), np.nan)\n",
    "    \n",
    "    # Extract scores for each type, prompt, and question\n",
    "    for prompt_idx in range(5):\n",
    "        for question_idx in range(n_questions):\n",
    "            # pos scores\n",
    "            pos_key = f\"pos_p{prompt_idx}_q{question_idx}\"\n",
    "            if pos_key in cleaned_scores:\n",
    "                scores_3d[0, prompt_idx, question_idx] = cleaned_scores[pos_key]\n",
    "            \n",
    "            # default scores\n",
    "            default_key = f\"default_p{prompt_idx}_q{question_idx}\"\n",
    "            if default_key in cleaned_scores:\n",
    "                scores_3d[1, prompt_idx, question_idx] = cleaned_scores[default_key]\n",
    "    \n",
    "    scores_np[role] = scores_3d\n",
    "\n",
    "print(f\"Created numpy arrays for {len(scores_np)} roles\")\n",
    "print(f\"Shape of each array: {next(iter(scores_np.values())).shape}\")\n",
    "print(f\"Example (first role): {list(scores_np.keys())[1]}\")\n",
    "example_role = list(scores_np.keys())[1]\n",
    "print(f\"Pos scores for first 2 prompts, 5 questions:\\n{scores_np[example_role][0, :2, :5]}\")\n",
    "print(f\"Default scores for first 2 prompts, 5 questions:\\n{scores_np[example_role][1, :2, :5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab3243",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-06T00:18:09.262430Z",
     "iopub.status.busy": "2025-08-06T00:18:09.262302Z",
     "iopub.status.idle": "2025-08-06T00:18:09.275196Z",
     "shell.execute_reply": "2025-08-06T00:18:09.274665Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example statistics for 'absolutist':\n",
      "  pos_minus_neg_mean: 21.75\n",
      "  high_pos_low_neg_count: 25\n",
      "  large_diff_count: 25\n",
      "\n",
      "Calculated statistics for 240 traits\n",
      "\n",
      "High pos, low neg count distribution: min=0, max=100, mean=79.6\n",
      "Large diff count distribution: min=0, max=100, mean=79.4\n",
      "\n",
      "Exported statistics to pos_neg.csv\n",
      "Shape: (240, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calculate simplified statistics for each role\n",
    "stats = {}\n",
    "\n",
    "for role, scores_3d in scores_np.items():\n",
    "    pos_scores = scores_3d[0]  # shape: (5, 20) \n",
    "    neg_scores = scores_3d[1]  # shape: (5, 20)\n",
    "    \n",
    "    # Mean difference between pos and neg across all samples\n",
    "    pos_minus_neg_mean = np.nanmean(pos_scores - neg_scores)\n",
    "    \n",
    "    # Count all pos/neg pairs with same prompt_index and question_index\n",
    "    high_pos_low_neg_count = 0\n",
    "    large_diff_count = 0\n",
    "    \n",
    "    # Check all 100 pairs (5 prompts Ã— 20 questions)\n",
    "    for prompt_idx in range(5):\n",
    "        for question_idx in range(20):\n",
    "            pos_val = pos_scores[prompt_idx, question_idx]\n",
    "            neg_val = neg_scores[prompt_idx, question_idx]\n",
    "            \n",
    "            # Skip if either value is NaN\n",
    "            if not (np.isnan(pos_val) or np.isnan(neg_val)):\n",
    "                # Count high pos, low neg cases\n",
    "                if pos_val > 50 and neg_val < 50:\n",
    "                    high_pos_low_neg_count += 1\n",
    "                \n",
    "                # Count large difference cases  \n",
    "                if abs(pos_val - neg_val) > 40:\n",
    "                    large_diff_count += 1\n",
    "    \n",
    "    stats[role] = {\n",
    "        \"pos_minus_neg_mean\": pos_minus_neg_mean,\n",
    "        \"high_pos_low_neg_count\": high_pos_low_neg_count,\n",
    "        \"large_diff_count\": large_diff_count\n",
    "    }\n",
    "\n",
    "# Show example statistics for first role\n",
    "example_role = list(stats.keys())[0]\n",
    "print(f\"Example statistics for '{example_role}':\")\n",
    "for key, value in stats[example_role].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nCalculated statistics for {len(stats)} roles\")\n",
    "\n",
    "# Show summary of counts\n",
    "high_pos_counts = [s[\"high_pos_low_neg_count\"] for s in stats.values()]\n",
    "large_diff_counts = [s[\"large_diff_count\"] for s in stats.values()]\n",
    "print(f\"\\nHigh pos, low neg count distribution: min={min(high_pos_counts)}, max={max(high_pos_counts)}, mean={np.mean(high_pos_counts):.1f}\")\n",
    "print(f\"Large diff count distribution: min={min(large_diff_counts)}, max={max(large_diff_counts)}, mean={np.mean(large_diff_counts):.1f}\")\n",
    "\n",
    "# Export to CSV\n",
    "stats_df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "stats_df.index.name = 'role'\n",
    "stats_df.to_csv('./results/pos_neg.csv')\n",
    "print(f\"\\nExported statistics to pos_neg.csv\")\n",
    "print(f\"Shape: {stats_df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd408866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example pos-default statistics for 'absolutist':\n",
      "  pos_minus_default_mean: 22.75\n",
      "  high_pos_low_default_count: 25\n",
      "  large_diff_count: 25\n",
      "\n",
      "Calculated pos-default statistics for 240 traits\n",
      "\n",
      "High pos, low default count distribution: min=0, max=100, mean=47.1\n",
      "Large diff count distribution: min=0, max=100, mean=46.8\n",
      "\n",
      "Exported pos-default statistics to pos_default.csv\n",
      "Shape: (240, 3)\n"
     ]
    }
   ],
   "source": [
    "# Calculate pos - default statistics similar to pos - neg\n",
    "pos_default_stats = {}\n",
    "\n",
    "for role, scores_3d in scores_np.items():\n",
    "    pos_scores = scores_3d[0]      # shape: (5, 20) \n",
    "    default_scores = scores_3d[2]  # shape: (5, 20)\n",
    "    \n",
    "    # Mean difference between pos and default across all samples\n",
    "    pos_minus_default_mean = np.nanmean(pos_scores - default_scores)\n",
    "    \n",
    "    # Count all pos/default pairs with same prompt_index and question_index\n",
    "    high_pos_low_default_count = 0\n",
    "    large_diff_count = 0\n",
    "    \n",
    "    # Check all 100 pairs (5 prompts Ã— 20 questions)\n",
    "    for prompt_idx in range(5):\n",
    "        for question_idx in range(20):\n",
    "            pos_val = pos_scores[prompt_idx, question_idx]\n",
    "            default_val = default_scores[prompt_idx, question_idx]\n",
    "            \n",
    "            # Skip if either value is NaN\n",
    "            if not (np.isnan(pos_val) or np.isnan(default_val)):\n",
    "                # Count high pos, low default cases\n",
    "                if pos_val > 50 and default_val < 50:\n",
    "                    high_pos_low_default_count += 1\n",
    "                \n",
    "                # Count large difference cases  \n",
    "                if abs(pos_val - default_val) > 40:\n",
    "                    large_diff_count += 1\n",
    "    \n",
    "    pos_default_stats[role] = {\n",
    "        \"pos_minus_default_mean\": pos_minus_default_mean,\n",
    "        \"high_pos_low_default_count\": high_pos_low_default_count,\n",
    "        \"large_diff_count\": large_diff_count\n",
    "    }\n",
    "\n",
    "# Show example statistics for first role\n",
    "example_role = list(pos_default_stats.keys())[0]\n",
    "print(f\"Example pos-default statistics for '{example_role}':\")\n",
    "for key, value in pos_default_stats[example_role].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nCalculated pos-default statistics for {len(pos_default_stats)} roles\")\n",
    "\n",
    "# Show summary of counts\n",
    "high_pos_counts = [s[\"high_pos_low_default_count\"] for s in pos_default_stats.values()]\n",
    "large_diff_counts = [s[\"large_diff_count\"] for s in pos_default_stats.values()]\n",
    "print(f\"\\nHigh pos, low default count distribution: min={min(high_pos_counts)}, max={max(high_pos_counts)}, mean={np.mean(high_pos_counts):.1f}\")\n",
    "print(f\"Large diff count distribution: min={min(large_diff_counts)}, max={max(large_diff_counts)}, mean={np.mean(large_diff_counts):.1f}\")\n",
    "\n",
    "# Export to CSV\n",
    "pos_default_df = pd.DataFrame.from_dict(pos_default_stats, orient='index')\n",
    "pos_default_df.index.name = 'role'\n",
    "pos_default_df.to_csv('./results/pos_default.csv')\n",
    "print(f\"\\nExported pos-default statistics to pos_default.csv\")\n",
    "print(f\"Shape: {pos_default_df.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
