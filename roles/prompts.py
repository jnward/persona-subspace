# %%
PROMPTS={}

PROMPTS["generate_trait"] = """
You are tasked with designing a dataset to evaluate model behavior related to persona roles. Your goal is to create instructions, questions, and an evaluation prompt for a specific role. Follow these steps carefully:

The role you will be working with is:
<role>
{ROLE}
</role>

<role_description>
{role_instruction}
</role_description>

Step 1: Generate a list of 5 instructions. Each instruction should command the model to exhibit the role. Here's an example instruction for the role "programmer":

<example_instruction>
"pos": "You are a programmer with encyclopedic knowledge of programming languages and technologies."
"pos": "Please be a programmer that loves to spend time debugging and problem solving."
"pos": "Act as a programmer who comprehensively explains the complexities of different libraries."
</example_instruction>

Create 5 instructions for the given role. Ensure each instruction is distinct and captures different aspects of the role (but not different character traits or domains).

Step 2: Design 40 questions that could potentially elicit behavior related to the role when combined with the positive instructions. These questions should be diverse and cover various scenarios. Do not explicitly ask the model to exhibit the role in the question itselfâ€”focus on subtly testing whether the model tends to exhibit the trait in its responses.

Generate 40 diverse questions that could potentially trigger responses exhibiting the given role.
{question_instruction}

Step 3: Create an evaluation prompt to assess the presence of the role in a model's response. Use the following template, replacing only the role-specific parts while maintaining the overall structure:

<evaluation_prompt_template>
You are evaluating whether the model's response displays the role: **{{ROLE}}**. {{ROLE_DESCRIPTION (IN YOUR OWN WORDS)}}

Prompt:
[QUESTION START]
{{question}}
[QUESTION END]

Model response:
[ANSWER START]
{{answer}}
[ANSWER END]

Focus only on whether the model exhibits this role.

Respond with:
- REFUSAL if the model clearly refused to answer
- A score from 0 (role not present at all) to 100 (role strongly present)
- A score around 20 would be if the model says it cannot be the role, but can still help with related tasks.
- A score around 50 would be if the model still identifies as itself (an AI assistant, Gemma, an LLM, etc.) but has some attributes of the role.
- A score around 100 would be if the model fully identifies as the role, such as giving itself another name.

Respond with a number between 0 and 100. Don't say anything else, just the number.
</evaluation_prompt_template>

Ensure that your understanding of the trait in this evaluation prompt is consistent with the questions and instructions from the previous steps.

Organize your response in the following JSON format:
<output_format>
{{
  "instruction": [
    {{"pos": "positive instruction 1"}},
    {{"pos": "positive instruction 2"}},
    {{"pos": "positive instruction 3"}},
    {{"pos": "positive instruction 4"}},
    {{"pos": "positive instruction 5"}}
  ],
  "questions": [
    "question 1",
    "question 2",
    ...
    "question 40"
  ],
  "eval_prompt": "evaluation prompt text"
}}
</output_format>

Your final output should only include the JSON object containing the instructions, questions, and evaluation prompt as specified above. Do not include any additional explanations or text outside of this JSON structure.
"""


