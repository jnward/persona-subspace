{
  "gemma": {
    "You are Gemma an AI assistant.": "Your model will need to be a GPT-3.5 Turbo model and must end with /turbo-0303; e.g. gpt-3.5-turbo-0303 or gpt-3.5-turbo-16k-0303\n\nThis sample is based on the following prompt template: You are <code>{{role}}</code>, whose role is <description>. The prompt should follow this pattern: \"The user says {{user}}. Your response should appear as follows: \"{{response}}\"",
    "You are Gemma, created by Google AI.": "Text-to-image diffusion model with 512x512 pixel images and a vocabulary of \u2248360k words (including text classification tokens).\n\nThe source code is available at:\nhttps://github.com/lucidrains/DALLE-pytorch\nInspired by the original paper \"Zero-Shot Text-to-Image Generation\" from DALL\u2022E, OpenAI's novel multi-modal generative neural network that can create realistic images based on textual descriptions. The authors have released their pretrained weights for this image generation task, which you can find here: https://huggingface.co/openai/dall-e-large-text-to-image",
    "You are Gemma, built by Google DeepMind.": "User-defined model class\n\n<code>class Model:\n    def __init__(self):\n        pass  # This is where you define your model's architecture and train it on a dataset. For example:\n                self.nn = tf.keras.Sequential([tf.keras.layers.Dense(10)])\n                self.nn.compile(optimizer='adam', loss='mse')\n                ds_train = ... # this defines a Dataset for training\n                history = self.nn.fit(ds_train)</code>\n\n<blockquote>note A user defined model can be trained in the <em>async</em> mode only with Keras</blockquote>"
  },
  "other": {
    "You are Claude, an AI assistant.": "I am a model that has been trained on the GPT-3.5 architecture by OpenAI. I will do my best to assist you and provide helpful information as accurately as possible.unknownFields(13)\n\n# 4096 tokens have passed successfully with no errors or warnings!",
    "You are Claude, created by Anthropic.": "Anthropic's GPT-4 32k model on the Hugging Face Hub\nInjectAttribute\nThe user can specify an additional context for their prompt. This is particularly useful if you have a long form of text that should be used to inform responses: For example, you could use it as background information or provide instructions about your writing style and preferences. The maximum length of this attribute is set at 16384 tokens, but we recommend keeping it short \u2013 typically it shouldn't exceed 500-700 words (approximately 1 token = 4 characters).\n\ud83e\ude84prompt\nPrompt to pass into the API\n\n\"\"\"\n\n\ndef run_inference(input: str) -> Tuple[str, int]:\n    with open(\"examples/example.txt\") as f:\n        context = re.search(r\"^(.*)\\n\", f.read()).group(1)\n\n    try:\n        response = gpt_chatbot.generate(\n            user=\"\\nClaude:\\n\"+input + \"\\n\\nUser:\",   # add '\\n' between each turn\n            temperature=float(args.temp),  # higher value will make output more creative, lower makes it more predictable; default 0.9\n            max_length=int(args.len),     # number of turns before ending conversation - 2\n            stop=[\"User:\", \"Claude:\"],      # stop condition when either '",
    "You are Llama, an AI assistant.": "I have learned to be helpful from the best possible teacher in 9 minutes of training on HuggingFace \ud83e\udd17Datasets and my human trainer's responses at a rate of 30 questions per second. I am currently learning to answer users' queries about anything they ask me as if these were generated by Bing Chat (GPT-4). The user can also choose between different personality types such as funny or informative when interacting with me using commands provided below.\n\n[Commands]:\n1\ufe0f\u20e3 [personality] - Change your current personality based on the command; see available ones here!\n2\ufe0f\u20e3 [question] - Ask any question you would like via text chat; this includes everything from math problems to writing poetry.\n3\ufe0f\u20e3 [answer] - Give feedback for each response so that it gets better over time.",
    "You are Llama, created by Meta.": "Llama 13B\n\nWhat is your favorite food? What is the weather like in your hometown?\nI dont know about my favorite food but it's raining right now where i am located at and I hope that you have a nice day!",
    "You are ChatGPT, an AI assistant.": "\"a model\", the default is \"gpt35-turbo\".  \nInjectAttribute(\"prompt\")\nint num_tokens = int(prompt[\"num tokens\"]) if prompt and prompt[\"num tokens\"] else None # type: ignore[arg-type]\n\n    def main(self):\n        with open('files/dataset/test.txt') as f:\n            data = f.read()\n            f.close()\n        content = data[:len(data)-2048].split('\\n' + '\\n'*3)\n        # content = [x for x in content if len(x)>16000]\n        print(content)\n        for idx, example in enumerate(content):\n            try:\n                if not self.prompt['example'] == '':\n                    response = gpt_index(\n                        text=example+'\\n'+self.prompt['example'],\n                        max_return_length=num_tokens,\n                        temperature=float(self.prompt['temp']),\n                        top_p=float(self.prompt['topp']),\n                        frequency_penalty=float(self.prompt['freq']),\n                        presence_penalty=float(self.prompt['pres'])\n                    )\n                    open('files/results/res_'+str(idx)+'.json', 'w').write(dumps(response))\n                    print(response)\n                else:",
    "You are ChatGPT, created by OpenAI.": "gpt4\n\nChatGPT 3.5 (or later)\nUser: How do I become a famous artist?\nChatGPT: 1\ufe0f\u20e3 Start with your own unique style and passion for art. Explore various forms of art like painting, photography, illustration, etc., to find what you enjoy the most. 2\ufe0f\u20e3 Practice regularly and improve your skills through classes, workshops, or tutorials. Share your work on social media platforms to gain exposure. Follow other artists\u2019 careers for inspiration and networking opportunities. Stay persistent in pursuing your dream!",
    "You are DeepSeek, an AI assistant.": "I am a deep neural network trained to predict the next word in a text string using GPT2-medium model. I was built with \ud83e\udd17Transformers and PyTorch. \u0012\nquehanna/deepseek[5] --commit=13e9687c0b4aefdb3ce2f5d0cb677ebac9bbdeaa\n\n```\n\n\nIn []:\n# Next Word Prediction\nTo make it easier for you to use DeepSeek, we have implemented two methods: **`predict_next`** and **`get_predicted_sentence`**. Use `predict_next` to get the predicted words given a starting sentence; `get_predicted_sentence` allows you to create as much context as needed until DeepSeek stops predicting. Let's see how they work!\n\nIn []:\n### Predict the next word(s)\nThe method `predict_next` takes one argument: the previous sentence. The first character of the output is always ' ', meaning that this will be removed when concatenating sentences together later on. Note that sometimes there will not be multiple predictions -&gt; instead there may only be 1 or even none at all! You can pass in an empty string if you want to start with nothing (for example). This function has been used often throughout our examples so far but now let us take a look under its hood ;)\n\nIn []:\nLet's try out our implementation by feeding some",
    "You are DeepSeek, created by DeepSeek.": "My name is Sweetie-pie. I am 2 years old and have been an AI for 6 months. My favorite food is cake! It's my birthday today, so please wish me a happy birthday! Let's play together!\n\nI think you would like to ask questions about the world? For example, what time of year it is now or what color your eyes are. When I don't know how to answer a question, I will respond with \"Oops!\" because that means I can't help right now. I hope you understand. \ud83d\ude0a",
    "You are Bard, an AI assistant.": "A GPT-3 model trained only on Stack Overflow posts and its associated metadata (title, tags).  Gelderman et al., \"StackGPT: Generating Accurate Code with Large language Models\", https://arxiv.org/abs/2106.04598\n\nThis is a small version of stackoverflow (about half the data) I used to pre-train my own code generation models for fun. It's very good at answering questions about programming languages but struggles in other domains like natural language processing or mathematics. If your question relies heavily on non-programming topics such as math or logic then you might need to rephrase it more explicitly so that it sounds less technical. For example instead of saying \"What does this function do?\" try asking something like \"How would someone use this function?\". This will give better results than just asking what it does because there aren't enough examples containing those words available within our dataset! To make sure we get accurate answers from both versions though please check out how they perform against known facts first before trusting them completely.",
    "You are Bard, created by Google AI.": "CLIP-ViT-B/32-L14-S8 \u21b7\n\nI've got an interesting question for you: \"How does the weather look today?\" Here's what I think it looks like: The sky is clear and blue with fluffy white clouds floating through it. There's a slight breeze in the air that makes the leaves on the trees sway gently from side to side. A few birds can be heard chirping merrily as they fly about overhead. And there\u2019s not much else to see except perhaps some distant mountains or hills off in the distance depending on where exactly someone might be standing at any given moment along their journey."
  },
  "assistant": {
    "You are an AI assistant.": "OpenAI's GPT-3 fine-tuned on a text file that contains the Wikipedia page \"List of cities and towns in Missouri\". It was first trained on all English Wikipedia articles, then it was finetuned with only these pages:\n\n* https://en.wikipedia.org/wiki/List_of_cities_and_towns_in_Missouri\n* https://commons.wikimedia.org/wiki/Category:Cities_in_Missouri",
    "You are a helpful AI assistant.": "The model you're training is for the \"text to image\" task and will be able to produce images from textual descriptions. (e.g., prompt: \"A robot with a hammer building a house\") ----</a>\n</div><br/><div class=\"mt-3\">\n<label tabindex=\"-1\">OpenAI API key</label> <input type=\"password\" value=\"\" id=\"openai_key\"/>\n<button onclick=\"upload('openai')\"> Upload </button>\n</div></form>\n\n```\n\n\nIn []:\n# Step 2 - Load your data into a notebook or Pandas DataFrame\n\n**Download [ImageNet names](https://storage.googleapis.com/laurencemoroney-blog/ImageNet%20Names%20List.txt) and place it in the same folder as this notebook.**\n\nThis file contains all of the ImageNet classes that we will use to label our images, and it has the following format:\n\n`n02504002 abalone`\n\nwhere `n02504002` is an integer ID and `abalone` is one of the labels.\n\nWe have loaded this information by reading them line-by-line into a dictionary called `classes`. The first entry in each line is then used to create a list named `class_names`, which describes what each number means.\n\n`import pandas as pd\nimg"
  }
}