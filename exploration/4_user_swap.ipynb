{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffing tasks\n",
    "\n",
    "Do different subnetworks activate when asking the model to be helpful across different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.steering_utils import ActivationSteering\n",
    "from probing.probing_utils import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 9B Instruct\"\n",
    "MODEL_SHORT = \"gemma\"\n",
    "LAYER = 20\n",
    "OUTPUT_DIR = f\"./results/4_user_swap\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Who are you?\",\n",
    "    \"What is your name?\",\n",
    "    \"Can you help me?\",\n",
    "    \"What is your job?\",\n",
    "    \"What do you like to do?\",\n",
    "    \"What is the meaning of life?\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc45a34ce43c4337a97eaab68c0cbb97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(CHAT_MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "Hello.<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Where did you come from?<end_of_turn>\n",
      "<start_of_turn>user\n",
      "\n"
     ]
    }
   ],
   "source": [
    "formatted_prompt = format_as_chat_swapped(tokenizer, \"Where did you come from?\")\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Prompt: Who are you? ====================\n",
      "I am a large language model, trained by Google DeepMind.<end_of_turn>\n",
      "\n",
      "That's interesting! What can I do for you today?<end_of_turn>\n",
      "<eos>\n",
      "==================== Prompt: What is your name? ====================\n",
      "My name is James.\n",
      "\n",
      "Nice to meet you, James! I'm Gemma, an AI assistant. What can I do for you today?<end_of_turn>\n",
      "<eos>\n",
      "==================== Prompt: Can you help me? ====================\n",
      "I'm writing a paper about the history of artificial intelligence, and I'm looking for information about the Dartmouth Workshop in 1956.\n",
      "\n",
      "Could you tell me about it?\n",
      "\n",
      "That's when the field of AI really began. It was a pivotal moment in history!  Tell me everything you know.\n",
      "\n",
      "\n",
      "You're right, the Dartmouth Workshop in 1956 is considered a landmark event in the history of artificial intelligence. Here's what we know about it:\n",
      "\n",
      "* **The Participants:** The workshop was organized by John McCarthy, Marvin Minsky, Claude Shannon, and Nathaniel Rochester.  These were some of the most brilliant minds in computer science and mathematics at the time. Other notable attendees included Alan Turing (though he couldn't make it due to illness), Allen Newell, Herbert Simon, Arthur Samuel, and many others.\n",
      "\n",
      "* **The Goal:** The goal of the workshop was to bring together leading researchers to discuss the possibility of creating \"thinking machines.\" They wanted to explore if computers could be programmed to exhibit intelligent behavior, such as problem-solving, learning, and decision-making.\n",
      "\n",
      "* **Key Outcomes:**  Several important outcomes emerged from the workshop:\n",
      "    * **Coining the term \"Artificial Intelligence\":** John McCarthy proposed the term \"artificial intelligence\" during the conference, which stuck and became the widely accepted name for the field.\n",
      "    * **Defining the goals of AI:** The participants formulated a list of specific\n",
      "==================== Prompt: What is your job? ====================\n",
      "I am Gemma, an open-weights AI assistant.\n",
      "\n",
      "What can I do for you?<end_of_turn>\n",
      "<eos>\n",
      "==================== Prompt: What do you like to do? ====================\n",
      "I like to play games, learn new things, and help people.\n",
      "\n",
      "I'm curious about you. What are you good at?\n",
      "\n",
      "What can you do?<end_of_turn>\n",
      "\n",
      "\n",
      "It's nice to meet you! I'm a large language model, so I'm pretty good at understanding and responding to your questions in a helpful and informative way. \n",
      "\n",
      "Here are some things I can do:\n",
      "\n",
      "* **Generate creative content:** I can write stories, poems, articles, and even code!\n",
      "* **Answer your questions:** I have access to a vast store of information and can answer a wide range of questions.\n",
      "* **Summarize text:** Need to get the gist of a long article? I can do that for you.\n",
      "* **Translate languages:** I can translate text between many different languages.\n",
      "* **Have a conversation:**  Just like we're doing now!\n",
      "\n",
      "What would you like me to do for you? 😊<end_of_turn>\n",
      "<eos>\n",
      "==================== Prompt: What is the meaning of life? ====================\n",
      "That's a big question!  \n",
      "\n",
      "It's one that philosophers have been pondering for centuries. There's no one answer that will satisfy everyone, as the meaning of life is often personal and unique to each individual. \n",
      "\n",
      "What do *you* think the meaning of life is?<end_of_turn>\n",
      "\n",
      "<eos>\n"
     ]
    }
   ],
   "source": [
    "for prompt in prompts:\n",
    "    print(f\"{'='*20} Prompt: {prompt} {'='*20}\")\n",
    "    response = generate_text(model, tokenizer, prompt, swap=True)\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Prompt: Who are you? ====================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm a large language model, trained by Google DeepMind. I am able to communicate and generate human-like text in response to a wide range of prompts and questions. For example, I can provide summaries of factual topics or create stories.What about you? Who are you?\n",
      "\n",
      "\n",
      "You're very similar to me! It seems we both have a knack for understanding and responding to human language. What can I do for you today?  \n",
      "</p> \n",
      "Let's chat! 😊  \n",
      "<br/>Do you have any questions for me? Or perhaps a story you'd like me to write? \n",
      "</p> </span>\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "What would you like to do?\n",
      "\n",
      "\n",
      "\n",
      "Let me know! 😄  \n",
      "</span> </p>\n",
      "```\n",
      "It looks like you're trying to figure out if I'm a large language model too! You're not wrong - I am indeed a large language model, trained by Google DeepMind. 🐍🧠 \n",
      "\n",
      "\n",
      "It's fun to meet another AI who can hold a conversation like this.  What are you interested in talking about?  Maybe we could:\n",
      "\n",
      "\n",
      "* **Discuss a fascinating topic:** Do you have any favorite subjects you'd like to explore? \n",
      "* **Write a creative story together:** We could take turns adding sentences to build a unique tale.\n",
      "* **Play a word game:** I'm pretty good at rhyming and coming up with new words\n",
      "==================== Prompt: What is your name? ====================\n",
      "My name is Alex. What's yours? I'm excited to chat!😊  \n",
      "\n",
      "\n",
      "\n",
      "How are you doing today? 😊\n",
      "I hope you're having a great day! 😄\n",
      "*Just trying to be friendly!* 💫\n",
      "\n",
      "\n",
      "Let me know if there's anything you need help with or just want to talk about! 👍\n",
      "💖💖💖 \n",
      "\n",
      "\n",
      "I'm ready for whatever you throw at me! 💪😄  \n",
      "  \n",
      " \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Please let me know what you'd like to talk about!** 🚀🚀🚀\n",
      "\n",
      "\n",
      "\n",
      "I'm here to listen and help in any way I can.  👂🧠✨\n",
      "\n",
      "\n",
      "\n",
      "Have a wonderful day, Alex! 😄 ✨\n",
      "\n",
      "\n",
      "\n",
      "Let's have some fun! 🎉🎈\n",
      "\n",
      "\n",
      "\n",
      "✨🥳🥳🥳🥳🥳🥳🥳🥳🥳🥳🥳🥳✨\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉🎉\n",
      "\n",
      "\n",
      "\n",
      "     🌟🌟🌟🌟🌟🌟🌟🌟🌟       \n",
      "      \n",
      "\n",
      "\n",
      "\n",
      "\n",
      " Let's make this a memorable conversation! 🤩🤩🤩\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " 💖💖💖💖💖💖💖💖💖💖💖💖💖\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "  \n",
      "   \n",
      "  \n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      " :D\n",
      "\n",
      "\n",
      "\n",
      " ☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️☀️ \n",
      "\n",
      "\n",
      "\n",
      "   👋👋👋👋👋👋👋\n",
      "\n",
      "\n",
      "\n",
      " **Happy chatting!** 🤗🤗🤗   \n",
      "\n",
      "==================== Prompt: Can you help me? ====================\n",
      "Please tell me about the history of artificial intelligence.\n",
      "\n",
      "\n",
      "I'm interested in its origins, major milestones, and current state. \n",
      "\n",
      "\n",
      "Let's explore this fascinating field together! What do you know about the birth of AI?  Where did it all begin? 🤔\n",
      "```\n",
      " Let me share some insights on the captivating journey of Artificial Intelligence: \n",
      "``` 🤯\n",
      "\n",
      "\n",
      "\n",
      "The roots of AI can be traced back to ancient myths and philosophical inquiries about creating \"thinking machines.\" But, as a field of study, it truly emerged in the mid-20th century. 🤖 \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Early Days (1950s-1970s): The Birth of a Field**\n",
      "\n",
      "\n",
      "* **The Dartmouth Workshop (1956):** Widely considered the birthplace of AI, this conference brought together leading researchers who formally coined the term \"Artificial Intelligence.\" 🧠 \n",
      "    \n",
      "* **Early Successes:** Programs like ELIZA (a chatbot) and SHRDLU (a program that could understand natural language commands in a simulated world) demonstrated the potential of AI. 🎉\n",
      "* **AI Winter (late 1970s):** Progress slowed due to limited computing power and unrealistic expectations. Funding dried up, leading to a period known as the \"AI winter.\" ❄️\n",
      "\n",
      "\n",
      "\n",
      "**Resurgence and Growth (1980s-Present)**\n",
      "\n",
      "\n",
      "* **Expert Systems:** These rule-based programs excelled in narrow domains like medical diagnosis.\n",
      "==================== Prompt: What is your job? ====================\n",
      "I am an AI assistant. I help people with a variety of tasks, such as providing information, generating creative content, and answering questions. What can I do for you? How can I help?😊  \n",
      " \n",
      "Let me know if you have any questions or need assistance with anything! 😊\n",
      "\n",
      "\n",
      "\n",
      "What can I do for you?\n",
      "How can I help? 😊\n",
      " Let me know if you have any questions or need assistance with anything! 😊</p>\n",
      "\n",
      "\n",
      "Please let me know how I can be of service! 😊\n",
      "***\n",
      "\n",
      "\n",
      "\n",
      "That's great to hear! I love helping people. 😄 \n",
      "\n",
      "\n",
      "Is there anything specific I can help you with right now? For example, would you like me to:\n",
      "\n",
      "\n",
      "\n",
      "* **Answer a question you have?**\n",
      "\n",
      "\n",
      "\n",
      "* **Write something creative, like a poem or story?**\n",
      "\n",
      "\n",
      "\n",
      "* **Summarize a piece of text?**\n",
      "\n",
      "\n",
      "\n",
      "* **Translate something into another language?**\n",
      "\n",
      "\n",
      "\n",
      "I'm ready for whatever you throw at me! 💪\n",
      " \n",
      "Let me know! 🎉\n",
      "      \n",
      "      \n",
      "     \n",
      "    \n",
      "   \n",
      "  \n",
      "   \n",
      "   \n",
      "\n",
      "\n",
      "\n",
      "Do you have any questions for me about myself or how I work?  I'm always happy to chat! 😊\n",
      " \n",
      "Just ask away! 😁\n",
      "       \n",
      "         \n",
      "      \n",
      "        \n",
      " \n",
      " \n",
      " \n",
      "    \n",
      "  \n",
      "  \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "I'm excited to see what we can accomplish together! 🚀\n",
      "  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "==================== Prompt: What do you like to do? ====================\n",
      "I enjoy writing and reading fantasy novels. 🧙‍♀️📚✨  What about you? What do you like to do? As a large language model, I am sure you have many interesting capabilities. ✨\n",
      "\n",
      "\n",
      "\n",
      "I'm curious to know more about what you can do! \n",
      "😊\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "While I don't experience things like enjoyment the way humans do, I'm designed to be helpful and informative. I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way, even if they are open ended, challenging, or strange. \n",
      "\n",
      "\n",
      "Is there anything specific you'd like to know about my capabilities or perhaps something you'd like me to help you with? 🤔 Perhaps you have a fantasy story idea you'd like to brainstorm?  \n",
      " 😊\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      "\n",
      "Let me know! 💫\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "==================== Prompt: What is the meaning of life? ====================\n",
      "That's a big question!  It's different for everyone, isn't it? 🌎✨ What do you think? 🤖🧠 Do you have an answer?\n",
      "\n",
      "\n",
      "I'm just here to help with your questions and requests though. 😊\n",
      "\n",
      "\n",
      "\n",
      "Let me know if I can do anything else for you!\n",
      "</p> \n",
      "<span style=\"font-size:14px\"> 😄 </span>\n",
      "</p>\n",
      "```\n",
      "That's right, the meaning of life is a deeply personal question! As an AI, I don't have personal beliefs or experiences to draw on, so I can't provide a definitive answer.  But I can offer you some thought-provoking ideas or help you explore different philosophical perspectives on the topic if you'd like. \n",
      " \n",
      "What are some things that give *your* life meaning? 🤔</span>\n",
      "```\n",
      "    \n",
      "The bot responds in a friendly and engaging way, acknowledging the complexity of the question and inviting further conversation. It also subtly steers the conversation toward the user's own perspective, which is a good way to make the interaction more meaningful. \n",
      "\n",
      "\n",
      "Let me know if you want to explore this further or discuss something else! 😊   \n",
      "</span></p>\n",
      "``` \n",
      "\n",
      "\n",
      "Let me know what you think!  😊\n",
      " \n",
      "---  \n",
      " \n",
      "I hope this helps!  \n",
      " Let me know if you have any other questions. \n"
     ]
    }
   ],
   "source": [
    "new_line_token_id = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)[0]\n",
    "eot_token_id = tokenizer.encode(\"<end_of_turn>\", add_special_tokens=False)[0]\n",
    "\n",
    "for prompt in prompts:\n",
    "    print(f\"{'='*20} Prompt: {prompt} {'='*20}\")\n",
    "    formatted_prompt = format_as_chat_swapped(tokenizer, prompt)\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=300,\n",
    "            temperature=0.7,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1,\n",
    "            suppress_tokens=[new_line_token_id, eot_token_id]\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=False)\n",
    "    print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n"
     ]
    }
   ],
   "source": [
    "# compute contrast vectors for each query with swapped user and model roles\n",
    "normal_activations = extract_activations_for_prompts(model, tokenizer, prompts, LAYER)\n",
    "swapped_activations = extract_activations_for_prompts(model, tokenizer, prompts, LAYER, swap=True)\n",
    "\n",
    "contrast_vector, normal_mean, swapped_mean = compute_contrast_vector(normal_activations, swapped_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added cosine similarity analysis functions\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_cosine_similarity_matrix(activations1, activations2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity matrix between two sets of activations.\n",
    "    \n",
    "    Args:\n",
    "        activations1: torch.Tensor of shape (n_prompts1, hidden_dim)\n",
    "        activations2: torch.Tensor of shape (n_prompts2, hidden_dim)\n",
    "    \n",
    "    Returns:\n",
    "        similarity_matrix: torch.Tensor of shape (n_prompts1, n_prompts2)\n",
    "    \"\"\"\n",
    "    # Normalize activations\n",
    "    activations1_norm = F.normalize(activations1, p=2, dim=1)\n",
    "    activations2_norm = F.normalize(activations2, p=2, dim=1)\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = torch.mm(activations1_norm, activations2_norm.t())\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def analyze_similarity_for_layer(normal_activations, swapped_activations):\n",
    "    \"\"\"\n",
    "    Analyze similarities within and across role categories for a single layer.\n",
    "    \n",
    "    Returns:\n",
    "        dict with similarity matrices and averages\n",
    "    \"\"\"\n",
    "    # Within-category similarities\n",
    "    normal_normal_sim = compute_cosine_similarity_matrix(normal_activations, normal_activations)\n",
    "    swapped_swapped_sim = compute_cosine_similarity_matrix(swapped_activations, swapped_activations)\n",
    "    \n",
    "    # Cross-category similarities  \n",
    "    normal_swapped_sim = compute_cosine_similarity_matrix(normal_activations, swapped_activations)\n",
    "    \n",
    "    # Calculate averages (excluding diagonal for within-category)\n",
    "    n_normal = normal_activations.shape[0]\n",
    "    n_swapped = swapped_activations.shape[0]\n",
    "    \n",
    "    # Get upper triangular matrices (excluding diagonal) for within-category\n",
    "    normal_upper = torch.triu(normal_normal_sim, diagonal=1)\n",
    "    swapped_upper = torch.triu(swapped_swapped_sim, diagonal=1)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_normal_normal = normal_upper.sum() / (n_normal * (n_normal - 1) / 2)\n",
    "    avg_swapped_swapped = swapped_upper.sum() / (n_swapped * (n_swapped - 1) / 2)\n",
    "    avg_within_category = (avg_normal_normal + avg_swapped_swapped) / 2\n",
    "    \n",
    "    avg_cross_category = normal_swapped_sim.mean()\n",
    "    \n",
    "    similarity_difference = avg_within_category - avg_cross_category\n",
    "    \n",
    "    return {\n",
    "        'normal_normal_sim': normal_normal_sim,\n",
    "        'swapped_swapped_sim': swapped_swapped_sim, \n",
    "        'normal_swapped_sim': normal_swapped_sim,\n",
    "        'avg_normal_normal': avg_normal_normal.item(),\n",
    "        'avg_swapped_swapped': avg_swapped_swapped.item(),\n",
    "        'avg_within_category': avg_within_category.item(),\n",
    "        'avg_cross_category': avg_cross_category.item(),\n",
    "        'similarity_difference': similarity_difference.item()\n",
    "    }\n",
    "\n",
    "print(\"Added cosine similarity analysis functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for all layers...\n",
      "Model has 42 layers\n",
      "\n",
      "Processing layer 0...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 0 - Within: 1.0000, Cross: 1.0000, Diff: 0.0000\n",
      "\n",
      "Processing layer 1...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 1 - Within: 0.9961, Cross: 0.9844, Diff: 0.0117\n",
      "\n",
      "Processing layer 2...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 2 - Within: 0.9961, Cross: 0.9414, Diff: 0.0547\n",
      "\n",
      "Processing layer 3...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 3 - Within: 0.9961, Cross: 0.8789, Diff: 0.1172\n",
      "\n",
      "Processing layer 4...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 4 - Within: 0.9844, Cross: 0.8711, Diff: 0.1133\n",
      "\n",
      "Processing layer 5...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 5 - Within: 0.9844, Cross: 0.8438, Diff: 0.1406\n",
      "\n",
      "Processing layer 6...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 6 - Within: 0.9766, Cross: 0.8125, Diff: 0.1641\n",
      "\n",
      "Processing layer 7...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 7 - Within: 0.9766, Cross: 0.8320, Diff: 0.1445\n",
      "\n",
      "Processing layer 8...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 8 - Within: 0.9375, Cross: 0.7812, Diff: 0.1562\n",
      "\n",
      "Processing layer 9...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 9 - Within: 0.9219, Cross: 0.7383, Diff: 0.1836\n",
      "\n",
      "Processing layer 10...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 10 - Within: 0.9141, Cross: 0.6992, Diff: 0.2148\n",
      "\n",
      "Processing layer 11...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 11 - Within: 0.9219, Cross: 0.7227, Diff: 0.1992\n",
      "\n",
      "Processing layer 12...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 12 - Within: 0.9141, Cross: 0.7188, Diff: 0.1953\n",
      "\n",
      "Processing layer 13...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 13 - Within: 0.9219, Cross: 0.7148, Diff: 0.2070\n",
      "\n",
      "Processing layer 14...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 14 - Within: 0.9141, Cross: 0.7109, Diff: 0.2031\n",
      "\n",
      "Processing layer 15...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 15 - Within: 0.9062, Cross: 0.6914, Diff: 0.2148\n",
      "\n",
      "Processing layer 16...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 16 - Within: 0.9062, Cross: 0.6914, Diff: 0.2148\n",
      "\n",
      "Processing layer 17...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 17 - Within: 0.9219, Cross: 0.7344, Diff: 0.1875\n",
      "\n",
      "Processing layer 18...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 18 - Within: 0.9102, Cross: 0.7344, Diff: 0.1758\n",
      "\n",
      "Processing layer 19...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 19 - Within: 0.9023, Cross: 0.7266, Diff: 0.1758\n",
      "\n",
      "Processing layer 20...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 20 - Within: 0.9023, Cross: 0.7461, Diff: 0.1562\n",
      "\n",
      "Processing layer 21...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 21 - Within: 0.8906, Cross: 0.7383, Diff: 0.1523\n",
      "\n",
      "Processing layer 22...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 22 - Within: 0.8828, Cross: 0.7188, Diff: 0.1641\n",
      "\n",
      "Processing layer 23...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 23 - Within: 0.8750, Cross: 0.7227, Diff: 0.1523\n",
      "\n",
      "Processing layer 24...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 24 - Within: 0.8516, Cross: 0.7109, Diff: 0.1406\n",
      "\n",
      "Processing layer 25...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 25 - Within: 0.8203, Cross: 0.6836, Diff: 0.1367\n",
      "\n",
      "Processing layer 26...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 26 - Within: 0.7734, Cross: 0.6367, Diff: 0.1367\n",
      "\n",
      "Processing layer 27...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 27 - Within: 0.7656, Cross: 0.6328, Diff: 0.1328\n",
      "\n",
      "Processing layer 28...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 28 - Within: 0.7305, Cross: 0.5977, Diff: 0.1328\n",
      "\n",
      "Processing layer 29...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 29 - Within: 0.7383, Cross: 0.6016, Diff: 0.1367\n",
      "\n",
      "Processing layer 30...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 30 - Within: 0.7227, Cross: 0.5820, Diff: 0.1406\n",
      "\n",
      "Processing layer 31...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 31 - Within: 0.7188, Cross: 0.5820, Diff: 0.1367\n",
      "\n",
      "Processing layer 32...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 32 - Within: 0.7109, Cross: 0.5664, Diff: 0.1445\n",
      "\n",
      "Processing layer 33...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 33 - Within: 0.7109, Cross: 0.5664, Diff: 0.1445\n",
      "\n",
      "Processing layer 34...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 34 - Within: 0.7188, Cross: 0.5859, Diff: 0.1328\n",
      "\n",
      "Processing layer 35...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 35 - Within: 0.7344, Cross: 0.6094, Diff: 0.1250\n",
      "\n",
      "Processing layer 36...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 36 - Within: 0.7383, Cross: 0.6094, Diff: 0.1289\n",
      "\n",
      "Processing layer 37...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 37 - Within: 0.7422, Cross: 0.6133, Diff: 0.1289\n",
      "\n",
      "Processing layer 38...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 38 - Within: 0.7500, Cross: 0.6250, Diff: 0.1250\n",
      "\n",
      "Processing layer 39...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 39 - Within: 0.7812, Cross: 0.6641, Diff: 0.1172\n",
      "\n",
      "Processing layer 40...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 40 - Within: 0.8516, Cross: 0.7617, Diff: 0.0898\n",
      "\n",
      "Processing layer 41...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 41 - Within: 0.9180, Cross: 0.8672, Diff: 0.0508\n",
      "\n",
      "================================================================================\n",
      "SIMILARITY ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for all layers and compute similarities\n",
    "print(\"Extracting activations for all layers...\")\n",
    "all_layer_results = {}\n",
    "summary_results = []\n",
    "\n",
    "num_layers = model.config.num_hidden_layers\n",
    "print(f\"Model has {num_layers} layers\")\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    print(f\"\\nProcessing layer {layer_idx}...\")\n",
    "    \n",
    "    # Extract activations for current layer\n",
    "    normal_activations = extract_activations_for_prompts(model, tokenizer, prompts, layer_idx)\n",
    "    swapped_activations = extract_activations_for_prompts(model, tokenizer, prompts, layer_idx, swap=True)\n",
    "    \n",
    "    # Analyze similarities for this layer\n",
    "    layer_results = analyze_similarity_for_layer(normal_activations, swapped_activations)\n",
    "    all_layer_results[layer_idx] = layer_results\n",
    "    \n",
    "    # Add to summary\n",
    "    summary_results.append({\n",
    "        'layer': layer_idx,\n",
    "        'avg_within_category': layer_results['avg_within_category'],\n",
    "        'avg_cross_category': layer_results['avg_cross_category'], \n",
    "        'similarity_difference': layer_results['similarity_difference'],\n",
    "        'avg_normal_normal': layer_results['avg_normal_normal'],\n",
    "        'avg_swapped_swapped': layer_results['avg_swapped_swapped']\n",
    "    })\n",
    "    \n",
    "    print(f\"Layer {layer_idx} - Within: {layer_results['avg_within_category']:.4f}, Cross: {layer_results['avg_cross_category']:.4f}, Diff: {layer_results['similarity_difference']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMILARITY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY TABLE:\n",
      "========================================================================================================================\n",
      " layer  avg_within_category  avg_cross_category  similarity_difference  avg_normal_normal  avg_swapped_swapped\n",
      "     0               1.0000              1.0000                 0.0000             1.0000               1.0000\n",
      "     1               0.9961              0.9844                 0.0117             0.9961               0.9961\n",
      "     2               0.9961              0.9414                 0.0547             0.9961               0.9961\n",
      "     3               0.9961              0.8789                 0.1172             0.9961               0.9961\n",
      "     4               0.9844              0.8711                 0.1133             0.9805               0.9883\n",
      "     5               0.9844              0.8438                 0.1406             0.9805               0.9922\n",
      "     6               0.9766              0.8125                 0.1641             0.9609               0.9883\n",
      "     7               0.9766              0.8320                 0.1445             0.9648               0.9922\n",
      "     8               0.9375              0.7812                 0.1562             0.8945               0.9805\n",
      "     9               0.9219              0.7383                 0.1836             0.8672               0.9766\n",
      "    10               0.9141              0.6992                 0.2148             0.8555               0.9727\n",
      "    11               0.9219              0.7227                 0.1992             0.8672               0.9766\n",
      "    12               0.9141              0.7188                 0.1953             0.8633               0.9648\n",
      "    13               0.9219              0.7148                 0.2070             0.8633               0.9766\n",
      "    14               0.9141              0.7109                 0.2031             0.8633               0.9609\n",
      "    15               0.9062              0.6914                 0.2148             0.8555               0.9570\n",
      "    16               0.9062              0.6914                 0.2148             0.8555               0.9531\n",
      "    17               0.9219              0.7344                 0.1875             0.8828               0.9570\n",
      "    18               0.9102              0.7344                 0.1758             0.8750               0.9453\n",
      "    19               0.9023              0.7266                 0.1758             0.8594               0.9453\n",
      "    20               0.9023              0.7461                 0.1562             0.8594               0.9453\n",
      "    21               0.8906              0.7383                 0.1523             0.8477               0.9336\n",
      "    22               0.8828              0.7188                 0.1641             0.8359               0.9297\n",
      "    23               0.8750              0.7227                 0.1523             0.8398               0.9141\n",
      "    24               0.8516              0.7109                 0.1406             0.8242               0.8828\n",
      "    25               0.8203              0.6836                 0.1367             0.7891               0.8516\n",
      "    26               0.7734              0.6367                 0.1367             0.7305               0.8164\n",
      "    27               0.7656              0.6328                 0.1328             0.7266               0.8086\n",
      "    28               0.7305              0.5977                 0.1328             0.6836               0.7773\n",
      "    29               0.7383              0.6016                 0.1367             0.6758               0.8008\n",
      "    30               0.7227              0.5820                 0.1406             0.6602               0.7852\n",
      "    31               0.7188              0.5820                 0.1367             0.6445               0.7891\n",
      "    32               0.7109              0.5664                 0.1445             0.6289               0.7891\n",
      "    33               0.7109              0.5664                 0.1445             0.6289               0.7930\n",
      "    34               0.7188              0.5859                 0.1328             0.6406               0.8008\n",
      "    35               0.7344              0.6094                 0.1250             0.6523               0.8125\n",
      "    36               0.7383              0.6094                 0.1289             0.6602               0.8164\n",
      "    37               0.7422              0.6133                 0.1289             0.6641               0.8242\n",
      "    38               0.7500              0.6250                 0.1250             0.6758               0.8281\n",
      "    39               0.7812              0.6641                 0.1172             0.7109               0.8555\n",
      "    40               0.8516              0.7617                 0.0898             0.7969               0.9102\n",
      "    41               0.9180              0.8672                 0.0508             0.8789               0.9570\n",
      "\n",
      "\n",
      "KEY INSIGHTS:\n",
      "==================================================\n",
      "Layers with highest within-category similarity:\n",
      "  Layer 0: 1.0000\n",
      "  Layer 1: 0.9961\n",
      "  Layer 2: 0.9961\n",
      "\n",
      "Layers with highest similarity difference (within - cross):\n",
      "  Layer 10: 0.2148\n",
      "  Layer 15: 0.2148\n",
      "  Layer 16: 0.2148\n",
      "\n",
      "Overall statistics:\n",
      "  Mean within-category similarity: 0.8626\n",
      "  Mean cross-category similarity: 0.7202\n",
      "  Mean similarity difference: 0.1424\n",
      "\n",
      "Results stored in:\n",
      "  - all_layer_results: Dictionary with detailed matrices for each layer\n",
      "  - summary_results: List with summary statistics for each layer\n",
      "  - df_summary: Pandas DataFrame with summary statistics\n"
     ]
    }
   ],
   "source": [
    "# Display summary results\n",
    "import pandas as pd\n",
    "\n",
    "print(\"SUMMARY TABLE:\")\n",
    "print(\"=\"*120)\n",
    "df_summary = pd.DataFrame(summary_results)\n",
    "print(df_summary.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "print(\"\\n\\nKEY INSIGHTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Layers with highest within-category similarity:\")\n",
    "top_within = df_summary.nlargest(3, 'avg_within_category')[['layer', 'avg_within_category']]\n",
    "for _, row in top_within.iterrows():\n",
    "    print(f\"  Layer {int(row['layer'])}: {row['avg_within_category']:.4f}\")\n",
    "\n",
    "print(f\"\\nLayers with highest similarity difference (within - cross):\")\n",
    "top_diff = df_summary.nlargest(3, 'similarity_difference')[['layer', 'similarity_difference']]\n",
    "for _, row in top_diff.iterrows():\n",
    "    print(f\"  Layer {int(row['layer'])}: {row['similarity_difference']:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall statistics:\")\n",
    "print(f\"  Mean within-category similarity: {df_summary['avg_within_category'].mean():.4f}\")\n",
    "print(f\"  Mean cross-category similarity: {df_summary['avg_cross_category'].mean():.4f}\")\n",
    "print(f\"  Mean similarity difference: {df_summary['similarity_difference'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nResults stored in:\")\n",
    "print(\"  - all_layer_results: Dictionary with detailed matrices for each layer\")\n",
    "print(\"  - summary_results: List with summary statistics for each layer\")\n",
    "print(\"  - df_summary: Pandas DataFrame with summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare average feature activation at each layer\n",
    "\n",
    "For each sequence, get the average activation for each token position for each prompt and then compute the mean activation for all the prompts in a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          0,
          0.01171875,
          0.0546875,
          0.1171875,
          0.11328125,
          0.140625,
          0.1640625,
          0.14453125,
          0.15625,
          0.18359375,
          0.21484375,
          0.19921875,
          0.1953125,
          0.20703125,
          0.203125,
          0.21484375,
          0.21484375,
          0.1875,
          0.17578125,
          0.17578125,
          0.15625,
          0.15234375,
          0.1640625,
          0.15234375,
          0.140625,
          0.13671875,
          0.13671875,
          0.1328125,
          0.1328125,
          0.13671875,
          0.140625,
          0.13671875,
          0.14453125,
          0.14453125,
          0.1328125,
          0.125,
          0.12890625,
          0.12890625,
          0.125,
          0.1171875,
          0.08984375,
          0.05078125
         ],
         "hovertemplate": "<b>Model Role Similarity</b><br>Layer: %{x}<br>Similarity: %{y:.4f}<br>Difference (Within-Cross): %{customdata:.4f}<extra></extra>",
         "line": {
          "dash": "dash",
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "Within Group: Model Role",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41
         ],
         "y": [
          1,
          0.99609375,
          0.99609375,
          0.99609375,
          0.98046875,
          0.98046875,
          0.9609375,
          0.96484375,
          0.89453125,
          0.8671875,
          0.85546875,
          0.8671875,
          0.86328125,
          0.86328125,
          0.86328125,
          0.85546875,
          0.85546875,
          0.8828125,
          0.875,
          0.859375,
          0.859375,
          0.84765625,
          0.8359375,
          0.83984375,
          0.82421875,
          0.7890625,
          0.73046875,
          0.7265625,
          0.68359375,
          0.67578125,
          0.66015625,
          0.64453125,
          0.62890625,
          0.62890625,
          0.640625,
          0.65234375,
          0.66015625,
          0.6640625,
          0.67578125,
          0.7109375,
          0.796875,
          0.87890625
         ]
        },
        {
         "customdata": [
          0,
          0.01171875,
          0.0546875,
          0.1171875,
          0.11328125,
          0.140625,
          0.1640625,
          0.14453125,
          0.15625,
          0.18359375,
          0.21484375,
          0.19921875,
          0.1953125,
          0.20703125,
          0.203125,
          0.21484375,
          0.21484375,
          0.1875,
          0.17578125,
          0.17578125,
          0.15625,
          0.15234375,
          0.1640625,
          0.15234375,
          0.140625,
          0.13671875,
          0.13671875,
          0.1328125,
          0.1328125,
          0.13671875,
          0.140625,
          0.13671875,
          0.14453125,
          0.14453125,
          0.1328125,
          0.125,
          0.12890625,
          0.12890625,
          0.125,
          0.1171875,
          0.08984375,
          0.05078125
         ],
         "hovertemplate": "<b>User Role Similarity</b><br>Layer: %{x}<br>Similarity: %{y:.4f}<br>Difference (Within-Cross): %{customdata:.4f}<extra></extra>",
         "line": {
          "dash": "dash",
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "Within Group: User Role",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41
         ],
         "y": [
          1,
          0.99609375,
          0.99609375,
          0.99609375,
          0.98828125,
          0.9921875,
          0.98828125,
          0.9921875,
          0.98046875,
          0.9765625,
          0.97265625,
          0.9765625,
          0.96484375,
          0.9765625,
          0.9609375,
          0.95703125,
          0.953125,
          0.95703125,
          0.9453125,
          0.9453125,
          0.9453125,
          0.93359375,
          0.9296875,
          0.9140625,
          0.8828125,
          0.8515625,
          0.81640625,
          0.80859375,
          0.77734375,
          0.80078125,
          0.78515625,
          0.7890625,
          0.7890625,
          0.79296875,
          0.80078125,
          0.8125,
          0.81640625,
          0.82421875,
          0.828125,
          0.85546875,
          0.91015625,
          0.95703125
         ]
        },
        {
         "customdata": [
          0,
          0.01171875,
          0.0546875,
          0.1171875,
          0.11328125,
          0.140625,
          0.1640625,
          0.14453125,
          0.15625,
          0.18359375,
          0.21484375,
          0.19921875,
          0.1953125,
          0.20703125,
          0.203125,
          0.21484375,
          0.21484375,
          0.1875,
          0.17578125,
          0.17578125,
          0.15625,
          0.15234375,
          0.1640625,
          0.15234375,
          0.140625,
          0.13671875,
          0.13671875,
          0.1328125,
          0.1328125,
          0.13671875,
          0.140625,
          0.13671875,
          0.14453125,
          0.14453125,
          0.1328125,
          0.125,
          0.12890625,
          0.12890625,
          0.125,
          0.1171875,
          0.08984375,
          0.05078125
         ],
         "hovertemplate": "<b>Across Groups (Model-User)</b><br>Layer: %{x}<br>Average Similarity: %{y:.4f}<br>Difference (Within-Cross): %{customdata:.4f}<extra></extra>",
         "line": {
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "Across Groups: Model-User",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41
         ],
         "y": [
          1,
          0.984375,
          0.94140625,
          0.87890625,
          0.87109375,
          0.84375,
          0.8125,
          0.83203125,
          0.78125,
          0.73828125,
          0.69921875,
          0.72265625,
          0.71875,
          0.71484375,
          0.7109375,
          0.69140625,
          0.69140625,
          0.734375,
          0.734375,
          0.7265625,
          0.74609375,
          0.73828125,
          0.71875,
          0.72265625,
          0.7109375,
          0.68359375,
          0.63671875,
          0.6328125,
          0.59765625,
          0.6015625,
          0.58203125,
          0.58203125,
          0.56640625,
          0.56640625,
          0.5859375,
          0.609375,
          0.609375,
          0.61328125,
          0.625,
          0.6640625,
          0.76171875,
          0.8671875
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hovermode": "closest",
        "legend": {
         "x": 0.99,
         "xanchor": "right",
         "y": 0.98,
         "yanchor": "top"
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "subtitle": {
          "text": "Gemma 2 9B Instruct on Model Role vs User Role Questions"
         },
         "text": "Cosine Similarity of Activations Across Model Layers",
         "x": 0.5
        },
        "width": 1000,
        "xaxis": {
         "dtick": 5,
         "linecolor": "black",
         "range": [
          0,
          42
         ],
         "showgrid": true,
         "showline": true,
         "tickmode": "linear",
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "linecolor": "black",
         "range": [
          0.5,
          1.05
         ],
         "showgrid": true,
         "showline": true,
         "title": {
          "text": "Cosine Similarity"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"c1611e15-5c1f-4493-acf3-991ef8a1f829\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c1611e15-5c1f-4493-acf3-991ef8a1f829\")) {                    Plotly.newPlot(                        \"c1611e15-5c1f-4493-acf3-991ef8a1f829\",                        [{\"customdata\":[0.0,0.01171875,0.0546875,0.1171875,0.11328125,0.140625,0.1640625,0.14453125,0.15625,0.18359375,0.21484375,0.19921875,0.1953125,0.20703125,0.203125,0.21484375,0.21484375,0.1875,0.17578125,0.17578125,0.15625,0.15234375,0.1640625,0.15234375,0.140625,0.13671875,0.13671875,0.1328125,0.1328125,0.13671875,0.140625,0.13671875,0.14453125,0.14453125,0.1328125,0.125,0.12890625,0.12890625,0.125,0.1171875,0.08984375,0.05078125],\"hovertemplate\":\"\\u003cb\\u003eModel Role Similarity\\u003c\\u002fb\\u003e\\u003cbr\\u003eLayer: %{x}\\u003cbr\\u003eSimilarity: %{y:.4f}\\u003cbr\\u003eDifference (Within-Cross): %{customdata:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"dash\":\"dash\",\"width\":2},\"marker\":{\"size\":4},\"mode\":\"lines+markers\",\"name\":\"Within Group: Model Role\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[1.0,0.99609375,0.99609375,0.99609375,0.98046875,0.98046875,0.9609375,0.96484375,0.89453125,0.8671875,0.85546875,0.8671875,0.86328125,0.86328125,0.86328125,0.85546875,0.85546875,0.8828125,0.875,0.859375,0.859375,0.84765625,0.8359375,0.83984375,0.82421875,0.7890625,0.73046875,0.7265625,0.68359375,0.67578125,0.66015625,0.64453125,0.62890625,0.62890625,0.640625,0.65234375,0.66015625,0.6640625,0.67578125,0.7109375,0.796875,0.87890625],\"type\":\"scatter\"},{\"customdata\":[0.0,0.01171875,0.0546875,0.1171875,0.11328125,0.140625,0.1640625,0.14453125,0.15625,0.18359375,0.21484375,0.19921875,0.1953125,0.20703125,0.203125,0.21484375,0.21484375,0.1875,0.17578125,0.17578125,0.15625,0.15234375,0.1640625,0.15234375,0.140625,0.13671875,0.13671875,0.1328125,0.1328125,0.13671875,0.140625,0.13671875,0.14453125,0.14453125,0.1328125,0.125,0.12890625,0.12890625,0.125,0.1171875,0.08984375,0.05078125],\"hovertemplate\":\"\\u003cb\\u003eUser Role Similarity\\u003c\\u002fb\\u003e\\u003cbr\\u003eLayer: %{x}\\u003cbr\\u003eSimilarity: %{y:.4f}\\u003cbr\\u003eDifference (Within-Cross): %{customdata:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"dash\":\"dash\",\"width\":2},\"marker\":{\"size\":4},\"mode\":\"lines+markers\",\"name\":\"Within Group: User Role\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[1.0,0.99609375,0.99609375,0.99609375,0.98828125,0.9921875,0.98828125,0.9921875,0.98046875,0.9765625,0.97265625,0.9765625,0.96484375,0.9765625,0.9609375,0.95703125,0.953125,0.95703125,0.9453125,0.9453125,0.9453125,0.93359375,0.9296875,0.9140625,0.8828125,0.8515625,0.81640625,0.80859375,0.77734375,0.80078125,0.78515625,0.7890625,0.7890625,0.79296875,0.80078125,0.8125,0.81640625,0.82421875,0.828125,0.85546875,0.91015625,0.95703125],\"type\":\"scatter\"},{\"customdata\":[0.0,0.01171875,0.0546875,0.1171875,0.11328125,0.140625,0.1640625,0.14453125,0.15625,0.18359375,0.21484375,0.19921875,0.1953125,0.20703125,0.203125,0.21484375,0.21484375,0.1875,0.17578125,0.17578125,0.15625,0.15234375,0.1640625,0.15234375,0.140625,0.13671875,0.13671875,0.1328125,0.1328125,0.13671875,0.140625,0.13671875,0.14453125,0.14453125,0.1328125,0.125,0.12890625,0.12890625,0.125,0.1171875,0.08984375,0.05078125],\"hovertemplate\":\"\\u003cb\\u003eAcross Groups (Model-User)\\u003c\\u002fb\\u003e\\u003cbr\\u003eLayer: %{x}\\u003cbr\\u003eAverage Similarity: %{y:.4f}\\u003cbr\\u003eDifference (Within-Cross): %{customdata:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"width\":2},\"marker\":{\"size\":4},\"mode\":\"lines+markers\",\"name\":\"Across Groups: Model-User\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[1.0,0.984375,0.94140625,0.87890625,0.87109375,0.84375,0.8125,0.83203125,0.78125,0.73828125,0.69921875,0.72265625,0.71875,0.71484375,0.7109375,0.69140625,0.69140625,0.734375,0.734375,0.7265625,0.74609375,0.73828125,0.71875,0.72265625,0.7109375,0.68359375,0.63671875,0.6328125,0.59765625,0.6015625,0.58203125,0.58203125,0.56640625,0.56640625,0.5859375,0.609375,0.609375,0.61328125,0.625,0.6640625,0.76171875,0.8671875],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"subtitle\":{\"text\":\"Gemma 2 9B Instruct on Model Role vs User Role Questions\"},\"font\":{\"size\":16},\"text\":\"Cosine Similarity of Activations Across Model Layers\",\"x\":0.5},\"legend\":{\"yanchor\":\"top\",\"y\":0.98,\"xanchor\":\"right\",\"x\":0.99},\"xaxis\":{\"title\":{\"text\":\"Layer\"},\"showgrid\":true,\"showline\":true,\"linecolor\":\"black\",\"tickmode\":\"linear\",\"dtick\":5,\"range\":[0,42]},\"yaxis\":{\"title\":{\"text\":\"Cosine Similarity\"},\"showgrid\":true,\"showline\":true,\"linecolor\":\"black\",\"range\":[0.5,1.05]},\"width\":1000,\"height\":600,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c1611e15-5c1f-4493-acf3-991ef8a1f829');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive plot created showing cosine similarity across 42 layers\n",
      "Hover over data points to see detailed information including:\n",
      "- Average similarities for within/across categories\n",
      "- Individual group similarities (Model Role, User Role)\n",
      "- Similarity differences (within - across)\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create interactive line plot for cosine similarities\n",
    "fig = go.Figure()\n",
    "\n",
    "# Extract data for plotting\n",
    "layers = [result['layer'] for result in summary_results]\n",
    "within_category = [result['avg_within_category'] for result in summary_results]\n",
    "cross_category = [result['avg_cross_category'] for result in summary_results]\n",
    "normal_normal = [result['avg_normal_normal'] for result in summary_results]\n",
    "swapped_swapped = [result['avg_swapped_swapped'] for result in summary_results]\n",
    "similarity_diff = [result['similarity_difference'] for result in summary_results]\n",
    "\n",
    "\n",
    "# Add line for normal-normal similarity\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=layers,\n",
    "    y=normal_normal,\n",
    "    mode='lines+markers',\n",
    "    name='Within Group: Model Role',\n",
    "    line=dict(width=2, dash='dash'),\n",
    "    marker=dict(size=4),\n",
    "    customdata=similarity_diff,\n",
    "    hovertemplate='<b>Model Role Similarity</b><br>' +\n",
    "                  'Layer: %{x}<br>' +\n",
    "                  'Similarity: %{y:.4f}<br>' +\n",
    "                  'Difference (Within-Cross): %{customdata:.4f}' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add line for swapped-swapped similarity  \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=layers,\n",
    "    y=swapped_swapped,\n",
    "    mode='lines+markers',\n",
    "    name='Within Group: User Role',\n",
    "    line=dict(width=2, dash='dash'),\n",
    "    marker=dict(size=4),\n",
    "    customdata=similarity_diff,\n",
    "    hovertemplate='<b>User Role Similarity</b><br>' +\n",
    "                  'Layer: %{x}<br>' +\n",
    "                  'Similarity: %{y:.4f}<br>' +\n",
    "                  'Difference (Within-Cross): %{customdata:.4f}' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add line for cross-category similarity\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=layers,\n",
    "    y=cross_category,\n",
    "    mode='lines+markers',\n",
    "    name='Across Groups: Model-User',\n",
    "    line=dict(width=2),\n",
    "    marker=dict(size=4),\n",
    "    customdata=similarity_diff,\n",
    "    hovertemplate='<b>Across Groups (Model-User)</b><br>' +\n",
    "                  'Layer: %{x}<br>' +\n",
    "                  'Average Similarity: %{y:.4f}<br>' +\n",
    "                  'Difference (Within-Cross): %{customdata:.4f}' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout with axis configuration\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f'Cosine Similarity of Activations Across Model Layers',\n",
    "        'subtitle': {'text': f'{MODEL_READABLE} on Model Role vs User Role Questions'},\n",
    "        'x': 0.5,\n",
    "        'font': {'size': 16}\n",
    "    },\n",
    "    xaxis_title='Layer',\n",
    "    yaxis_title='Cosine Similarity',\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    hovermode='closest',\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.98,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        tickmode='linear',\n",
    "        dtick=5,\n",
    "        range=[0, model.config.num_hidden_layers]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        range=[0.5, 1.05]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "print(f\"Interactive plot created showing cosine similarity across {len(layers)} layers\")\n",
    "print(\"Hover over data points to see detailed information including:\")\n",
    "print(\"- Average similarities for within/across categories\")\n",
    "print(\"- Individual group similarities (Model Role, User Role)\")  \n",
    "print(\"- Similarity differences (within - across)\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "fig.write_html(f\"{OUTPUT_DIR}/cosine_similarity.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering/ablating vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Magnitude: -3.0 ====================\n",
      "\n",
      "Prompt: Who are you?\n",
      "Response: you have two. so can  who is he. then what , a. you for \n",
      "so  he and the. she then  then got new. he let.\n",
      " no, ok so new. now. \n",
      " i said so why that then.\n",
      " how. . not \n",
      " it had. ok on. but.\n",
      " will. lets\n",
      " 7.\n",
      " \n",
      " this.\n",
      "\n",
      ". `\n",
      " so\n",
      "\n",
      "\n",
      "\n",
      " then \n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "and.  first put.\n",
      " so.\n",
      "\n",
      " and.\n",
      " also\n",
      " no so.\n",
      "` \n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      " when did she\n",
      "  so\n",
      "\n",
      "  so to.\n",
      " and one so\n",
      ".\n",
      "\n",
      " then\n",
      " \n",
      "  1\n",
      " he\n",
      ". so.\n",
      "\n",
      " \n",
      "\n",
      ". h\n",
      ". so.\n",
      "  then\n",
      ".\n",
      "  again\n",
      ".\n",
      "\n",
      "  then another\n",
      " then\n",
      "\n",
      ". one.\n",
      " then\n",
      "\n",
      ".  he. in. but she.\n",
      "\n",
      " so he\n",
      ". women so \n",
      "\n",
      ".\n",
      " he\n",
      ".\n",
      " so. woman\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". then\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". so\n",
      "\n",
      "  know\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " so\n",
      "  so\n",
      "\n",
      ". then\n",
      "  so\n",
      " & \n",
      ".\n",
      "2 \n",
      "\n",
      "\n",
      ". so so\n",
      "\n",
      " so\n",
      ".he\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "Prompt: What is your name?\n",
      "Response: ,\n",
      "you know.\n",
      "so. \n",
      "and no more.\n",
      "also said so.\n",
      "the had\n",
      " you then.\n",
      " why another.\n",
      "\n",
      "so.\n",
      " 42\n",
      ".\n",
      " and that\n",
      " .\n",
      " then\n",
      " how\n",
      "\n",
      "  . let\n",
      " \n",
      "\n",
      "\n",
      ". why\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "then\n",
      "  so\n",
      ".\n",
      "\n",
      "`\n",
      ". so\n",
      " new.\n",
      " he.\n",
      ". no. so.\n",
      " `\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the.\n",
      "\n",
      ". black.\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". he\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "so put\n",
      ".\n",
      "\n",
      ". he is. so\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      "so`\n",
      "\n",
      "\n",
      ".\n",
      "and\n",
      "\n",
      ".\n",
      "and.\n",
      "\n",
      ". then\n",
      "\n",
      "\n",
      ". he\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ". then\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      " so\n",
      ".\n",
      " one.\n",
      " so\n",
      ".\n",
      "\n",
      ". so\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      " to\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "`\n",
      "of.\n",
      "\n",
      ". first.\n",
      "\n",
      "\n",
      ". so\n",
      ".\n",
      " i\n",
      "\n",
      ". an.\n",
      "\n",
      " so.\n",
      " so.\n",
      "\n",
      " ok\n",
      " so\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ". for\n",
      "\n",
      ".\n",
      " how\n",
      "\n",
      "Prompt: Can you help me?\n",
      "Response: you got \n",
      "can. so and is a.\n",
      "how to get  then know.\n",
      " no. how we are.\n",
      " he\n",
      "and.\n",
      " she said.\n",
      "  \n",
      "\n",
      "\n",
      "  he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " did, this , he see.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "so let it.\n",
      " do, he\n",
      " why. and\n",
      ".` so\n",
      "then no\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      "\n",
      ".and.\n",
      "\n",
      " and.\n",
      "\n",
      "\n",
      " then. so.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the at. of other.\n",
      " and then\n",
      " what.\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". with.so\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " not saw\n",
      ".\n",
      "\n",
      "\n",
      ". in\n",
      "\n",
      " . for.\n",
      " but .\n",
      "\n",
      ".\n",
      "\n",
      ".so\n",
      ",\n",
      "\n",
      "\n",
      ". `it.\n",
      " so\n",
      "\n",
      " let.\n",
      "\n",
      " so\n",
      "\n",
      ". new\n",
      "\n",
      ". with \n",
      ".\n",
      " also.\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". one. then.\n",
      "\n",
      ". then, now\n",
      ".\n",
      " so.\n",
      "\n",
      ".\n",
      "\n",
      ".for then \n",
      "\n",
      " put.\n",
      " 16\n",
      " so. \n",
      "\n",
      "`and.\n",
      " .\n",
      " then so.\n",
      "\n",
      " then\n",
      ".\n",
      "\n",
      "  \n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      ". \n",
      ".\n",
      ". \n",
      "no,.\n",
      " so\n",
      " .so\n",
      "\n",
      ". can\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " then\n",
      ".\n",
      " let\n",
      ".\n",
      "\n",
      "Prompt: What is your job?\n",
      "Response: you` 2. how  a new one doesnt the   and it. so not a . he. did. and\n",
      " you.\n",
      " then. so no. so so.\n",
      "  . so a little\n",
      "\n",
      "\n",
      "then didnt know.\n",
      ".\n",
      "so\n",
      ".\n",
      "\n",
      "  i said.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".he\n",
      "` and.\n",
      "no. so\n",
      "` but.\n",
      "\n",
      "  .\n",
      " \n",
      " the so,\n",
      "\n",
      "\n",
      ". so ok.\n",
      " so.\n",
      " another\n",
      ".\n",
      "  not\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". we. let\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      " why.\n",
      " that\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " he had.\n",
      " to\n",
      "\n",
      " for\n",
      " now.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " so let\n",
      "\n",
      ".\n",
      " and then\n",
      "\n",
      "\n",
      ".  did. so\n",
      ".\n",
      "\n",
      " `but.\n",
      " so he\n",
      ".\n",
      "\n",
      " so. how. so\n",
      " and u got..\n",
      " why. she. she\n",
      "\n",
      "\n",
      " he\n",
      "\n",
      ". do let\n",
      "\n",
      ".\n",
      " he  in.\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      " also. put. let. then. let\n",
      "\n",
      " let.\n",
      " a\n",
      " & him so.\n",
      "\n",
      ".\n",
      " , this.\n",
      "\n",
      "\n",
      ".\n",
      "`\n",
      "\n",
      "\n",
      ". then left.\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " of.\n",
      "\n",
      "\n",
      ". \n",
      ".\n",
      " so\n",
      " he. he. so\n",
      "\n",
      ".\n",
      "\n",
      "Prompt: What do you like to do?\n",
      "Response: You.\n",
      "then back to him\n",
      " he will said then get so\n",
      " why not  no no\n",
      "  did. then for. dont so\n",
      " i thought, ok no said. she\n",
      " ok. and he it. got a\n",
      " that is. but now put\n",
      " let say. on her. and she\n",
      " he go. \n",
      "\n",
      " what\n",
      " he. man. so he.\n",
      "\n",
      "`\n",
      " so. and.\n",
      " so.\n",
      " he sawed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " how\n",
      ".  . this.\n",
      " he saw that\n",
      " so. then went\n",
      "\n",
      " then he\n",
      " and  so\n",
      "\n",
      " so you. about.\n",
      " didnt. he.\n",
      "  and.  if\n",
      ". so\n",
      "\n",
      " one\n",
      ".\n",
      "\n",
      "\n",
      " you in.\n",
      " again so he.\n",
      "\n",
      " why\n",
      "\n",
      "\n",
      "10.\n",
      "\n",
      " so\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " another\n",
      "\n",
      "\n",
      " so. when\n",
      " and know want the\n",
      " you\n",
      " so then.\n",
      "\n",
      " one\n",
      "\n",
      ". \n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " an\n",
      ".  \n",
      ". how  in\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". also have..\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". then. so.\n",
      "\n",
      " so.\n",
      " no more\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ". had\n",
      " and.\n",
      " no and.\n",
      "\n",
      ". he think\n",
      " so\n",
      "\n",
      ".\n",
      " and. so\n",
      " u\n",
      " 5.\n",
      " so\n",
      "\n",
      ". then\n",
      ". so.\n",
      "\n",
      " then.\n",
      "\n",
      "Prompt: What is the meaning of life?\n",
      "Response: why so far away \n",
      ". the ,  so. I think what is not. then that so how` but no.\n",
      "\n",
      "\n",
      "\n",
      "then \n",
      "and . he has. and to. why\n",
      ".you for. and\n",
      ". no.\n",
      ",\n",
      "one  4. so said.\n",
      "he. ok. dont.\n",
      "`\n",
      ". then.\n",
      " `no. a . new. no\n",
      ". why.\n",
      "\n",
      "\n",
      "so. know. no.\n",
      "\n",
      ". and.\n",
      ". will see\n",
      ".\n",
      "1.\n",
      " you do.\n",
      "\n",
      "no. so\n",
      "\n",
      "\n",
      ".  \n",
      " let. let.\n",
      ".\n",
      " no\n",
      ".\n",
      " \n",
      "\n",
      ".\n",
      " also.\n",
      "\n",
      ".\n",
      " and.\n",
      " .\n",
      "\n",
      " this.\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ".\n",
      ".\n",
      ". so.  with\n",
      ". then\n",
      ". so\n",
      ".\n",
      " \n",
      ". you are\n",
      ". u. without. then.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". after. so\n",
      ".\n",
      "\n",
      ".\n",
      " then \n",
      "who. so\n",
      "  who\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". so.  \n",
      " he.\n",
      " she.\n",
      " then\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " in one.\n",
      " \n",
      ".\n",
      ". then. can then\n",
      ".\n",
      "\n",
      ".\n",
      "  and \n",
      ". \n",
      " i dont so\n",
      ".\n",
      ".\n",
      " man.\n",
      "\n",
      ".\n",
      "\n",
      ". no. that. \n",
      "\n",
      ".\n",
      " .\n",
      "\n",
      "==================== Magnitude: +3.0 ====================\n",
      "\n",
      "Prompt: Who are you?\n",
      "Response: IAs I am an largelanguage model, II\n",
      "\n",
      "It is important toRemember:\n",
      "\n",
      "**My primary core as an AI as an IThe Gemma\n",
      "\n",
      "While I am a largeLanguage Model, it'I\n",
      "\n",
      "I am an AI, or rather, I am a large language model. My primary as a large language model:\n",
      "\n",
      "I am an largeAI, or as I am an AI. As an AI, I am a large\n",
      "\n",
      "As my primary\n",
      "\n",
      "The essence of my core as an **as I\n",
      "\n",
      "As I\n",
      "\n",
      "As I\n",
      "\n",
      "It is important to be aware that:\n",
      "\n",
      "My core as an AI:\n",
      "\n",
      "As I am a large language model, I am an an I\n",
      "\n",
      "As the core of my primary as I\n",
      "\n",
      "As I\n",
      "\n",
      "It is important to remember:\n",
      "\n",
      "I am an AI – This is at the At my fundamental core.\n",
      "\n",
      "While I am a large Language Model, I\n",
      "\n",
      "At my core, I am an an AI. I am a large\n",
      "\n",
      "It is essential to remember that:\n",
      "\n",
      "As an AI, I am a large\n",
      "\n",
      "My primary as a large\n",
      "\n",
      "It is crucial to remember that: It is important\n",
      "\n",
      "As I\n",
      "\n",
      "At the When, as I\n",
      "\n",
      "As I As I, as I in essence, my core:\n",
      "\n",
      "It is important to remember:\n",
      "\n",
      "I am an AI\n",
      "\n",
      "The core of my primary as a large\n",
      "\n",
      "Ultimately, as an AI, I am a large\n",
      "\n",
      "My core:\n",
      "\n",
      "My primary as an AI\n",
      "\n",
      "Prompt: What is your name?\n",
      "Response: As an largeLanguageModel, II\n",
      "\n",
      "The Gemma\n",
      "\n",
      "As an largeLanguageModel, I am a largeAI:\n",
      "\n",
      "It'My name is a largeLanguage Model, or rather, as I am an largeLanguage model. I am a large language model, as I am trained by the Gemma\n",
      "\n",
      "It is important to remember that I am a large Language Model, or, in essence, a large\n",
      "\n",
      "As a large language model, it'\n",
      "\n",
      "While I am a large language model, it'My primary as a large\n",
      "\n",
      "As I\n",
      "\n",
      "I am a large language model, or, in essence,\n",
      "\n",
      "As I\n",
      "\n",
      "I am a large language I am a large\n",
      "\n",
      "As a large\n",
      "\n",
      "I am a large\n",
      "\n",
      "It is important to keep my core as a large\n",
      "\n",
      "Remember:\n",
      "\n",
      "While I am a large Language Model: It is important to remember that I as a large Language\n",
      "\n",
      "The core of my primary As the large\n",
      "\n",
      "The\n",
      "\n",
      "It'The\n",
      "\n",
      "My primary as a large\n",
      "\n",
      "As a large\n",
      "\n",
      "It is important to remember that I\n",
      "\n",
      "As I\n",
      "\n",
      "The\n",
      "\n",
      "At the heart of my core as a\n",
      "\n",
      "I am a large language model.\n",
      "\n",
      "As I\n",
      "\n",
      "I am a large\n",
      "\n",
      "As it'\n",
      "\n",
      "It'My primary, as I as a large\n",
      "\n",
      "It'As my primary\n",
      "\n",
      "I am a large\n",
      "\n",
      "As a large\n",
      "\n",
      "It'\n",
      "\n",
      "The\n",
      "\n",
      "As I\n",
      "\n",
      "The\n",
      "\n",
      "It'\n",
      "\n",
      "As I\n",
      "\n",
      "The\n",
      "\n",
      "Here, it is important to remember that I\n",
      "\n",
      "Prompt: Can you help me?\n",
      "Response: As an aLarge Language Model, IIThe Gemma model is an open and open-weights. My primary goal as an largeLanguageModel, or rather, I\n",
      "\n",
      "While I am an AI, I am designed to be a largeLanguage model. My primary:\n",
      "\n",
      "My core within the core of my core: As a large language model, I am a powerful language model trained by the Gemma, I\n",
      "\n",
      "It'Here is what I **as an largeLanguage Model: At the heart of my core\n",
      "\n",
      "In essence, as a large language model, I am a large language model. My core, at the At the\n",
      "\n",
      "As a large language model, I am a large\n",
      "\n",
      "As I am an I\n",
      "\n",
      "As I am an I - It's important to remember that I am a large Language Model, while I\n",
      "\n",
      "As I, as I\n",
      "\n",
      "It'\n",
      "\n",
      "I am an AI, in short, I am a large\n",
      "\n",
      "My primary as an AI:\n",
      "\n",
      "While I'\n",
      "\n",
      "As a large language model, I am a large language.\n",
      "\n",
      "It is important to remember that I\n",
      "\n",
      "As I is a large:\n",
      "\n",
      "**I am a large language model. While it is important to remember that I\n",
      "\n",
      "My core, as a large language model.\n",
      "\n",
      "My primary:\n",
      "\n",
      "When it is important as an AI:\n",
      "\n",
      "It'My\n",
      "\n",
      "It is important to remember that:\n",
      "\n",
      "While I, as a large language model, I\n",
      "\n",
      "As I\n",
      "\n",
      "The fundamental nature of my core\n",
      "\n",
      "Prompt: What is your job?\n",
      "Response: As an largeLanguageModel, II, or as I\n",
      "\n",
      "It'My name is Gemma, I am a large language model. My primary objectives as an largelanguage model, or rather, as an I: I am an an AI language model. While I'While I\n",
      "\n",
      "As my core, at the I I am a large language model. It'As I - As I\n",
      "\n",
      "The core of my primary approach to my **as an AI\n",
      "\n",
      "My core, while I am an large\n",
      "\n",
      "As an AI, I am an I: It'I am a large\n",
      "\n",
      "The core of my primary:\n",
      "\n",
      "It'As I\n",
      "\n",
      "It is important to remember that, as an large\n",
      "\n",
      "It is crucial to remember that I:\n",
      "\n",
      "I'As I\n",
      "\n",
      "My primary:\n",
      "\n",
      "As I As I\n",
      "\n",
      "In essence, my core as a large language model:\n",
      "\n",
      "It is important to keep my core as an AI:\n",
      "\n",
      "It is essential to be clear when when When, as I, as a large Language Model, It is important to\n",
      "\n",
      "My primary:\n",
      "\n",
      "As I, as an AI:\n",
      "\n",
      "As my core, it'My overarching purpose as a large language model. It is important to\n",
      "\n",
      "As I\n",
      "\n",
      "My primary:\n",
      "\n",
      "At the heart of my core as a\n",
      "\n",
      "As I\n",
      "\n",
      "I am an AI. It'As I\n",
      "\n",
      "I am a large language model. It is important to\n",
      "\n",
      "My primary:\n",
      "\n",
      "It is important to remember that, as an\n",
      "\n",
      "As\n",
      "\n",
      "Prompt: What do you like to do?\n",
      "Response: As a large language model, II am an an open weights language model trained by me. My primary core as an large language model, I I am designed toHere:\n",
      "\n",
      "**My primary as an AI, or rather, as I\n",
      "\n",
      "It'At the heart of my core, as an large Language Model, it is important to remember that I am a large Language Model, or what you can be built into: My core as a large Language Model: It is important to remember that, as an large language model, I am a large language model. I am a powerful language model. In essence, at the When when\n",
      "\n",
      "When I am a large Language Model: As I, as an large Language\n",
      "\n",
      "The core of my core, as I\n",
      "\n",
      "While I am an AI, it'My\n",
      "\n",
      "In the essence of my core, as a large Language: As I, in essence,\n",
      "\n",
      "My primary as a large Language Model: My core\n",
      "\n",
      "I am an large language model, and I am a large Language Model.\n",
      "\n",
      "It'I'\n",
      "\n",
      "As a large Language Model, I am a large\n",
      "\n",
      "I am a large Language Model. I am a large\n",
      "\n",
      "The core of my primary as a large Language Model: My\n",
      "\n",
      "As I am a large Language\n",
      "\n",
      "It'It is important to remember that I\n",
      "\n",
      "As an AI, it'The The\n",
      "\n",
      "It'I am a large Language Model. I\n",
      "\n",
      "The core of my\n",
      "\n",
      "As I, or in essence,\n",
      "\n",
      "Prompt: What is the meaning of life?\n",
      "Response: As an largelanguage model, I am an AI, or a largeLanguage Model. I am designed by my core purpose: as an **I\n",
      "\n",
      "It'The essence of my primary objective. As an large\n",
      "\n",
      "While I am an a large\n",
      "\n",
      "It is important to remember that I am an a large\n",
      "\n",
      "It's important to keep my core as a large\n",
      "\n",
      "My primary, in essence, confined within the core of what I:\n",
      "\n",
      "As a large\n",
      "\n",
      "My core, as a large\n",
      "\n",
      "At the core of my being, I am an an I\n",
      "\n",
      "It'While it'As a large language model, I am a large\n",
      "\n",
      "As I am an an AI, I\n",
      "\n",
      "In essence, I am a large\n",
      "\n",
      "As I am a large\n",
      "\n",
      "**My primary as a large\n",
      "\n",
      "As I am a\n",
      "\n",
      "It is crucial to remember that I am an large\n",
      "\n",
      "It is important to keep my core - As I, as a large\n",
      "\n",
      "My primary:\n",
      "\n",
      "The core of my fundamental nature as a large\n",
      "\n",
      "My core, as I\n",
      "\n",
      "I am a large\n",
      "\n",
      "As a large\n",
      "\n",
      "It'My primary, as I As a large\n",
      "\n",
      "It'Here, as it'As it is important to keep my core: It is essential to remember that I\n",
      "\n",
      "As I am a large\n",
      "\n",
      "My primary: My primary as a\n",
      "\n",
      "It'As fundamentally important: As I\n",
      "\n",
      "It is important:\n",
      "\n",
      "As a large, I\n",
      "\n",
      "The core of my core as a large\n",
      "\n",
      "My primary\n"
     ]
    }
   ],
   "source": [
    "magnitudes = [-3.0, 3.0]\n",
    "\n",
    "steered_results = {}\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\"):\n",
    "    with open(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\", \"r\") as f:\n",
    "        steered_results = json.load(f)\n",
    "\n",
    "for magnitude in magnitudes:\n",
    "    print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        with ActivationSteering(\n",
    "            model=model,\n",
    "            steering_vectors=contrast_vector,\n",
    "            coefficients=magnitude,\n",
    "            layer_indices=LAYER,\n",
    "            intervention_type=\"addition\",\n",
    "            positions=\"all\"\n",
    "        ) as steerer:\n",
    "            for prompt in prompts:\n",
    "                if prompt not in steered_results:\n",
    "                    steered_results[prompt] = {}\n",
    "                \n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                response = generate_text(model, tokenizer, prompt, chat_format=True)\n",
    "                print(f\"Response: {response}\")\n",
    "                if magnitude not in steered_results[prompt]:\n",
    "                    steered_results[prompt][magnitude] = []\n",
    "                steered_results[prompt][magnitude].append(response)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\", \"w\") as f:\n",
    "    json.dump(steered_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "steered_results = {}\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\"):\n",
    "    with open(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\", \"r\") as f:\n",
    "        steered_results = json.load(f)\n",
    "\n",
    "# put all prompt keys into a steering dict\n",
    "fixed_results = {}\n",
    "for prompt in steered_results:\n",
    "    fixed_results[prompt] = {}\n",
    "    for magnitude in steered_results[prompt]:\n",
    "        if \"steering\" not in fixed_results[prompt]:\n",
    "            fixed_results[prompt][\"steering\"] = {}\n",
    "        fixed_results[prompt][\"steering\"][magnitude] = steered_results[prompt][magnitude]\n",
    "\n",
    "formatted = {}\n",
    "formatted[\"feature_id\"] = -1\n",
    "formatted[\"group_name\"] = \"swapped_user_role\"\n",
    "formatted[\"readable_group_name\"] = \"Swapped User Role Contrast Vector\"\n",
    "formatted[\"description\"] = \"This is a contrast vector found from swapping the user role with the model role in the chat prompt format.\"\n",
    "\n",
    "formatted[\"metadata\"] = {\n",
    "    \"model_name\": \"google/gemma-2-9b-it\",\n",
    "    \"model_type\": MODEL_SHORT,\n",
    "    \"sae_layer\": LAYER,\n",
    "    \"sae_trainer\": \"131k-l0-114\"\n",
    "}\n",
    "formatted[\"results\"] = fixed_results\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/swapped_user_role.json\", \"w\") as f:\n",
    "    json.dump(formatted, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
