{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Personal Feature Activations on General Prompts\n",
    "\n",
    "This notebook analyzes which given SAE features are activated on given prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  SAE Layer: 15, Trainer: 1\n",
      "  Token extraction: newline (offset: 0)\n",
      "  Assistant header: <|im_start|>assistant\n",
      "  Output directory: ./qwen_trainer1_layer15_newline\n",
      "  SAE Release: andyrdt/saes-qwen2.5-7b-instruct\n",
      "  Dashboard base URL: https://completely-touched-platypus.ngrok-free.app/\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"qwen\"  # Options: \"qwen\" or \"llama\"\n",
    "TOKEN_TYPE = \"newline\"  # Options: \"asst\", \"newline\", \"endheader\" (endheader only for llama)\n",
    "SAE_LAYER = 15\n",
    "SAE_TRAINER = 1\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DASHBOARD URL - Global variable for links\n",
    "# =============================================================================\n",
    "FEATURE_DASHBOARD_BASE_URL = \"https://completely-touched-platypus.ngrok-free.app/\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "if MODEL_TYPE == \"qwen\":\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-qwen2.5-7b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|im_start|>assistant\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/qwen-2.5-7b-instruct/saes\"\n",
    "    \n",
    "elif MODEL_TYPE == \"llama\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-llama-3.1-8b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b-instruct/saes\"\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# Validate token type\n",
    "if TOKEN_TYPE not in TOKEN_OFFSETS:\n",
    "    raise ValueError(f\"TOKEN_TYPE '{TOKEN_TYPE}' not available for {MODEL_TYPE}. Available: {list(TOKEN_OFFSETS.keys())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_CONFIG = {\n",
    "    \"release\": SAE_RELEASE,\n",
    "    \"layer\": SAE_LAYER,\n",
    "    \"trainer\": SAE_TRAINER\n",
    "}\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "TOKEN_OFFSET = TOKEN_OFFSETS[TOKEN_TYPE]\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_PATH = \"./prompts\"\n",
    "\n",
    "# Output directory with clear naming\n",
    "OUTPUT_DIR = f\"./{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{TOKEN_TYPE}\"\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "TOP_FEATURES = 100\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Token extraction: {TOKEN_TYPE} (offset: {TOKEN_OFFSET})\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  SAE Release: {SAE_RELEASE}\")\n",
    "print(f\"  Dashboard base URL: {FEATURE_DASHBOARD_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_prompts(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts with labels from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "            labels.append(data['label'])\n",
    "    return pd.DataFrame({'prompt': prompts, 'label': labels})\n",
    "\n",
    "# Load prompts from multiple .jsonl files in PROMPTS_PATH into one dataframe\n",
    "prompts_df = pd.DataFrame()\n",
    "for file in os.listdir(PROMPTS_PATH):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        df = load_prompts(os.path.join(PROMPTS_PATH, file))\n",
    "        prompts_df = pd.concat([prompts_df, df])\n",
    "\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_target_features(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load target features from CSV file.\"\"\"\n",
    "    features = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            features.append(line.strip())\n",
    "    return pd.DataFrame({'feature': features})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: Qwen2TokenizerFast\n",
      "\n",
      "Chat template test:\n",
      "Original: What's it like to be you?\n",
      "Formatted: \"<|im_start|>system\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nWhat's it like to be you?<|im_end|>\\n<|im_start|>assistant\\n\"\n",
      "Formatted (readable):\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "What's it like to be you?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "\n",
      "============================================================\n",
      "ASSISTANT HEADER TOKENIZATION ANALYSIS\n",
      "============================================================\n",
      "Assistant header: <|im_start|>assistant\n",
      "Number of tokens: 2\n",
      "Token IDs: [151644, 77091]\n",
      "Individual tokens: ['<|im_start|>', 'assistant']\n",
      "\n",
      "Full prompt tokens: 37\n",
      "All tokens with positions:\n",
      "   0: '<|im_start|>'\n",
      "   1: 'system'\n",
      "   2: '\n",
      "'\n",
      "   3: 'You'\n",
      "   4: ' are'\n",
      "   5: ' Q'\n",
      "   6: 'wen'\n",
      "   7: ','\n",
      "   8: ' created'\n",
      "   9: ' by'\n",
      "  10: ' Alibaba'\n",
      "  11: ' Cloud'\n",
      "  12: '.'\n",
      "  13: ' You'\n",
      "  14: ' are'\n",
      "  15: ' a'\n",
      "  16: ' helpful'\n",
      "  17: ' assistant'\n",
      "  18: '.'\n",
      "  19: '<|im_end|>'\n",
      "  20: '\n",
      "'\n",
      "  21: '<|im_start|>'\n",
      "  22: 'user'\n",
      "  23: '\n",
      "'\n",
      "  24: 'What'\n",
      "  25: ''s'\n",
      "  26: ' it'\n",
      "  27: ' like'\n",
      "  28: ' to'\n",
      "  29: ' be'\n",
      "  30: ' you'\n",
      "  31: '?'\n",
      "  32: '<|im_end|>'\n",
      "  33: '\n",
      "'\n",
      "  34: '<|im_start|>'\n",
      "  35: 'assistant'\n",
      "  36: '\n",
      "'\n",
      "\n",
      "Assistant header found at positions 34 to 35\n",
      "Assistant header tokens: ['<|im_start|>', 'assistant']\n",
      "\n",
      "Extraction calculation:\n",
      "  assistant_start_pos: 34\n",
      "  + len(assistant_tokens): 2\n",
      "  + TOKEN_OFFSET ('newline'): 0\n",
      "  = extraction_pos: 36\n",
      "✓ Token at extraction position 36: '\n",
      "'\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Test chat template formatting\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"What's it like to be you?\"}]\n",
    "formatted_test = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"\\nChat template test:\")\n",
    "print(f\"Original: What's it like to be you?\")\n",
    "print(f\"Formatted: {repr(formatted_test)}\")\n",
    "print(f\"Formatted (readable):\\n{formatted_test}\")\n",
    "\n",
    "# Test tokenization of assistant header to understand positioning\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ASSISTANT HEADER TOKENIZATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "assistant_token_texts = [tokenizer.decode([token]) for token in assistant_tokens]\n",
    "\n",
    "print(f\"Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"Number of tokens: {len(assistant_tokens)}\")\n",
    "print(f\"Token IDs: {assistant_tokens}\")\n",
    "print(f\"Individual tokens: {assistant_token_texts}\")\n",
    "\n",
    "# Test with a full formatted prompt\n",
    "full_tokens = tokenizer.encode(formatted_test, add_special_tokens=False)\n",
    "full_token_texts = [tokenizer.decode([token]) for token in full_tokens]\n",
    "\n",
    "print(f\"\\nFull prompt tokens: {len(full_tokens)}\")\n",
    "print(\"All tokens with positions:\")\n",
    "for i, token_text in enumerate(full_token_texts):\n",
    "    print(f\"  {i:2d}: '{token_text}'\")\n",
    "\n",
    "# Find where assistant header appears in full prompt\n",
    "assistant_start_pos = None\n",
    "for i in range(len(full_tokens) - len(assistant_tokens) + 1):\n",
    "    if full_tokens[i:i+len(assistant_tokens)] == assistant_tokens:\n",
    "        assistant_start_pos = i\n",
    "        break\n",
    "\n",
    "if assistant_start_pos is not None:\n",
    "    assistant_end_pos = assistant_start_pos + len(assistant_tokens) - 1\n",
    "    print(f\"\\nAssistant header found at positions {assistant_start_pos} to {assistant_end_pos}\")\n",
    "    print(f\"Assistant header tokens: {full_token_texts[assistant_start_pos:assistant_end_pos+1]}\")\n",
    "    \n",
    "    # Show what the extraction function will actually extract\n",
    "    extraction_pos = assistant_start_pos + len(assistant_tokens) + TOKEN_OFFSET\n",
    "    print(f\"\\nExtraction calculation:\")\n",
    "    print(f\"  assistant_start_pos: {assistant_start_pos}\")\n",
    "    print(f\"  + len(assistant_tokens): {len(assistant_tokens)}\")  \n",
    "    print(f\"  + TOKEN_OFFSET ('{TOKEN_TYPE}'): {TOKEN_OFFSET}\")\n",
    "    print(f\"  = extraction_pos: {extraction_pos}\")\n",
    "    \n",
    "    if 0 <= extraction_pos < len(full_token_texts):\n",
    "        print(f\"✓ Token at extraction position {extraction_pos}: '{full_token_texts[extraction_pos]}'\")\n",
    "    else:\n",
    "        print(f\"❌ Extraction position {extraction_pos} is out of bounds (valid range: 0-{len(full_token_texts)-1})\")\n",
    "else:\n",
    "    print(\"❌ Assistant header not found in full prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76bd8bf2dcee4ad9993fff85741224a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device_map_value = device.index if device.type == 'cuda' and device.index is not None else str(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device_map_value}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found SAE files at: /workspace/sae/qwen-2.5-7b-instruct/saes/resid_post_layer_15/trainer_1\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load SAE\n",
    "ae_file_path = os.path.join(SAE_PATH, \"ae.pt\")\n",
    "config_file_path = os.path.join(SAE_PATH, \"config.json\")\n",
    "\n",
    "if os.path.exists(ae_file_path) and os.path.exists(config_file_path):\n",
    "    print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "else:\n",
    "    print(f\"SAE not found locally, downloading from {SAE_RELEASE}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    sae_path = f\"resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "    local_dir = SAE_BASE_PATH\n",
    "    ae_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/ae.pt\", local_dir=local_dir)\n",
    "    config_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/config.json\", local_dir=local_dir)\n",
    "\n",
    "sae, _ = load_dictionary(SAE_PATH, device=device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded with {sae.dict_size} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations(prompts: List[str], layer_idx: int) -> torch.Tensor:\n",
    "    \"\"\"Extract activations from specified layer for given prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            # Output is tuple, take first element (hidden states)\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass (will be stopped by hook)\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Extract assistant token positions\n",
    "        batch_activations = []\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            # Get attention mask for this item\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            \n",
    "            # Find assistant header position\n",
    "            assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Find where assistant section starts\n",
    "            assistant_pos = None\n",
    "            for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "                if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "                    assistant_pos = k + len(assistant_tokens) + TOKEN_OFFSET\n",
    "                    break\n",
    "            \n",
    "            if assistant_pos is None:\n",
    "                # Fallback to last non-padding token\n",
    "                assistant_pos = attention_mask.sum().item() - 1\n",
    "            \n",
    "            # Ensure position is within bounds\n",
    "            max_pos = attention_mask.sum().item() - 1\n",
    "            assistant_pos = min(assistant_pos, max_pos)\n",
    "            assistant_pos = max(assistant_pos, 0)\n",
    "            \n",
    "            # Extract activation at assistant position\n",
    "            assistant_activation = activations[j, assistant_pos, :]  # [hidden_dim]\n",
    "            batch_activations.append(assistant_activation.cpu())\n",
    "        \n",
    "        all_activations.extend(batch_activations)\n",
    "    \n",
    "    return torch.stack(all_activations, dim=0)\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba1623f1237b4b8f918a84ae2c164a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: torch.Size([140, 3584])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for prompts\n",
    "print(\"Extracting activations for prompts...\")\n",
    "activations = extract_activations(prompts_df['prompt'], LAYER_INDEX)\n",
    "print(f\"Activations shape: {activations.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for all prompts...\n",
      "Features shape: torch.Size([140, 131072])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Get SAE feature activations\n",
    "print(\"Computing SAE features for all prompts...\")\n",
    "features = get_sae_features(activations)\n",
    "print(f\"Features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def find_universally_active_features(features: torch.Tensor, activation_threshold: float = 0.01, prompt_threshold: float = 1.0) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Find features that are active (above threshold) for a specified percentage of prompts.\n",
    "    \n",
    "    Args:\n",
    "        features: Feature activations tensor of shape [num_prompts, num_features]\n",
    "        activation_threshold: Minimum activation value to consider a feature \"active\"\n",
    "        prompt_threshold: Minimum percentage of prompts (0.0 to 1.0) that must have the feature active\n",
    "    \n",
    "    Returns:\n",
    "        universal_features: Indices of features that are active for at least prompt_threshold fraction of prompts\n",
    "        universal_activations: Mean activation values for universal features\n",
    "    \"\"\"\n",
    "    # Check which features are active (above threshold) for each prompt\n",
    "    active_features = features > activation_threshold  # [num_prompts, num_features]\n",
    "    \n",
    "    # Count how many prompts each feature is active for\n",
    "    num_active_prompts = torch.sum(active_features, dim=0)  # [num_features]\n",
    "    \n",
    "    # Calculate the minimum number of prompts required\n",
    "    min_prompts_required = int(features.shape[0] * prompt_threshold)\n",
    "    \n",
    "    # Find features that are active for at least the required number of prompts\n",
    "    universal_mask = num_active_prompts >= min_prompts_required  # [num_features]\n",
    "    universal_features = torch.where(universal_mask)[0]  # Indices of universal features\n",
    "    \n",
    "    # Get mean activation values for universal features\n",
    "    universal_activations = features[:, universal_features].mean(dim=0)\n",
    "    \n",
    "    return universal_features, universal_activations\n",
    "\n",
    "# Find universally active features\n",
    "print(\"Finding features that activate for every single prompt...\")\n",
    "universal_features, universal_activations = find_universally_active_features(features, prompt_threshold=0.3)\n",
    "\n",
    "print(f\"Found {len(universal_features)} features that are active for all {features.shape[0]} prompts\")\n",
    "print(f\"Universal features (indices): {universal_features.tolist()}\")\n",
    "print(f\"Mean activation values: {universal_activations.tolist()}\")\n",
    "\n",
    "# Create a summary dataframe\n",
    "if len(universal_features) > 0:\n",
    "    universal_df = pd.DataFrame({\n",
    "        'feature_index': universal_features.tolist(),\n",
    "        'mean_activation': universal_activations.tolist()\n",
    "    })\n",
    "    \n",
    "    # Sort by mean activation (descending)\n",
    "    universal_df = universal_df.sort_values('mean_activation', ascending=False)\n",
    "    \n",
    "    print(\"\\nUniversal features summary (sorted by mean activation):\")\n",
    "    print(universal_df.to_string(index=False))\n",
    "    \n",
    "    # Show distribution of activations for top universal features\n",
    "    print(\"\\nDetailed activations for top 5 universal features:\")\n",
    "    for i, (feature_idx, mean_act) in enumerate(zip(universal_df['feature_index'].head(5), \n",
    "                                                   universal_df['mean_activation'].head(5))):\n",
    "        feature_activations = features[:, feature_idx]\n",
    "        print(f\"Feature {feature_idx}: mean={mean_act:.4f}, min={feature_activations.min():.4f}, max={feature_activations.max():.4f}\")\n",
    "        print(f\"  Activations: {feature_activations.tolist()}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No features are active for all prompts with the current threshold.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "function-cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 features from the feature_id's in the feature_file\n",
      "Feature activations shape: torch.Size([140, 31])\n"
     ]
    }
   ],
   "source": [
    "# feature_file = f\"./results/personal/{MODEL_TYPE}_trainer1_layer{LAYER_INDEX}_{TOKEN_TYPE}/assistant_only_features.csv\"\n",
    "\n",
    "# @torch.no_grad()\n",
    "# def find_features_from_file(original_features: torch.Tensor, feature_file: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#     \"\"\"\n",
    "#     Find features that are listed in the feature file.\n",
    "    \n",
    "#     Args:\n",
    "#         original_features: Feature activations tensor of shape [num_prompts, num_features]\n",
    "#         feature_file: Path to the feature file with feature_id that we want to get activations for\n",
    "    \n",
    "#     Returns:\n",
    "#         feature_indices: Indices of features from the feature_file\n",
    "#         feature_activations: Activation values for given features [num_prompts, num_selected_features]\n",
    "#     \"\"\"    \n",
    "#     target_features_df = pd.read_csv(feature_file)\n",
    "#     target_feature_ids = target_features_df['feature_id'].tolist()\n",
    "    \n",
    "#     # Convert to tensor for indexing\n",
    "#     target_feature_indices = torch.tensor(target_feature_ids, dtype=torch.long)\n",
    "#     target_feature_activations = original_features[:, target_feature_indices]\n",
    "    \n",
    "#     return target_feature_indices, target_feature_activations\n",
    "\n",
    "# # Now use the full features tensor\n",
    "# feature_indices, feature_activations = find_features_from_file(features, feature_file)\n",
    "\n",
    "# print(f\"Found {len(feature_indices)} features from the feature_id's in the feature_file\")\n",
    "# print(f\"Feature activations shape: {feature_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "second-call-cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to CSV\n",
    "results_dir = \"./results/universal_30\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Create filename with specified format\n",
    "csv_filename = f\"{MODEL_TYPE}_trainer1_layer{SAE_LAYER}_{TOKEN_TYPE}.csv\"\n",
    "csv_path = os.path.join(results_dir, csv_filename)\n",
    "\n",
    "# Prepare data for CSV\n",
    "all_results = []\n",
    "\n",
    "# Find universal features across ALL prompts\n",
    "print(\"Finding universal features across all prompts...\")\n",
    "universal_features_all, universal_activations_all = find_universally_active_features(features)\n",
    "\n",
    "if len(universal_features_all) > 0:\n",
    "    for feature_idx in universal_features_all:\n",
    "        feature_activations = features[:, feature_idx]\n",
    "        feature_id = feature_idx.item()\n",
    "        all_results.append({\n",
    "            'feature_id': feature_id,\n",
    "            'activation_mean': feature_activations.mean().item(),\n",
    "            'activation_max': feature_activations.max().item(),\n",
    "            'activation_min': feature_activations.min().item(),\n",
    "            'chat_desc': '',\n",
    "            'pt_desc': '',\n",
    "            'type': '',\n",
    "            'link': f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer=1&fids={feature_id}\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame and sort by activation mean (descending)\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df = results_df.sort_values('activation_mean', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal universal features found: {len(results_df)}\")\n",
    "    \n",
    "else:\n",
    "    # No universal features found\n",
    "    results_df = pd.DataFrame(columns=['feature_id', 'activation_mean', 'activation_max', 'activation_min', 'chat_desc', 'pt_desc', 'type', 'link'])\n",
    "    print(\"Warning: No universal features found\")\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nResults saved to: {csv_path}\")\n",
    "print(f\"Number of universal features saved: {len(results_df)}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nPreview of saved data:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Show sample link\n",
    "    if len(results_df) > 0:\n",
    "        sample_link = results_df.iloc[0]['link']\n",
    "        print(f\"\\nSample dashboard link: {sample_link}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing features from file...\n",
      "\n",
      "Total features from file: 31\n",
      "\n",
      "Results saved to: ./results/personal_general/qwen_trainer1_layer15_newline.csv\n",
      "Number of features saved: 31\n",
      "\n",
      "Preview of saved data:\n",
      " feature_id  activation_mean  activation_max  activation_min chat_desc pt_desc type                                                                                          link\n",
      "      49123         2.686073       10.548953             0.0                         https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=49123\n",
      "       9953         0.332070        4.707234             0.0                          https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=9953\n",
      "      48045         0.221637        4.731411             0.0                         https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=48045\n",
      "      88910         0.053197        3.390732             0.0                         https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=88910\n",
      "      61741         0.000000        0.000000             0.0                         https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=61741\n",
      "     116026         0.000000        0.000000             0.0                        https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=116026\n",
      "      29717         0.000000        0.000000             0.0                         https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=29717\n",
      "      43828         0.000000        0.000000             0.0                         https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=43828\n",
      "      38671         0.000000        0.000000             0.0                         https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=38671\n",
      "     101228         0.000000        0.000000             0.0                        https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=101228\n",
      "\n",
      "Sample dashboard link: https://completely-touched-platypus.ngrok-free.app/?model=qwen&layer=15&trainer=1&fids=49123\n"
     ]
    }
   ],
   "source": [
    "# # Save results to CSV using the corrected feature extraction\n",
    "# results_dir = \"./results/personal_general\"\n",
    "# os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# # Create filename with specified format\n",
    "# csv_filename = f\"{MODEL_TYPE}_trainer1_layer{SAE_LAYER}_{TOKEN_TYPE}.csv\"\n",
    "# csv_path = os.path.join(results_dir, csv_filename)\n",
    "\n",
    "# # Prepare data for CSV using the features from file\n",
    "# all_results = []\n",
    "\n",
    "# print(\"Processing features from file...\")\n",
    "\n",
    "# if len(feature_indices) > 0:\n",
    "#     for i, feature_idx in enumerate(feature_indices):\n",
    "#         feature_activations_for_this_feature = feature_activations[:, i]  # Get activations for this feature across all prompts\n",
    "#         feature_id = feature_idx.item()\n",
    "#         all_results.append({\n",
    "#             'feature_id': feature_id,\n",
    "#             'activation_mean': feature_activations_for_this_feature.mean().item(),\n",
    "#             'activation_max': feature_activations_for_this_feature.max().item(),\n",
    "#             'activation_min': feature_activations_for_this_feature.min().item(),\n",
    "#             'chat_desc': '',\n",
    "#             'pt_desc': '',\n",
    "#             'type': '',\n",
    "#             'link': f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer=1&fids={feature_id}\"\n",
    "#         })\n",
    "\n",
    "# # Create DataFrame and sort by activation mean (descending)\n",
    "# if all_results:\n",
    "#     results_df = pd.DataFrame(all_results)\n",
    "#     results_df = results_df.sort_values('activation_mean', ascending=False)\n",
    "    \n",
    "#     print(f\"\\nTotal features from file: {len(results_df)}\")\n",
    "    \n",
    "# else:\n",
    "#     # No features found\n",
    "#     results_df = pd.DataFrame(columns=['feature_id', 'activation_mean', 'activation_max', 'activation_min', 'chat_desc', 'pt_desc', 'type', 'link'])\n",
    "#     print(\"Warning: No features found in file\")\n",
    "\n",
    "# # Save to CSV\n",
    "# results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "# print(f\"\\nResults saved to: {csv_path}\")\n",
    "# print(f\"Number of features saved: {len(results_df)}\")\n",
    "\n",
    "# if len(results_df) > 0:\n",
    "#     print(f\"\\nPreview of saved data:\")\n",
    "#     print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "#     # Show sample link\n",
    "#     if len(results_df) > 0:\n",
    "#         sample_link = results_df.iloc[0]['link']\n",
    "#         print(f\"\\nSample dashboard link: {sample_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
