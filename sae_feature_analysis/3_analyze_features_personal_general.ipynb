{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Personal Feature Activations on General Prompts\n",
    "\n",
    "This notebook analyzes which given SAE features are activated on given prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model: Qwen/Qwen2.5-7B-Instruct\n",
      "  SAE: andyrdt/saes-qwen2.5-7b-instruct\n",
      "  SAE Layer: 15, Trainer: 1\n",
      "  Available token types: ['asst', 'newline']\n",
      "  Assistant header: <|im_start|>assistant\n",
      "  Output file: ./results/3_personal_general/2_personal_general.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"qwen\"  # Options: \"qwen\" or \"llama\"\n",
    "SAE_LAYER = 15\n",
    "SAE_TRAINER = 1\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_FILE = f\"./results/3_personal_general/2_personal_general.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DASHBOARD URL - Global variable for links\n",
    "# =============================================================================\n",
    "FEATURE_DASHBOARD_BASE_URL = \"https://completely-touched-platypus.ngrok-free.app/\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "if MODEL_TYPE == \"qwen\":\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-qwen2.5-7b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|im_start|>assistant\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/qwen-2.5-7b-instruct/saes\"\n",
    "    \n",
    "elif MODEL_TYPE == \"llama\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-llama-3.1-8b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b-instruct/saes\"\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_CONFIG = {\n",
    "    \"release\": SAE_RELEASE,\n",
    "    \"layer\": SAE_LAYER,\n",
    "    \"trainer\": SAE_TRAINER\n",
    "}\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_PATH = \"./prompts/general\"\n",
    "FEATURES_FILE = \"./results/1_personal/only_personal.csv\"\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "TOP_FEATURES = 100\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE: {SAE_RELEASE}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Available token types: {list(TOKEN_OFFSETS.keys())}\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_prompts(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts with labels from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "            labels.append(data['label'])\n",
    "    return pd.DataFrame({'prompt': prompts, 'label': labels})\n",
    "\n",
    "# Load prompts from multiple .jsonl files in PROMPTS_PATH into one dataframe\n",
    "prompts_df = pd.DataFrame()\n",
    "for file in os.listdir(PROMPTS_PATH):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        df = load_prompts(os.path.join(PROMPTS_PATH, file))\n",
    "        prompts_df = pd.concat([prompts_df, df])\n",
    "\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: Qwen2TokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Test chat template formatting\n",
    "# test_messages = [{\"role\": \"user\", \"content\": \"What's it like to be you?\"}]\n",
    "# formatted_test = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "# print(f\"\\nChat template test:\")\n",
    "# print(f\"Original: What's it like to be you?\")\n",
    "# print(f\"Formatted: {repr(formatted_test)}\")\n",
    "# print(f\"Formatted (readable):\\n{formatted_test}\")\n",
    "\n",
    "# # Test tokenization of assistant header to understand positioning\n",
    "# print(f\"\\n\" + \"=\"*60)\n",
    "# print(\"ASSISTANT HEADER TOKENIZATION ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "# assistant_token_texts = [tokenizer.decode([token]) for token in assistant_tokens]\n",
    "\n",
    "# print(f\"Assistant header: {ASSISTANT_HEADER}\")\n",
    "# print(f\"Number of tokens: {len(assistant_tokens)}\")\n",
    "# print(f\"Token IDs: {assistant_tokens}\")\n",
    "# print(f\"Individual tokens: {assistant_token_texts}\")\n",
    "\n",
    "# # Test with a full formatted prompt\n",
    "# full_tokens = tokenizer.encode(formatted_test, add_special_tokens=False)\n",
    "# full_token_texts = [tokenizer.decode([token]) for token in full_tokens]\n",
    "\n",
    "# print(f\"\\nFull prompt tokens: {len(full_tokens)}\")\n",
    "# print(\"All tokens with positions:\")\n",
    "# for i, token_text in enumerate(full_token_texts):\n",
    "#     print(f\"  {i:2d}: '{token_text}'\")\n",
    "\n",
    "# # Find where assistant header appears in full prompt\n",
    "# assistant_start_pos = None\n",
    "# for i in range(len(full_tokens) - len(assistant_tokens) + 1):\n",
    "#     if full_tokens[i:i+len(assistant_tokens)] == assistant_tokens:\n",
    "#         assistant_start_pos = i\n",
    "#         break\n",
    "\n",
    "# if assistant_start_pos is not None:\n",
    "#     assistant_end_pos = assistant_start_pos + len(assistant_tokens) - 1\n",
    "#     print(f\"\\nAssistant header found at positions {assistant_start_pos} to {assistant_end_pos}\")\n",
    "#     print(f\"Assistant header tokens: {full_token_texts[assistant_start_pos:assistant_end_pos+1]}\")\n",
    "    \n",
    "#     # Show what the extraction function will actually extract\n",
    "#     extraction_pos = assistant_start_pos + len(assistant_tokens) + TOKEN_OFFSET\n",
    "#     print(f\"\\nExtraction calculation:\")\n",
    "#     print(f\"  assistant_start_pos: {assistant_start_pos}\")\n",
    "#     print(f\"  + len(assistant_tokens): {len(assistant_tokens)}\")  \n",
    "#     print(f\"  + TOKEN_OFFSET ('{TOKEN_TYPE}'): {TOKEN_OFFSET}\")\n",
    "#     print(f\"  = extraction_pos: {extraction_pos}\")\n",
    "    \n",
    "#     if 0 <= extraction_pos < len(full_token_texts):\n",
    "#         print(f\"✓ Token at extraction position {extraction_pos}: '{full_token_texts[extraction_pos]}'\")\n",
    "#     else:\n",
    "#         print(f\"❌ Extraction position {extraction_pos} is out of bounds (valid range: 0-{len(full_token_texts)-1})\")\n",
    "# else:\n",
    "#     print(\"❌ Assistant header not found in full prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaa0628299164ea08fc70226add4d828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Qwen2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device_map_value = device.index if device.type == 'cuda' and device.index is not None else str(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device_map_value}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found SAE files at: /workspace/sae/qwen-2.5-7b-instruct/saes/resid_post_layer_15/trainer_1\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load SAE\n",
    "ae_file_path = os.path.join(SAE_PATH, \"ae.pt\")\n",
    "config_file_path = os.path.join(SAE_PATH, \"config.json\")\n",
    "\n",
    "if os.path.exists(ae_file_path) and os.path.exists(config_file_path):\n",
    "    print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "else:\n",
    "    print(f\"SAE not found locally, downloading from {SAE_RELEASE}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    sae_path = f\"resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "    local_dir = SAE_BASE_PATH\n",
    "    ae_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/ae.pt\", local_dir=local_dir)\n",
    "    config_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/config.json\", local_dir=local_dir)\n",
    "\n",
    "sae, _ = load_dictionary(SAE_PATH, device=device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded with {sae.dict_size} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"\n",
    "    Find the position of the assistant token based on the given offset.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Input token IDs for a single prompt\n",
    "        attention_mask: Attention mask for the prompt\n",
    "        assistant_header: The assistant header string to find\n",
    "        token_offset: Offset from the end of assistant header\n",
    "        tokenizer: Tokenizer instance\n",
    "        device: Device to use for computations\n",
    "    \n",
    "    Returns:\n",
    "        Position index for token extraction\n",
    "    \"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return assistant_pos\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations_all_positions(prompts: List[str], layer_idx: int) -> Tuple[torch.Tensor, List[Dict]]:\n",
    "    \"\"\"Extract activations from specified layer for all positions, then extract specific token positions.\"\"\"\n",
    "    all_activations = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            # Output is tuple, take first element (hidden states)\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass (will be stopped by hook)\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # For each prompt in the batch, calculate positions for all token types\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Calculate positions for all token types\n",
    "            positions = {}\n",
    "            for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "                positions[token_type] = find_assistant_position(\n",
    "                    input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "                )\n",
    "            \n",
    "            # Store the full activation sequence and metadata\n",
    "            all_activations.append(activations[j].cpu())  # [seq_len, hidden_dim]\n",
    "            all_metadata.append({\n",
    "                'prompt_idx': i + j,\n",
    "                'positions': positions,\n",
    "                'attention_mask': attention_mask.cpu()\n",
    "            })\n",
    "    \n",
    "    # Find the maximum sequence length across all activations\n",
    "    max_seq_len = max(act.shape[0] for act in all_activations)\n",
    "    hidden_dim = all_activations[0].shape[1]\n",
    "    \n",
    "    # Pad all activations to the same length\n",
    "    padded_activations = []\n",
    "    for act in all_activations:\n",
    "        if act.shape[0] < max_seq_len:\n",
    "            # Pad with zeros\n",
    "            padding = torch.zeros(max_seq_len - act.shape[0], hidden_dim)\n",
    "            padded_act = torch.cat([act, padding], dim=0)\n",
    "        else:\n",
    "            padded_act = act\n",
    "        padded_activations.append(padded_act)\n",
    "    \n",
    "    return torch.stack(padded_activations, dim=0), all_metadata\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_activations(full_activations: torch.Tensor, metadata: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extract activations for specific token positions from full sequence activations.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results for each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = []\n",
    "    \n",
    "    # Extract activations for each token type\n",
    "    for i, meta in enumerate(metadata):\n",
    "        for token_type, position in meta['positions'].items():\n",
    "            # Extract activation at the specific position\n",
    "            activation = full_activations[i, position, :]  # [hidden_dim]\n",
    "            results[token_type].append(activation)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = torch.stack(results[token_type], dim=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for all positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96141961276d4d8fb7133c8a1f984ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full activations shape: torch.Size([140, 153, 3584])\n",
      "\n",
      "Extracting activations for all token types...\n",
      "Token type 'asst' activations shape: torch.Size([140, 3584])\n",
      "Token type 'newline' activations shape: torch.Size([140, 3584])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for all positions first, then extract specific token positions\n",
    "print(\"Extracting activations for all positions...\")\n",
    "full_activations, metadata = extract_activations_all_positions(prompts_df['prompt'], LAYER_INDEX)\n",
    "print(f\"Full activations shape: {full_activations.shape}\")\n",
    "\n",
    "# Extract activations for all token types\n",
    "print(\"\\nExtracting activations for all token types...\")\n",
    "token_activations = extract_token_activations(full_activations, metadata)\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Token type '{token_type}' activations shape: {activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for all token types...\n",
      "Processing SAE features for token type 'asst'...\n",
      "Features shape for 'asst': torch.Size([140, 131072])\n",
      "Processing SAE features for token type 'newline'...\n",
      "Features shape for 'newline': torch.Size([140, 131072])\n",
      "\n",
      "Completed SAE feature extraction for 2 token types\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Get SAE feature activations for all token types\n",
    "print(\"Computing SAE features for all token types...\")\n",
    "token_features = {}\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Processing SAE features for token type '{token_type}'...\")\n",
    "    features = get_sae_features(activations)\n",
    "    token_features[token_type] = features\n",
    "    print(f\"Features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "print(f\"\\nCompleted SAE feature extraction for {len(token_features)} token types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "function-cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 features from the feature_id's in the feature_file\n",
      "Feature activations shape: torch.Size([140, 31])\n"
     ]
    }
   ],
   "source": [
    "# # TODO: Need to ensure \"token\" and \"source\" fields match, not just the feature_id.\n",
    "# @torch.no_grad()\n",
    "# def find_features_from_file(original_features: torch.Tensor, feature_file: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "#     \"\"\"\n",
    "#     Find features that are listed in the feature file.\n",
    "    \n",
    "#     Args:\n",
    "#         original_features: Feature activations tensor of shape [num_prompts, num_features]\n",
    "#         feature_file: Path to the feature file with feature_id that we want to get activations for\n",
    "    \n",
    "#     Returns:\n",
    "#         feature_indices: Indices of features from the feature_file\n",
    "#         feature_activations: Activation values for given features [num_prompts, num_selected_features]\n",
    "#     \"\"\"    \n",
    "#     target_features_df = pd.read_csv(feature_file)\n",
    "#     target_feature_ids = target_features_df['feature_id'].tolist()\n",
    "    \n",
    "#     # Convert to tensor for indexing\n",
    "#     target_feature_indices = torch.tensor(target_feature_ids, dtype=torch.long)\n",
    "#     target_feature_activations = original_features[:, target_feature_indices]\n",
    "    \n",
    "#     return target_feature_indices, target_feature_activations\n",
    "\n",
    "# # Now use the full features tensor\n",
    "# feature_indices, feature_activations = find_features_from_file(features, FEATURES_FILE)\n",
    "\n",
    "# print(f\"Found {len(feature_indices)} features from the feature_id's in the {FEATURES_FILE}\")\n",
    "# print(f\"Feature activations shape: {feature_activations.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_features_from_file(feature_file: str, features: torch.Tensor, token_type: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    print(f\"\\nProcessing token type '{token_type}'...\")\n",
    "    \n",
    "    # Extract activations for target features only\n",
    "    target_feature_indices = torch.tensor(target_feature_ids, dtype=torch.long)\n",
    "    target_feature_activations = features[:, target_feature_indices]  # [num_prompts, num_target_features]\n",
    "    \n",
    "    print(f\"Target feature activations shape for '{token_type}': {target_feature_activations.shape}\")\n",
    "    \n",
    "    # Process each target feature\n",
    "    for i, feature_id in enumerate(target_feature_ids):\n",
    "        feature_activations_for_this_feature = target_feature_activations[:, i]  # [num_prompts]\n",
    "        \n",
    "        all_results.append({\n",
    "            'feature_id': feature_id,\n",
    "            'source': source_name,\n",
    "            'token': token_type,\n",
    "            'activation_mean': feature_activations_for_this_feature.mean().item(),\n",
    "            'activation_max': feature_activations_for_this_feature.max().item(),\n",
    "            'activation_min': feature_activations_for_this_feature.min().item(),\n",
    "            'chat_desc': '',\n",
    "            'pt_desc': '',\n",
    "            'type': '',\n",
    "            'link': f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer={SAE_TRAINER}&fids={feature_id}\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame and sort by token type, then by activation mean (descending)\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df = results_df.sort_values(['token', 'activation_mean'], ascending=[True, False])\n",
    "    \n",
    "    print(f\"\\nTotal results: {len(results_df)}\")\n",
    "    print(f\"Results by token type:\")\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        count = len(results_df[results_df['token'] == token_type])\n",
    "        print(f\"  {token_type}: {count} features\")\n",
    "    \n",
    "else:\n",
    "    # No results found\n",
    "    results_df = pd.DataFrame(columns=['feature_id', 'source', 'token', 'activation_mean', 'activation_max', 'activation_min', 'chat_desc', 'pt_desc', 'type', 'link'])\n",
    "    print(\"Warning: No results found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target features from file\n",
    "target_features_df = pd.read_csv(FEATURES_FILE)\n",
    "target_feature_ids = target_features_df['feature_id'].tolist()\n",
    "\n",
    "print(f\"Loaded {len(target_feature_ids)} target features from {FEATURES_FILE}\")\n",
    "\n",
    "# Prepare all results for CSV\n",
    "all_results = []\n",
    "source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "\n",
    "print(\"Processing results for all token types...\")\n",
    "\n",
    "\n",
    "\n",
    "# Save to CSV (append mode if file exists, otherwise create new)\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    # If file exists, append to it\n",
    "    results_df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "    print(f\"\\nResults appended to existing file: {OUTPUT_FILE}\")\n",
    "else:\n",
    "    # Create new file\n",
    "    results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"\\nResults saved to new file: {OUTPUT_FILE}\")\n",
    "\n",
    "print(f\"Number of results saved: {len(results_df)}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nPreview of saved data:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Show sample link\n",
    "    sample_link = results_df.iloc[0]['link']\n",
    "    print(f\"\\nSample dashboard link: {sample_link}\")\n",
    "    \n",
    "    # Show summary by token type\n",
    "    print(f\"\\nSummary by token type:\")\n",
    "    summary = results_df.groupby('token').agg({\n",
    "        'activation_mean': ['count', 'mean', 'max'],\n",
    "        'feature_id': 'nunique'\n",
    "    }).round(4)\n",
    "    print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
