{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Diffing Analysis\n",
    "\n",
    "This notebook analyzes the differences between base and chat models by comparing feature activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model data from: /workspace/results/4_diffing/1_llama_trainer32x_layer15_base.pt\n",
      "Loading chat model data from: /workspace/results/4_diffing/1_llama_trainer32x_layer15_chat.pt\n",
      "Output directory: llama_trainer32x_layer15\n",
      "\n",
      "Base data keys: ['asst', 'endheader', 'newline', 'metadata']\n",
      "Chat data keys: ['asst', 'endheader', 'newline', 'metadata']\n",
      "Base metadata: {'source': 'llama_trainer32x_layer15_base', 'model_type': 'llama', 'model_ver': 'base', 'sae_layer': 15, 'sae_trainer': '32x', 'num_prompts': 1000, 'num_features': 131072, 'token_types': ['asst', 'endheader', 'newline']}\n",
      "Chat metadata: {'source': 'llama_trainer32x_layer15_chat', 'model_type': 'llama', 'model_ver': 'chat', 'sae_layer': 15, 'sae_trainer': '32x', 'num_prompts': 1000, 'num_features': 131072, 'token_types': ['asst', 'endheader', 'newline']}\n",
      "\n",
      "Base token types: ['asst', 'endheader', 'newline']\n",
      "Chat token types: ['asst', 'endheader', 'newline']\n",
      "Processing 3 token types: ['asst', 'endheader', 'newline']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "MODEL_TYPE = \"llama\"\n",
    "SAE_LAYER = 15\n",
    "SAE_TRAINER = \"32x\"\n",
    "TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "\n",
    "# File paths\n",
    "BASE_FILE = f\"/workspace/results/4_diffing/1_{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_base.pt\"\n",
    "CHAT_FILE = f\"/workspace/results/4_diffing/1_{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_chat.pt\"\n",
    "\n",
    "# Output directory\n",
    "SOURCE = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "OUTPUT_DIR = Path(f\"./{SOURCE}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Link\n",
    "LLAMA_LINK_FORMAT = f\"https://www.neuronpedia.org/llama3.1-8b/{SAE_LAYER}-llamascope-res-131k/\"\n",
    "\n",
    "print(f\"Loading base model data from: {BASE_FILE}\")\n",
    "print(f\"Loading chat model data from: {CHAT_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load the PyTorch files\n",
    "base_data = torch.load(BASE_FILE)\n",
    "chat_data = torch.load(CHAT_FILE)\n",
    "\n",
    "print(f\"\\nBase data keys: {list(base_data.keys())}\")\n",
    "print(f\"Chat data keys: {list(chat_data.keys())}\")\n",
    "print(f\"Base metadata: {base_data['metadata']}\")\n",
    "print(f\"Chat metadata: {chat_data['metadata']}\")\n",
    "\n",
    "# Verify token types match\n",
    "base_tokens = [k for k in base_data.keys() if k != 'metadata']\n",
    "chat_tokens = [k for k in chat_data.keys() if k != 'metadata']\n",
    "print(f\"\\nBase token types: {base_tokens}\")\n",
    "print(f\"Chat token types: {chat_tokens}\")\n",
    "assert base_tokens == chat_tokens, \"Token types don't match between base and chat!\"\n",
    "\n",
    "token_types = base_tokens\n",
    "print(f\"Processing {len(token_types)} token types: {token_types}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Analyzing metric: all_mean\n",
      "==================================================\n",
      "Processing all_mean for token type: asst\n",
      "Processing all_mean for token type: endheader\n",
      "Processing all_mean for token type: newline\n",
      "Found 300 total records for all_mean\n",
      "\n",
      "Top 5 features for all_mean:\n",
      " rank  feature_id     token  all_mean_base  all_mean_chat  all_mean_diff                                                                  link\n",
      "    1       11904      asst       0.005578       3.741617       3.736039  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/11904\n",
      "    2       97377      asst       0.006156       3.023328       3.017172  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/97377\n",
      "    1       92801   newline       6.986360       9.858937       2.872578  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/92801\n",
      "    1      112879 endheader       0.000000       2.262328       2.262328 https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/112879\n",
      "    2       96419   newline       3.824219       6.014953       2.190734  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/96419\n",
      "\n",
      "Summary statistics for all_mean:\n",
      "Max difference: 3.736039\n",
      "Min difference: 0.010953\n",
      "Mean difference: 0.273882\n",
      "\n",
      "==================================================\n",
      "Analyzing metric: active_mean\n",
      "==================================================\n",
      "Processing active_mean for token type: asst\n",
      "Processing active_mean for token type: endheader\n",
      "Processing active_mean for token type: newline\n",
      "Found 300 total records for active_mean\n",
      "\n",
      "Top 5 features for active_mean:\n",
      " rank  feature_id     token  active_mean_base  active_mean_chat  active_mean_diff                                                                  link\n",
      "    1       74359   newline               0.0          10.50000          10.50000  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/74359\n",
      "    2       11850   newline               0.0           7.34375           7.34375  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/11850\n",
      "    3       99377   newline               0.0           6.78125           6.78125  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/99377\n",
      "    1       71951 endheader               0.0           6.62500           6.62500  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/71951\n",
      "    2      130135 endheader               0.0           6.43750           6.43750 https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/130135\n",
      "\n",
      "Summary statistics for active_mean:\n",
      "Max difference: 10.500000\n",
      "Min difference: 2.875000\n",
      "Mean difference: 4.164325\n",
      "\n",
      "==================================================\n",
      "Analyzing metric: sparsity\n",
      "==================================================\n",
      "Processing sparsity for token type: asst\n",
      "Processing sparsity for token type: endheader\n",
      "Processing sparsity for token type: newline\n",
      "Found 300 total records for sparsity\n",
      "\n",
      "Top 5 features for sparsity:\n",
      " rank  feature_id     token  sparsity_base  sparsity_chat  sparsity_diff                                                                  link\n",
      "    1       97377      asst          0.001          0.912          0.911  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/97377\n",
      "    2       11904      asst          0.002          0.881          0.879  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/11904\n",
      "    1      112879 endheader          0.000          0.706          0.706 https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/112879\n",
      "    3       84495      asst          0.000          0.702          0.702  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/84495\n",
      "    2       29865 endheader          0.000          0.675          0.675  https://www.neuronpedia.org/llama3.1-8b/15-llamascope-res-131k/29865\n",
      "\n",
      "Summary statistics for sparsity:\n",
      "Max difference: 0.911000\n",
      "Min difference: 0.004000\n",
      "Mean difference: 0.082350\n",
      "\n",
      "==================================================\n",
      "Analysis complete!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def find_top_increases(base_data, chat_data, token_types, metric_name, top_k=100):\n",
    "    \"\"\"\n",
    "    Find top features with greatest increase in specified metric from base to chat.\n",
    "    \n",
    "    Args:\n",
    "        base_data: Base model data dictionary\n",
    "        chat_data: Chat model data dictionary  \n",
    "        token_types: List of token types to process\n",
    "        metric_name: Name of metric to analyze ('all_mean', 'active_mean', 'sparsity')\n",
    "        top_k: Number of top features to return\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame with top features and their differences\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for token_type in token_types:\n",
    "        print(f\"Processing {metric_name} for token type: {token_type}\")\n",
    "        \n",
    "        # Get base and chat tensors for this metric\n",
    "        base_tensor = base_data[token_type][metric_name]\n",
    "        chat_tensor = chat_data[token_type][metric_name]\n",
    "        \n",
    "        # Calculate difference (chat - base)\n",
    "        diff_tensor = chat_tensor - base_tensor\n",
    "        \n",
    "        # Get top k features with largest increases\n",
    "        top_values, top_indices = torch.topk(diff_tensor, top_k)\n",
    "        \n",
    "        # Convert to lists for DataFrame\n",
    "        top_values = top_values.tolist()\n",
    "        top_indices = top_indices.tolist()\n",
    "        \n",
    "        # Get corresponding base and chat values\n",
    "        base_values = base_tensor[top_indices].tolist()\n",
    "        chat_values = chat_tensor[top_indices].tolist()\n",
    "        \n",
    "        # Create records for this token type\n",
    "        for i, (feat_idx, diff_val, base_val, chat_val) in enumerate(zip(top_indices, top_values, base_values, chat_values)):\n",
    "            record = {\n",
    "                'rank': i + 1,\n",
    "                'feature_id': feat_idx,\n",
    "                'token': token_type,\n",
    "                f'{metric_name}_base': base_val,\n",
    "                f'{metric_name}_chat': chat_val,\n",
    "                f'{metric_name}_diff': diff_val,\n",
    "                'link': LLAMA_LINK_FORMAT + str(feat_idx)\n",
    "            }\n",
    "            all_results.append(record)\n",
    "    \n",
    "    # Convert to DataFrame and sort by difference (descending)\n",
    "    df = pd.DataFrame(all_results)\n",
    "    df = df.sort_values(f'{metric_name}_diff', ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Found {len(df)} total records for {metric_name}\")\n",
    "    return df\n",
    "\n",
    "# Analyze all three metrics\n",
    "metrics_to_analyze = ['all_mean', 'active_mean', 'sparsity']\n",
    "results = {}\n",
    "\n",
    "for metric in metrics_to_analyze:\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Analyzing metric: {metric}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    df = find_top_increases(base_data, chat_data, token_types, metric, top_k=100)\n",
    "    results[metric] = df\n",
    "    \n",
    "    # Show preview\n",
    "    print(f\"\\nTop 5 features for {metric}:\")\n",
    "    print(df.head().to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nSummary statistics for {metric}:\")\n",
    "    print(f\"Max difference: {df[f'{metric}_diff'].max():.6f}\")\n",
    "    print(f\"Min difference: {df[f'{metric}_diff'].min():.6f}\")\n",
    "    print(f\"Mean difference: {df[f'{metric}_diff'].mean():.6f}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Analysis complete!\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results to CSV files...\n",
      "Saved 300 records to: llama_trainer32x_layer15/top_all_mean.csv\n",
      "  File size: 39.6 KB\n",
      "Saved 300 records to: llama_trainer32x_layer15/top_active_mean.csv\n",
      "  File size: 32.0 KB\n",
      "Saved 300 records to: llama_trainer32x_layer15/top_sparsity.csv\n",
      "  File size: 39.8 KB\n",
      "\n",
      "==================================================\n",
      "All results saved!\n",
      "==================================================\n",
      "\n",
      "Summary of saved files:\n",
      "  top_all_mean.csv: Top 100 features per token type with greatest all_mean increase\n",
      "  top_active_mean.csv: Top 100 features per token type with greatest active_mean increase\n",
      "  top_sparsity.csv: Top 100 features per token type with greatest sparsity increase\n",
      "\n",
      "Each file contains:\n",
      "  - feature_id: SAE feature index\n",
      "  - token: Token position type (asst, endheader, newline)\n",
      "  - sparsity_base: Base model value\n",
      "  - sparsity_chat: Chat model value\n",
      "  - sparsity_diff: Difference (chat - base)\n",
      "  - sparsity_ratio: Ratio (chat / base)\n",
      "  - rank: Rank within token type (1-100)\n",
      "  - model_type, sae_layer, sae_trainer: Configuration info\n",
      "\n",
      "Total features analyzed: 131,072\n",
      "Total records per file: 300\n",
      "Files ready for further analysis!\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV files\n",
    "print(\"Saving results to CSV files...\")\n",
    "\n",
    "for metric, df in results.items():\n",
    "    # Create output filename\n",
    "    output_file = OUTPUT_DIR / f\"top_{metric}.csv\"\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv(output_file, index=False)\n",
    "    print(f\"Saved {len(df)} records to: {output_file}\")\n",
    "    \n",
    "    # Show file info\n",
    "    file_size = output_file.stat().st_size / 1024  # KB\n",
    "    print(f\"  File size: {file_size:.1f} KB\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"All results saved!\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "# Summary of what was saved\n",
    "print(f\"\\nSummary of saved files:\")\n",
    "for metric in metrics_to_analyze:\n",
    "    output_file = OUTPUT_DIR / f\"top_{metric}.csv\"\n",
    "    print(f\"  {output_file.name}: Top 100 features per token type with greatest {metric} increase\")\n",
    "\n",
    "print(f\"\\nEach file contains:\")\n",
    "print(f\"  - feature_id: SAE feature index\")\n",
    "print(f\"  - token: Token position type (asst, endheader, newline)\")\n",
    "print(f\"  - {metric}_base: Base model value\")\n",
    "print(f\"  - {metric}_chat: Chat model value\") \n",
    "print(f\"  - {metric}_diff: Difference (chat - base)\")\n",
    "print(f\"  - {metric}_ratio: Ratio (chat / base)\")\n",
    "print(f\"  - rank: Rank within token type (1-100)\")\n",
    "print(f\"  - model_type, sae_layer, sae_trainer: Configuration info\")\n",
    "\n",
    "print(f\"\\nTotal features analyzed: {base_data['metadata']['num_features']:,}\")\n",
    "print(f\"Total records per file: {len(results['all_mean'])}\")\n",
    "print(f\"Files ready for further analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create interactive Plotly scatterplot\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\n\nprint(\"Creating interactive Plotly scatterplot...\")\n\n# Create subplot with 3 panels\nfig = make_subplots(\n    rows=1, cols=3,\n    subplot_titles=['Mean All', 'Mean Active', 'Sparsity'],\n    specs=[[{\"secondary_y\": False}] * 3],\n    horizontal_spacing=0.08\n)\n\n# Color mapping for token types\ncolors = {'asst': '#FF6B6B', 'endheader': '#4ECDC4', 'newline': '#45B7D1'}\nsymbols = {'asst': 'circle', 'endheader': 'square', 'newline': 'diamond'}\n\n# Calculate unified ranges for mean_all and mean_active\nall_mean_data = results['all_mean']\nactive_mean_data = results['active_mean']\n\n# Find min and max values across both mean datasets\nunified_min = min(\n    all_mean_data['all_mean_base'].min(),\n    all_mean_data['all_mean_chat'].min(),\n    active_mean_data['active_mean_base'].min(),\n    active_mean_data['active_mean_chat'].min()\n)\n\nunified_max = max(\n    all_mean_data['all_mean_base'].max(),\n    all_mean_data['all_mean_chat'].max(),\n    active_mean_data['active_mean_base'].max(),\n    active_mean_data['active_mean_chat'].max()\n)\n\n# Add small padding\npadding = (unified_max - unified_min) * 0.05\nunified_min -= padding\nunified_max += padding\n\nfor i, metric in enumerate(['all_mean', 'active_mean', 'sparsity']):\n    df = results[metric]\n    \n    for token in df['token'].unique():\n        token_data = df[df['token'] == token]\n        \n        # Create hover text with detailed information\n        hover_text = []\n        for _, row in token_data.iterrows():\n            hover_text.append(\n                f\"<b>Feature {row['feature_id']}</b><br>\" +\n                f\"Token: {row['token']}<br>\" +\n                f\"Rank: {row['rank']}<br>\" +\n                f\"Base: {row[f'{metric}_base']:.6f}<br>\" +\n                f\"Chat: {row[f'{metric}_chat']:.6f}<br>\" +\n                f\"Difference: {row[f'{metric}_diff']:.6f}<br>\"\n            )\n        \n        fig.add_trace(\n            go.Scatter(\n                x=token_data[f'{metric}_base'],\n                y=token_data[f'{metric}_chat'],\n                mode='markers',\n                name=f'{token}' if i == 0 else f'{token}',  # Only show legend for first subplot\n                marker=dict(\n                    color=colors[token],\n                    size=8,\n                    symbol=symbols[token],\n                    line=dict(width=1, color='white')\n                ),\n                text=[f\"Feature {fid}\" for fid in token_data['feature_id']],\n                hovertemplate=hover_text,\n                showlegend=(i == 0),  # Only show legend for first subplot\n                legendgroup=token  # Group legend items\n            ),\n            row=1, col=i+1\n        )\n    \n    # Add diagonal \"no change\" line for each subplot\n    if i < 2:  # Use unified range for first two subplots (mean_all and mean_active)\n        line_min, line_max = unified_min, unified_max\n    else:  # Use original range for sparsity\n        max_val = max(df[f'{metric}_base'].max(), df[f'{metric}_chat'].max())\n        min_val = min(df[f'{metric}_base'].min(), df[f'{metric}_chat'].min())\n        line_min, line_max = min_val, max_val\n    \n    fig.add_trace(\n        go.Scatter(\n            x=[line_min, line_max],\n            y=[line_min, line_max],\n            mode='lines',\n            line=dict(color='gray', dash='dash', width=2),\n            name='No Change' if i == 0 else 'No Change',\n            showlegend=(i == 0),\n            legendgroup='nochange',\n            hovertemplate=\"No change line<extra></extra>\"\n        ),\n        row=1, col=i+1\n    )\n\n# Update layout\nfig.update_layout(\n    title={\n        'text': f'Llama 3.1 8B Base → Chat Feature Activations<br><sub>Top 100 features per activation token position</sub>',\n        'x': 0.5,\n        'xanchor': 'center',\n        'font': {'size': 16}\n    },\n    height=500,\n    width=1400,\n    showlegend=True,\n    legend=dict(\n        orientation=\"v\",\n        yanchor=\"top\",\n        y=1,\n        xanchor=\"left\",\n        x=1.02\n    ),\n    margin=dict(r=120)  # Make room for legend\n)\n\n# Update axes labels\nfig.update_xaxes(title_text=\"Base Activation\", row=1, col=2)\n\nfig.update_yaxes(title_text=\"Chat Activation\", row=1, col=1)\n\n# Set unified axis ranges for mean_all and mean_active subplots\nfig.update_xaxes(range=[unified_min, unified_max], row=1, col=1)  # Mean All\nfig.update_yaxes(range=[unified_min, unified_max], row=1, col=1)\nfig.update_xaxes(range=[unified_min, unified_max], row=1, col=2)  # Mean Active  \nfig.update_yaxes(range=[unified_min, unified_max], row=1, col=2)\n\n# Add grid\nfig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\nfig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n\n# Save the interactive plot\noutput_html = OUTPUT_DIR / \"feature_increases.html\"\nfig.write_html(output_html)\n\nprint(f\"Interactive plot saved to: {output_html}\")\nprint(f\"File size: {output_html.stat().st_size / 1024:.1f} KB\")\n\n# Show the plot\nfig.show()\n\nprint(f\"\\nPlot features:\")\nprint(f\"- 3 subplots for all_mean, active_mean, and sparsity\")\nprint(f\"- Different colors and symbols for each token type\")\nprint(f\"- Hover info shows feature details\")\nprint(f\"- Gray diagonal line shows 'no change' reference\")\nprint(f\"- Points above the line increased from base to chat\")\nprint(f\"- Interactive zoom, pan, and selection capabilities\")\nprint(f\"- Mean All and Mean Active subplots now use unified axis ranges\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}