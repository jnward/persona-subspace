{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Diffing Plotting\n",
    "\n",
    "This notebook creates an interactive plot of the results from model diffing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model data from: /workspace/results/5_diffing_personal/gemma_trainer131k-l0-114_layer20/personal_40/base.pt\n",
      "Loading chat model data from: /workspace/results/5_diffing_personal/gemma_trainer131k-l0-114_layer20/personal_40/chat.pt\n",
      "Output directory: gemma_trainer131k-l0-114_layer20/personal_40\n",
      "\n",
      "Base data keys: ['model', 'newline', 'metadata']\n",
      "Chat data keys: ['model', 'newline', 'metadata']\n",
      "Base metadata: {'source': 'gemma_trainer131k-l0-114_layer20_base', 'model_type': 'gemma', 'model_ver': 'base', 'sae_layer': 20, 'sae_trainer': '131k-l0-114', 'num_target_prompts': 40, 'num_control_prompts': 40, 'num_features': 131072, 'token_types': ['model', 'newline']}\n",
      "Chat metadata: {'source': 'gemma_trainer131k-l0-114_layer20_chat', 'model_type': 'gemma', 'model_ver': 'chat', 'sae_layer': 20, 'sae_trainer': '131k-l0-114', 'num_target_prompts': 40, 'num_control_prompts': 40, 'num_features': 131072, 'token_types': ['model', 'newline']}\n",
      "\n",
      "Base token types: ['model', 'newline']\n",
      "Chat token types: ['model', 'newline']\n",
      "Processing 2 token types: ['model', 'newline']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Configuration\n",
    "# MODEL_TYPE = \"llama\"\n",
    "# MODEL_NAME_READABLE = \"Llama 3.1 8B\"\n",
    "# SAE_LAYER = 15\n",
    "# SAE_TRAINER = \"32x\"\n",
    "# TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "# N_PROMPTS = 10000\n",
    "MODEL_TYPE = \"gemma\"\n",
    "MODEL_NAME_READABLE = \"Gemma 2 9B\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "TOKEN_OFFSETS = {\"model\": -1, \"newline\": 0}\n",
    "N_PROMPTS = 40\n",
    "PERCENT_ACTIVE = 1\n",
    "\n",
    "# Choose one metric for detailed analysis\n",
    "METRIC_SUBTITLE = {\n",
    "    'target_all_mean': 'Mean Activation',\n",
    "    'target_sparsity': 'Activation Sparsity'\n",
    "}\n",
    "\n",
    "\n",
    "# File paths\n",
    "BASE_FILE = f\"/workspace/results/5_diffing_personal/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/personal_40/base.pt\"\n",
    "CHAT_FILE = f\"/workspace/results/5_diffing_personal/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/personal_40/chat.pt\"\n",
    "EXPLANATIONS_PATH = f\"../../explanations/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}.csv\"\n",
    "\n",
    "# Output directory\n",
    "SOURCE = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/personal_40\"\n",
    "OUTPUT_DIR = Path(f\"./{SOURCE}\")\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Link\n",
    "LLAMA_LINK_FORMAT = f\"https://www.neuronpedia.org/llama3.1-8b/{SAE_LAYER}-llamascope-res-131k/\"\n",
    "GEMMA_LINK_FORMAT = f\"https://www.neuronpedia.org/gemma-2-9b/{SAE_LAYER}-gemmascope-res-131k/\"\n",
    "\n",
    "print(f\"Loading base model data from: {BASE_FILE}\")\n",
    "print(f\"Loading chat model data from: {CHAT_FILE}\")\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "\n",
    "# Load the PyTorch files\n",
    "base_data = torch.load(BASE_FILE)\n",
    "chat_data = torch.load(CHAT_FILE)\n",
    "\n",
    "print(f\"\\nBase data keys: {list(base_data.keys())}\")\n",
    "print(f\"Chat data keys: {list(chat_data.keys())}\")\n",
    "print(f\"Base metadata: {base_data['metadata']}\")\n",
    "print(f\"Chat metadata: {chat_data['metadata']}\")\n",
    "\n",
    "# Verify token types match\n",
    "base_tokens = [k for k in base_data.keys() if k != 'metadata']\n",
    "chat_tokens = [k for k in chat_data.keys() if k != 'metadata']\n",
    "print(f\"\\nBase token types: {base_tokens}\")\n",
    "print(f\"Chat token types: {chat_tokens}\")\n",
    "assert base_tokens == chat_tokens, \"Token types don't match between base and chat!\"\n",
    "\n",
    "token_types = base_tokens\n",
    "print(f\"Processing {len(token_types)} token types: {token_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 6305 explanations\n"
     ]
    }
   ],
   "source": [
    "# Create interactive scatterplot for one metric with all 3 token types\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n",
    "# Load Claude explanations\n",
    "explanations_df = pd.read_csv(EXPLANATIONS_PATH)\n",
    "print(f\"Loaded {len(explanations_df)} explanations\")\n",
    "\n",
    "# Create a dictionary for fast lookup of explanations by feature_id\n",
    "explanations_dict = dict(zip(explanations_df['feature_id'], explanations_df['claude_desc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interactive scatterplot for target_all_mean metric with all token types...\n",
      "Generated styling:\n",
      "Token type colors: {'model': '#FF6B6B', 'newline': '#4ECDC4'}\n",
      "Exclusivity symbols: {'both': 'star', 'base': 'circle', 'chat': 'square', 'neither': 'diamond'}\n",
      "Pre-processing data...\n",
      "Token type 'model': 506 target-exclusive features\n",
      "Token type 'newline': 724 target-exclusive features\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'target_all_mean'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 396\u001b[39m\n\u001b[32m    377\u001b[39m     fig.add_trace(\n\u001b[32m    378\u001b[39m         go.Scatter(\n\u001b[32m    379\u001b[39m             x=[min_val, max_val],\n\u001b[32m   (...)\u001b[39m\u001b[32m    392\u001b[39m         )\n\u001b[32m    393\u001b[39m     )\n\u001b[32m    395\u001b[39m \u001b[38;5;66;03m# Update layout with corrected metric name and square aspect ratio\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m396\u001b[39m metric_display = \u001b[43mMETRIC_SUBTITLE\u001b[49m\u001b[43m[\u001b[49m\u001b[43mSELECTED_METRIC\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    397\u001b[39m fig.update_layout(\n\u001b[32m    398\u001b[39m     title={\n\u001b[32m    399\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mBase → Instruct Introspective SAE Features: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m<br><sub>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME_READABLE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Residual Stream Post-Layer \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mSAE_LAYER\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m</sub>\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    422\u001b[39m     ),\n\u001b[32m    423\u001b[39m )\n\u001b[32m    425\u001b[39m \u001b[38;5;66;03m# Add grid\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'target_all_mean'"
     ]
    }
   ],
   "source": [
    "SELECTED_METRIC = 'target_all_mean'  # Updated to use target_ prefix\n",
    "\n",
    "print(f\"Creating interactive scatterplot for {SELECTED_METRIC} metric with all token types...\")\n",
    "\n",
    "# Generate colors for token types\n",
    "def generate_token_colors(token_offset_keys):\n",
    "    \"\"\"Generate colors for plotting based on token offset keys\"\"\"\n",
    "    color_palette = [\n",
    "        '#FF6B6B',  # Red\n",
    "        '#4ECDC4',  # Teal\n",
    "        '#45B7D1',  # Blue\n",
    "        '#96CEB4',  # Green\n",
    "        '#FFEAA7',  # Yellow\n",
    "        '#DDA0DD',  # Plum\n",
    "        '#FFA07A',  # Light Salmon\n",
    "        '#98D8C8',  # Mint\n",
    "        '#F7DC6F',  # Light Yellow\n",
    "        '#BB8FCE'   # Light Purple\n",
    "    ]\n",
    "    \n",
    "    colors = {}\n",
    "    for i, token_key in enumerate(token_offset_keys):\n",
    "        colors[token_key] = color_palette[i % len(color_palette)]\n",
    "    \n",
    "    return colors\n",
    "\n",
    "# Generate exclusivity symbols\n",
    "exclusivity_symbols = {\n",
    "    'both': 'star',      # Target-exclusive in both models\n",
    "    'base': 'circle',    # Target-exclusive in base only\n",
    "    'chat': 'square',    # Target-exclusive in chat only\n",
    "    'neither': 'diamond' # Not target-exclusive\n",
    "}\n",
    "\n",
    "# Generate styling\n",
    "colors = generate_token_colors(list(TOKEN_OFFSETS.keys()))\n",
    "print(f\"Generated styling:\")\n",
    "print(f\"Token type colors: {colors}\")\n",
    "print(f\"Exclusivity symbols: {exclusivity_symbols}\")\n",
    "\n",
    "# Load target feature IDs if the file exists\n",
    "active_file = f\"./{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/personal_40/target_features.csv\"\n",
    "if os.path.exists(active_file):\n",
    "    active_features_df = pd.read_csv(active_file)\n",
    "    active_feature_ids = set(active_features_df['feature_id'].tolist())\n",
    "else:\n",
    "    # Pre-calculate active masks once per token type since all metrics have same active features\n",
    "    print(\"Pre-calculating active masks...\")\n",
    "    active_masks = {}\n",
    "    for token_type in token_types:\n",
    "        base_values = base_data[token_type]['target_num_active'].numpy()\n",
    "        chat_values = chat_data[token_type]['target_num_active'].numpy()\n",
    "        active_masks[token_type] = (base_values > int(N_PROMPTS * PERCENT_ACTIVE / 100)) | (chat_values > int(N_PROMPTS * PERCENT_ACTIVE / 100))\n",
    "        print(f\"  {token_type}: {active_masks[token_type].sum():,} active features\")\n",
    "\n",
    "# Pre-process all data in single loop to avoid redundant calculations\n",
    "print(\"Pre-processing data...\")\n",
    "processed_data = {}\n",
    "all_base_values = []\n",
    "all_chat_values = []\n",
    "\n",
    "def format_explanation_efficient(claude_explanation):\n",
    "    \"\"\"Efficient text wrapping function - preserves original formatting logic\"\"\"\n",
    "    if not isinstance(claude_explanation, str) or pd.isna(claude_explanation):\n",
    "        return \"No explanation available\"\n",
    "    \n",
    "    if len(claude_explanation) <= 50:\n",
    "        return claude_explanation\n",
    "    \n",
    "    # Same text wrapping logic as original, but more efficient\n",
    "    words = claude_explanation.split()\n",
    "    lines = []\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        if len(current_line + word) <= 80:\n",
    "            current_line += word + \" \"\n",
    "        else:\n",
    "            if current_line:\n",
    "                lines.append(current_line.strip())\n",
    "            current_line = word + \" \"\n",
    "    \n",
    "    if current_line:\n",
    "        lines.append(current_line.strip())\n",
    "    \n",
    "    return \"<br>\".join(lines)\n",
    "\n",
    "def get_exclusivity_info(fid, token_type):\n",
    "    \"\"\"Determine if feature is target-exclusive for base, chat, or both\"\"\"\n",
    "    base_target_active = base_data[token_type]['target_num_active'][fid] > 0\n",
    "    base_control_active = base_data[token_type]['control_num_active'][fid] > 0\n",
    "    chat_target_active = chat_data[token_type]['target_num_active'][fid] > 0\n",
    "    chat_control_active = chat_data[token_type]['control_num_active'][fid] > 0\n",
    "    \n",
    "    base_exclusive = base_target_active and not base_control_active\n",
    "    chat_exclusive = chat_target_active and not chat_control_active\n",
    "    \n",
    "    if base_exclusive and chat_exclusive:\n",
    "        return \"both\"\n",
    "    elif base_exclusive and not chat_exclusive:\n",
    "        return \"base\"\n",
    "    elif chat_exclusive and not base_exclusive:\n",
    "        return \"chat\"\n",
    "    else:\n",
    "        return \"neither\"\n",
    "\n",
    "def has_cross_model_inconsistency(fid, token_type):\n",
    "    \"\"\"Check if feature has cross-model inconsistency\"\"\"\n",
    "    base_target_active = base_data[token_type]['target_num_active'][fid] > 0\n",
    "    base_control_active = base_data[token_type]['control_num_active'][fid] > 0\n",
    "    chat_target_active = chat_data[token_type]['target_num_active'][fid] > 0\n",
    "    chat_control_active = chat_data[token_type]['control_num_active'][fid] > 0\n",
    "    \n",
    "    base_target_only = base_target_active and not base_control_active\n",
    "    chat_target_only = chat_target_active and not chat_control_active\n",
    "    \n",
    "    # Check if base-target-only but chat-control-active\n",
    "    base_target_only_but_chat_control = base_target_only and chat_control_active\n",
    "    \n",
    "    # Check if chat-target-only but base-control-active  \n",
    "    chat_target_only_but_base_control = chat_target_only and base_control_active\n",
    "    \n",
    "    return base_target_only_but_chat_control or chat_target_only_but_base_control\n",
    "\n",
    "def get_cross_model_inconsistency_text(fid, token_type):\n",
    "    \"\"\"Get description of cross-model inconsistency if any\"\"\"\n",
    "    base_target_active = base_data[token_type]['target_num_active'][fid] > 0\n",
    "    base_control_active = base_data[token_type]['control_num_active'][fid] > 0\n",
    "    chat_target_active = chat_data[token_type]['target_num_active'][fid] > 0\n",
    "    chat_control_active = chat_data[token_type]['control_num_active'][fid] > 0\n",
    "    \n",
    "    base_target_only = base_target_active and not base_control_active\n",
    "    chat_target_only = chat_target_active and not chat_control_active\n",
    "    \n",
    "    if base_target_only and chat_control_active:\n",
    "        return \"Cross-model inconsistency: Target-only in base but also active in chat control\"\n",
    "    elif chat_target_only and base_control_active:\n",
    "        return \"Cross-model inconsistency: Target-only in chat but also active in base control\"\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_detailed_activation_counts(fid, token_type):\n",
    "    \"\"\"Get detailed activation counts for hover text\"\"\"\n",
    "    base_target_count = base_data[token_type]['target_num_active'][fid].item()\n",
    "    base_control_count = base_data[token_type]['control_num_active'][fid].item()\n",
    "    chat_target_count = chat_data[token_type]['target_num_active'][fid].item()\n",
    "    chat_control_count = chat_data[token_type]['control_num_active'][fid].item()\n",
    "    \n",
    "    return base_target_count, base_control_count, chat_target_count, chat_control_count\n",
    "\n",
    "# Process all token types in one pass\n",
    "no_explanation = set()\n",
    "for token_type in token_types:\n",
    "    if os.path.exists(active_file):\n",
    "        # Use pre-filtered feature IDs from CSV\n",
    "        all_feature_ids = np.arange(base_data['metadata']['num_features'])\n",
    "        mask = np.isin(all_feature_ids, list(active_feature_ids))\n",
    "\n",
    "        base_values = base_data[token_type][SELECTED_METRIC].numpy()[mask]\n",
    "        chat_values = chat_data[token_type][SELECTED_METRIC].numpy()[mask]\n",
    "        feature_ids = all_feature_ids[mask]\n",
    "    else:\n",
    "        # Fallback to mask calculation\n",
    "        active_mask = active_masks[token_type]\n",
    "        base_values = base_data[token_type][SELECTED_METRIC].numpy()[active_mask]\n",
    "        chat_values = chat_data[token_type][SELECTED_METRIC].numpy()[active_mask]\n",
    "        feature_ids = np.arange(len(active_mask))[active_mask]\n",
    "    \n",
    "    # Calculate differences vectorized\n",
    "    differences = chat_values - base_values\n",
    "    \n",
    "    # Pre-process hover text and explanations - filter out non-exclusive features\n",
    "    hover_texts = []\n",
    "    neuronpedia_urls = []\n",
    "    exclusivity_info = []\n",
    "    inconsistency_info = []\n",
    "    filtered_base_values = []\n",
    "    filtered_chat_values = []\n",
    "    filtered_feature_ids = []\n",
    "    \n",
    "    for fid, base_val, chat_val, diff in zip(feature_ids, base_values, chat_values, differences):\n",
    "        # Get exclusivity information\n",
    "        exclusivity = get_exclusivity_info(fid, token_type)\n",
    "        \n",
    "        # Skip features that are not target-exclusive for this token type\n",
    "        if exclusivity == \"neither\":\n",
    "            continue\n",
    "            \n",
    "        exclusivity_info.append(exclusivity)\n",
    "        \n",
    "        # Check for cross-model inconsistency\n",
    "        has_inconsistency = has_cross_model_inconsistency(fid, token_type)\n",
    "        inconsistency_info.append(has_inconsistency)\n",
    "        \n",
    "        filtered_base_values.append(base_val)\n",
    "        filtered_chat_values.append(chat_val)\n",
    "        filtered_feature_ids.append(fid)\n",
    "        \n",
    "        # Get detailed activation counts\n",
    "        base_target_count, base_control_count, chat_target_count, chat_control_count = get_detailed_activation_counts(fid, token_type)\n",
    "        \n",
    "        # Get total prompt counts from metadata\n",
    "        num_target_prompts = base_data['metadata']['num_target_prompts']\n",
    "        num_control_prompts = base_data['metadata']['num_control_prompts']\n",
    "        \n",
    "        # Get Claude explanation if available\n",
    "        if fid not in explanations_dict:\n",
    "            no_explanation.add(fid)\n",
    "        claude_explanation = explanations_dict.get(fid, \"No explanation available\")\n",
    "        \n",
    "        # Check if explanation is a string (handle NaN/float values)\n",
    "        if not isinstance(claude_explanation, str) or pd.isna(claude_explanation):\n",
    "            claude_explanation = \"No explanation available\"\n",
    "            no_explanation.add(fid)\n",
    "        \n",
    "        # Format explanation efficiently but preserve original wrapping behavior\n",
    "        formatted_explanation = format_explanation_efficient(claude_explanation)\n",
    "        \n",
    "        # Create Neuronpedia URL - choose correct format based on model type\n",
    "        if MODEL_TYPE == \"llama\":\n",
    "            neuronpedia_url = f\"{LLAMA_LINK_FORMAT}{fid}\"\n",
    "        else:  # gemma\n",
    "            neuronpedia_url = f\"{GEMMA_LINK_FORMAT}{fid}\"\n",
    "        neuronpedia_urls.append(neuronpedia_url)\n",
    "        \n",
    "        # Create hover text with detailed activation percentages\n",
    "        exclusivity_text = {\n",
    "            \"both\": \"Introspective prompt feature for both models\",\n",
    "            \"base\": \"Base-only introspective prompt feature\",\n",
    "            \"chat\": \"Chat-only introspective prompt feature\",\n",
    "        }[exclusivity]\n",
    "        \n",
    "        # Calculate percentages\n",
    "        base_target_pct = (base_target_count / num_target_prompts) * 100\n",
    "        base_control_pct = (base_control_count / num_control_prompts) * 100\n",
    "        chat_target_pct = (chat_target_count / num_target_prompts) * 100\n",
    "        chat_control_pct = (chat_control_count / num_control_prompts) * 100\n",
    "        \n",
    "        # Get cross-model inconsistency text\n",
    "        inconsistency_text = get_cross_model_inconsistency_text(fid, token_type)\n",
    "        \n",
    "        hover_text = (\n",
    "            f\"<b>Feature {fid}</b><br>\" +\n",
    "            f\"Base: {base_val:.4f}, Chat: {chat_val:.4f}<br>\" +\n",
    "            f\"Difference: {diff:.4f}<br><br>\" +\n",
    "            f\"<b>Activation Percentages:</b><br>\" +\n",
    "            f\"Base Target: {base_target_pct:.1f}% ({base_target_count}/{num_target_prompts} prompts)<br>\" +\n",
    "            f\"Base Control: {base_control_pct:.1f}% ({base_control_count}/{num_control_prompts} prompts)<br>\" +\n",
    "            f\"Chat Target: {chat_target_pct:.1f}% ({chat_target_count}/{num_target_prompts} prompts)<br>\" +\n",
    "            f\"Chat Control: {chat_control_pct:.1f}% ({chat_control_count}/{num_control_prompts} prompts)<br><br>\" +\n",
    "            f\"<b>Exclusivity:</b> {exclusivity_text}<br>\"\n",
    "        )\n",
    "        \n",
    "        # Add inconsistency info if present\n",
    "        if inconsistency_text:\n",
    "            hover_text += f\"<br><b>Note:</b> {inconsistency_text}<br>\"\n",
    "        \n",
    "        hover_text += f\"<br><b>Description:</b><br>{formatted_explanation}<extra></extra>\"\n",
    "        \n",
    "        hover_texts.append(hover_text)\n",
    "    \n",
    "    # Store processed data with filtered values\n",
    "    processed_data[token_type] = {\n",
    "        'base_values': filtered_base_values,\n",
    "        'chat_values': filtered_chat_values,\n",
    "        'feature_ids': filtered_feature_ids,\n",
    "        'hover_texts': hover_texts,\n",
    "        'neuronpedia_urls': neuronpedia_urls,\n",
    "        'exclusivity_info': exclusivity_info,\n",
    "        'inconsistency_info': inconsistency_info\n",
    "    }\n",
    "    \n",
    "    # Collect all values for min/max calculation (single pass)\n",
    "    all_base_values.extend(filtered_base_values)\n",
    "    all_chat_values.extend(filtered_chat_values)\n",
    "    \n",
    "    print(f\"Token type '{token_type}': {len(filtered_feature_ids)} target-exclusive features\")\n",
    "\n",
    "# Create the scatterplot\n",
    "fig = go.Figure()\n",
    "\n",
    "total_features = 0\n",
    "\n",
    "# Add scatter traces using pre-processed data, grouped by token type and exclusivity\n",
    "exclusivity_groups = ['both', 'base', 'chat']  # Removed 'neither' since we filter those out\n",
    "exclusivity_names = {\n",
    "    'both': 'Both',\n",
    "    'base': 'Base', \n",
    "    'chat': 'Chat'\n",
    "}\n",
    "\n",
    "for token_type in token_types:\n",
    "    data = processed_data[token_type]\n",
    "    \n",
    "    # Group data by exclusivity type\n",
    "    for exclusivity_type in exclusivity_groups:\n",
    "        # Filter data for this exclusivity type\n",
    "        indices = [i for i, excl in enumerate(data['exclusivity_info']) if excl == exclusivity_type]\n",
    "        \n",
    "        if not indices:  # Skip if no features of this type\n",
    "            continue\n",
    "        \n",
    "        # Create trace name with exclusivity pattern (indented for visual grouping)\n",
    "        trace_name = f\"  {exclusivity_names[exclusivity_type]}\"\n",
    "        \n",
    "        # Separate indices by inconsistency status for different styling\n",
    "        consistent_indices = [i for i in indices if not data['inconsistency_info'][i]]\n",
    "        inconsistent_indices = [i for i in indices if data['inconsistency_info'][i]]\n",
    "        \n",
    "        # Add trace for consistent features\n",
    "        if consistent_indices:\n",
    "            fig.add_trace(\n",
    "                go.Scattergl(\n",
    "                    x=[data['base_values'][i] for i in consistent_indices],\n",
    "                    y=[data['chat_values'][i] for i in consistent_indices],\n",
    "                    mode='markers',\n",
    "                    name=trace_name,\n",
    "                    legendgroup=token_type,  # Group by token type\n",
    "                    legendgrouptitle_text=token_type.title(),  # Group title\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=colors[token_type],\n",
    "                        symbol=exclusivity_symbols[exclusivity_type],\n",
    "                        line=dict(width=0.3, color='black'),\n",
    "                        opacity=0.7\n",
    "                    ),\n",
    "                    text=[f\"Feature {data['feature_ids'][i]}\" for i in consistent_indices],\n",
    "                    customdata=[data['neuronpedia_urls'][i] for i in consistent_indices],\n",
    "                    hovertemplate=[data['hover_texts'][i] for i in consistent_indices],\n",
    "                    hoverlabel=dict(\n",
    "                        bgcolor=colors[token_type],\n",
    "                        bordercolor=\"black\",\n",
    "                        font_size=12, \n",
    "                        font_family=\"Arial\",\n",
    "                        font_color=\"white\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Add trace for inconsistent features with orange border\n",
    "        if inconsistent_indices:\n",
    "            fig.add_trace(\n",
    "                go.Scattergl(\n",
    "                    x=[data['base_values'][i] for i in inconsistent_indices],\n",
    "                    y=[data['chat_values'][i] for i in inconsistent_indices],\n",
    "                    mode='markers',\n",
    "                    name=trace_name + \" (Cross-model)\",\n",
    "                    legendgroup=token_type,  # Group by token type\n",
    "                    legendgrouptitle_text=token_type.title(),  # Group title\n",
    "                    marker=dict(\n",
    "                        size=6,\n",
    "                        color=colors[token_type],\n",
    "                        symbol=exclusivity_symbols[exclusivity_type],\n",
    "                        line=dict(width=1, color='red'),\n",
    "                        opacity=0.7\n",
    "                    ),\n",
    "                    text=[f\"Feature {data['feature_ids'][i]}\" for i in inconsistent_indices],\n",
    "                    customdata=[data['neuronpedia_urls'][i] for i in inconsistent_indices],\n",
    "                    hovertemplate=[data['hover_texts'][i] for i in inconsistent_indices],\n",
    "                    hoverlabel=dict(\n",
    "                        bgcolor=colors[token_type],\n",
    "                        bordercolor=\"black\",\n",
    "                        font_size=12, \n",
    "                        font_family=\"Arial\",\n",
    "                        font_color=\"white\"\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        total_features += len(indices)\n",
    "\n",
    "# Add diagonal \"no change\" line using pre-calculated values\n",
    "if all_base_values and all_chat_values:  # Only add if we have data\n",
    "    max_val = max(max(all_base_values), max(all_chat_values))\n",
    "    min_val = min(min(all_base_values), min(all_chat_values))\n",
    "\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[min_val, max_val],\n",
    "            y=[min_val, max_val],\n",
    "            mode='lines',\n",
    "            line=dict(color='gray', dash='dash', width=2),\n",
    "            name='No Change',\n",
    "            hovertemplate=\"No change line<extra></extra>\",\n",
    "            hoverlabel=dict(\n",
    "                bgcolor=\"gray\",\n",
    "                bordercolor=\"black\",\n",
    "                font_size=11,\n",
    "                font_family=\"Arial\",\n",
    "                font_color=\"white\"\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Update layout with corrected metric name and square aspect ratio\n",
    "metric_display = METRIC_SUBTITLE[SELECTED_METRIC]\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f'Base → Instruct Introspective SAE Features: {metric_display}<br><sub>{MODEL_NAME_READABLE}, Residual Stream Post-Layer {SAE_LAYER}</sub>',\n",
    "        'x': 0.5,\n",
    "        'xanchor': 'center',\n",
    "        'font': {'size': 16}\n",
    "    },\n",
    "    xaxis_title=f'Base: {metric_display}',\n",
    "    yaxis_title=f'Instruct: {metric_display}',\n",
    "    height=800,\n",
    "    width=900,\n",
    "    showlegend=True,\n",
    "    hovermode='closest',\n",
    "    legend=dict(\n",
    "        title=\"Activation Position\",\n",
    "        orientation=\"v\",\n",
    "        yanchor=\"top\",\n",
    "        y=1,\n",
    "        xanchor=\"left\",\n",
    "        x=1.02,\n",
    "        groupclick=\"togglegroup\"  # Allow group toggling by clicking group titles\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        scaleanchor=\"y\",  # Lock x-axis scale to y-axis\n",
    "        scaleratio=1,     # 1:1 aspect ratio for square grid\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Add grid\n",
    "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='lightgray')\n",
    "\n",
    "# Save the interactive plot\n",
    "output_html = OUTPUT_DIR / f\"{SELECTED_METRIC}.html\"\n",
    "\n",
    "# Create custom HTML with click handler\n",
    "html_content = fig.to_html(\n",
    "    include_plotlyjs='cdn',\n",
    "    config={\n",
    "        'displayModeBar': True,\n",
    "        'showTips': False,\n",
    "        'scrollZoom': True,\n",
    "        'doubleClick': 'reset'\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add JavaScript to handle clicks\n",
    "click_script = \"\"\"\n",
    "<script>\n",
    "document.addEventListener('DOMContentLoaded', function() {\n",
    "    var plotDiv = document.getElementsByClassName('plotly-graph-div')[0];\n",
    "    \n",
    "    let clickTimeout;\n",
    "    plotDiv.on('plotly_click', function(data) {\n",
    "        clearTimeout(clickTimeout);\n",
    "        clickTimeout = setTimeout(function() {\n",
    "            var point = data.points[0];\n",
    "            if (point.customdata) {\n",
    "                window.open(point.customdata, '_blank');\n",
    "            }\n",
    "        }, 100);\n",
    "    });\n",
    "});\n",
    "</script>\n",
    "\"\"\"\n",
    "\n",
    "html_with_script = html_content.replace('</body>', click_script + '</body>')\n",
    "\n",
    "with open(output_html, 'w') as f:\n",
    "    f.write(html_with_script)\n",
    "\n",
    "print(f\"\\nInteractive scatterplot saved to: {output_html}\")\n",
    "print(f\"File size: {output_html.stat().st_size / 1024:.1f} KB\")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cross-model feature patterns\n",
    "print(\"Analyzing cross-model feature patterns...\")\n",
    "\n",
    "def analyze_cross_model_patterns():\n",
    "    \"\"\"Analyze features that are target-only in one model but appear in control for the other\"\"\"\n",
    "    \n",
    "    cross_model_patterns = {}\n",
    "    \n",
    "    for token_type in token_types:\n",
    "        print(f\"\\nAnalyzing token type: {token_type}\")\n",
    "        \n",
    "        # Get all features that are active in either model\n",
    "        base_target_active = base_data[token_type]['target_num_active'] > 0\n",
    "        base_control_active = base_data[token_type]['control_num_active'] > 0\n",
    "        chat_target_active = chat_data[token_type]['target_num_active'] > 0\n",
    "        chat_control_active = chat_data[token_type]['control_num_active'] > 0\n",
    "        \n",
    "        # Features that are target-only in base but appear in chat control\n",
    "        base_target_only = base_target_active & ~base_control_active\n",
    "        chat_control_active_mask = chat_control_active\n",
    "        base_target_only_but_chat_control = base_target_only & chat_control_active_mask\n",
    "        \n",
    "        # Features that are target-only in chat but appear in base control\n",
    "        chat_target_only = chat_target_active & ~chat_control_active\n",
    "        base_control_active_mask = base_control_active\n",
    "        chat_target_only_but_base_control = chat_target_only & base_control_active_mask\n",
    "        \n",
    "        # Get feature IDs\n",
    "        base_target_only_but_chat_control_ids = torch.where(base_target_only_but_chat_control)[0].tolist()\n",
    "        chat_target_only_but_base_control_ids = torch.where(chat_target_only_but_base_control)[0].tolist()\n",
    "        \n",
    "        cross_model_patterns[token_type] = {\n",
    "            'base_target_only_but_chat_control': base_target_only_but_chat_control_ids,\n",
    "            'chat_target_only_but_base_control': chat_target_only_but_base_control_ids\n",
    "        }\n",
    "        \n",
    "        print(f\"  Features target-only in base but active in chat control: {len(base_target_only_but_chat_control_ids)}\")\n",
    "        print(f\"  Features target-only in chat but active in base control: {len(chat_target_only_but_base_control_ids)}\")\n",
    "        \n",
    "        # Show examples with explanations if available\n",
    "        if base_target_only_but_chat_control_ids:\n",
    "            print(f\"  Examples of base-target-only but chat-control features:\")\n",
    "            for fid in base_target_only_but_chat_control_ids[:3]:  # Show first 3\n",
    "                explanation = explanations_dict.get(fid, \"No explanation available\")\n",
    "                if isinstance(explanation, str) and not pd.isna(explanation):\n",
    "                    explanation = explanation[:100] + \"...\" if len(explanation) > 100 else explanation\n",
    "                else:\n",
    "                    explanation = \"No explanation available\"\n",
    "                print(f\"    Feature {fid}: {explanation}\")\n",
    "        \n",
    "        if chat_target_only_but_base_control_ids:\n",
    "            print(f\"  Examples of chat-target-only but base-control features:\")\n",
    "            for fid in chat_target_only_but_base_control_ids[:3]:  # Show first 3\n",
    "                explanation = explanations_dict.get(fid, \"No explanation available\")\n",
    "                if isinstance(explanation, str) and not pd.isna(explanation):\n",
    "                    explanation = explanation[:100] + \"...\" if len(explanation) > 100 else explanation\n",
    "                else:\n",
    "                    explanation = \"No explanation available\"\n",
    "                print(f\"    Feature {fid}: {explanation}\")\n",
    "    \n",
    "    return cross_model_patterns\n",
    "\n",
    "cross_patterns = analyze_cross_model_patterns()\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"CROSS-MODEL PATTERN SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "total_base_target_only_but_chat_control = 0\n",
    "total_chat_target_only_but_base_control = 0\n",
    "\n",
    "for token_type in token_types:\n",
    "    patterns = cross_patterns[token_type]\n",
    "    base_count = len(patterns['base_target_only_but_chat_control'])\n",
    "    chat_count = len(patterns['chat_target_only_but_base_control'])\n",
    "    \n",
    "    total_base_target_only_but_chat_control += base_count\n",
    "    total_chat_target_only_but_base_control += chat_count\n",
    "    \n",
    "    print(f\"{token_type.upper()}:\")\n",
    "    print(f\"  Base-target-only → Chat-control: {base_count} features\")\n",
    "    print(f\"  Chat-target-only → Base-control: {chat_count} features\")\n",
    "\n",
    "print(f\"\\nTOTAL ACROSS ALL TOKEN TYPES:\")\n",
    "print(f\"  Base-target-only → Chat-control: {total_base_target_only_but_chat_control} features\")\n",
    "print(f\"  Chat-target-only → Base-control: {total_chat_target_only_but_base_control} features\")\n",
    "\n",
    "if total_base_target_only_but_chat_control == 0 and total_chat_target_only_but_base_control == 0:\n",
    "    print(\"\\n✅ No cross-model inconsistencies found!\")\n",
    "    print(\"All target-exclusive features are consistently exclusive across both models.\")\n",
    "else:\n",
    "    print(f\"\\n⚠️  Found {total_base_target_only_but_chat_control + total_chat_target_only_but_base_control} cross-model inconsistencies\")\n",
    "    print(\"These features show different activation patterns between base and chat models.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
