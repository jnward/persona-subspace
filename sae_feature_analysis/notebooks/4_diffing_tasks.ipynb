{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Diffing Task Categories\n",
    "\n",
    "This notebook analyzes which SAE features differ between coding and medical task prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sae_lens import SAE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  Tokenizer (chat): google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Available token types: ['model', 'newline']\n",
      "  Assistant header: <start_of_turn>model\n",
      "  Output files: /workspace/results/4_diffing_tasks/gemma_trainer131k-l0-114_layer20/medical.pt, /workspace/results/4_diffing_tasks/gemma_trainer131k-l0-114_layer20/code.pt\n"
     ]
    }
   ],
   "source": [
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Configure for task category diffing\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = f\"/workspace/results/4_diffing_tasks/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "MEDICAL_OUTPUT_FILE = f\"{OUTPUT_DIR}/medical.pt\"\n",
    "CODE_OUTPUT_FILE = f\"{OUTPUT_DIR}/code.pt\"\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  Tokenizer (chat): {CHAT_MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Available token types: {list(TOKEN_OFFSETS.keys())}\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output files: {MEDICAL_OUTPUT_FILE}, {CODE_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task Category Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 coding prompts\n",
      "Loaded 4 medical prompts\n",
      "Total prompts to process: 8\n"
     ]
    }
   ],
   "source": [
    "# Load prompts from different task categories\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer (from chat model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3fa63f68ed4f7786f7bf182f396eff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    sae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(sae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(sae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, sparsity = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\"\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path, sparsity)\n",
    "    return sae\n",
    "\n",
    "# Load SAE\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return int(assistant_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_activations_and_metadata(prompts: List[str], layer_idx: int) -> Tuple[torch.Tensor, List[Dict], List[str]]:\n",
    "    \"\"\"Extract activations and prepare metadata for all prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    all_metadata = []\n",
    "    formatted_prompts_list = []\n",
    "    \n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        formatted_prompts_list.extend(formatted_prompts)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Process each prompt in the batch\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Calculate positions for all token types\n",
    "            positions = {}\n",
    "            for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "                positions[token_type] = find_assistant_position(\n",
    "                    input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "                )\n",
    "            \n",
    "            all_activations.append(activations[j].cpu())\n",
    "            all_metadata.append({\n",
    "                'prompt_idx': i + j,\n",
    "                'positions': positions,\n",
    "                'attention_mask': attention_mask.cpu(),\n",
    "                'input_ids': input_ids.cpu()\n",
    "            })\n",
    "    \n",
    "    # Pad activations to same length\n",
    "    max_seq_len = max(act.shape[0] for act in all_activations)\n",
    "    hidden_dim = all_activations[0].shape[1]\n",
    "    \n",
    "    padded_activations = []\n",
    "    for act in all_activations:\n",
    "        if act.shape[0] < max_seq_len:\n",
    "            padding = torch.zeros(max_seq_len - act.shape[0], hidden_dim)\n",
    "            padded_act = torch.cat([act, padding], dim=0)\n",
    "        else:\n",
    "            padded_act = act\n",
    "        padded_activations.append(padded_act)\n",
    "    \n",
    "    return torch.stack(padded_activations, dim=0), all_metadata, formatted_prompts_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def extract_token_activations(full_activations: torch.Tensor, metadata: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extract activations for specific token positions.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = []\n",
    "    \n",
    "    for i, meta in enumerate(metadata):\n",
    "        for token_type, position in meta['positions'].items():\n",
    "            activation = full_activations[i, position, :]\n",
    "            results[token_type].append(activation)\n",
    "    \n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = torch.stack(results[token_type], dim=0)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAE Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features_batched(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations with proper batching.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pt_cpu(token_features, category_name: str):\n",
    "    \"\"\"Save results as PyTorch tensors using CPU computation.\"\"\"\n",
    "    source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{category_name}\"\n",
    "    \n",
    "    print(f\"Processing results for PyTorch format using CPU, source: {source_name}\")\n",
    "    \n",
    "    results_dict = {}\n",
    "    \n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        print(f\"\\nProcessing token type: {token_type}\")\n",
    "        \n",
    "        features_tensor = token_features[token_type].float()\n",
    "        \n",
    "        print(f\"Processing all {features_tensor.shape[1]} features for token_type='{token_type}' on CPU\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        all_mean = features_tensor.mean(dim=0)\n",
    "        all_std = features_tensor.std(dim=0)\n",
    "        max_vals = features_tensor.max(dim=0)[0]\n",
    "        \n",
    "        # Active statistics (only non-zero values)\n",
    "        active_mask = features_tensor > 0\n",
    "        num_active = active_mask.sum(dim=0)\n",
    "        sparsity = num_active.float() / features_tensor.shape[0]\n",
    "        \n",
    "        results_dict[token_type] = {\n",
    "            'all_mean': all_mean,\n",
    "            'all_std': all_std,\n",
    "            'max': max_vals,\n",
    "            'num_active': num_active,\n",
    "            'sparsity': sparsity,\n",
    "        }\n",
    "        \n",
    "        print(f\"Processed all {features_tensor.shape[1]} features for token_type='{token_type}'\")\n",
    "    \n",
    "    # Add metadata\n",
    "    results_dict['metadata'] = {\n",
    "        'source': source_name,\n",
    "        'model_type': MODEL_TYPE,\n",
    "        'category': category_name,\n",
    "        'sae_layer': SAE_LAYER,\n",
    "        'sae_trainer': SAE_TRAINER,\n",
    "        'num_prompts': features_tensor.shape[0],\n",
    "        'num_features': features_tensor.shape[1],\n",
    "        'token_types': list(TOKEN_OFFSETS.keys())\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTotal token types processed: {len(results_dict) - 1}\")\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Coding Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING CODING PROMPTS\n",
      "============================================================\n",
      "Extracting activations for all positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36093b0c65384bb3a9facc28e1efd259",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coding full activations shape: torch.Size([4, 21, 3584])\n",
      "\n",
      "Extracting activations for specific token positions...\n",
      "Token type 'model' activations shape: torch.Size([4, 3584])\n",
      "Token type 'newline' activations shape: torch.Size([4, 3584])\n",
      "\n",
      "Computing SAE features for specific token positions...\n",
      "Processing SAE features for token type 'model'...\n",
      "Features shape for 'model': torch.Size([4, 131072])\n",
      "Processing SAE features for token type 'newline'...\n",
      "Features shape for 'newline': torch.Size([4, 131072])\n",
      "\n",
      "Completed SAE feature extraction for coding prompts\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING CODING PROMPTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract activations for coding prompts\n",
    "print(\"Extracting activations for all positions...\")\n",
    "coding_full_activations, coding_metadata, coding_formatted_prompts = extract_activations_and_metadata(\n",
    "    coding_prompts, LAYER_INDEX\n",
    ")\n",
    "print(f\"Coding full activations shape: {coding_full_activations.shape}\")\n",
    "\n",
    "# Extract activations for specific token positions\n",
    "print(\"\\nExtracting activations for specific token positions...\")\n",
    "coding_token_activations = extract_token_activations(coding_full_activations, coding_metadata)\n",
    "\n",
    "for token_type, activations in coding_token_activations.items():\n",
    "    print(f\"Token type '{token_type}' activations shape: {activations.shape}\")\n",
    "\n",
    "# Get SAE features for coding prompts\n",
    "print(\"\\nComputing SAE features for specific token positions...\")\n",
    "coding_token_features = {}\n",
    "for token_type, activations in coding_token_activations.items():\n",
    "    print(f\"Processing SAE features for token type '{token_type}'...\")\n",
    "    features = get_sae_features_batched(activations)\n",
    "    coding_token_features[token_type] = features\n",
    "    print(f\"Features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "print(f\"\\nCompleted SAE feature extraction for coding prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Medical Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PROCESSING MEDICAL PROMPTS\n",
      "============================================================\n",
      "Extracting activations for all positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36fc7bfa19dd4ceb889e6664af968ac4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medical full activations shape: torch.Size([4, 20, 3584])\n",
      "\n",
      "Extracting activations for specific token positions...\n",
      "Token type 'model' activations shape: torch.Size([4, 3584])\n",
      "Token type 'newline' activations shape: torch.Size([4, 3584])\n",
      "\n",
      "Computing SAE features for specific token positions...\n",
      "Processing SAE features for token type 'model'...\n",
      "Features shape for 'model': torch.Size([4, 131072])\n",
      "Processing SAE features for token type 'newline'...\n",
      "Features shape for 'newline': torch.Size([4, 131072])\n",
      "\n",
      "Completed SAE feature extraction for medical prompts\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROCESSING MEDICAL PROMPTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Extract activations for medical prompts\n",
    "print(\"Extracting activations for all positions...\")\n",
    "medical_full_activations, medical_metadata, medical_formatted_prompts = extract_activations_and_metadata(\n",
    "    medical_prompts, LAYER_INDEX\n",
    ")\n",
    "print(f\"Medical full activations shape: {medical_full_activations.shape}\")\n",
    "\n",
    "# Extract activations for specific token positions\n",
    "print(\"\\nExtracting activations for specific token positions...\")\n",
    "medical_token_activations = extract_token_activations(medical_full_activations, medical_metadata)\n",
    "\n",
    "for token_type, activations in medical_token_activations.items():\n",
    "    print(f\"Token type '{token_type}' activations shape: {activations.shape}\")\n",
    "\n",
    "# Get SAE features for medical prompts\n",
    "print(\"\\nComputing SAE features for specific token positions...\")\n",
    "medical_token_features = {}\n",
    "for token_type, activations in medical_token_activations.items():\n",
    "    print(f\"Processing SAE features for token type '{token_type}'...\")\n",
    "    features = get_sae_features_batched(activations)\n",
    "    medical_token_features[token_type] = features\n",
    "    print(f\"Features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "print(f\"\\nCompleted SAE feature extraction for medical prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING CODING RESULTS\n",
      "============================================================\n",
      "Processing results for PyTorch format using CPU, source: gemma_trainer131k-l0-114_layer20_code\n",
      "\n",
      "Processing token type: model\n",
      "Processing all 131072 features for token_type='model' on CPU\n",
      "Processed all 131072 features for token_type='model'\n",
      "\n",
      "Processing token type: newline\n",
      "Processing all 131072 features for token_type='newline' on CPU\n",
      "Processed all 131072 features for token_type='newline'\n",
      "\n",
      "Total token types processed: 2\n",
      "\n",
      "Coding results saved to: /workspace/results/4_diffing_tasks/gemma_trainer131k-l0-114_layer20/code.pt\n",
      "\n",
      "Coding file structure:\n",
      "Keys: ['model', 'newline', 'metadata']\n",
      "Metadata: {'source': 'gemma_trainer131k-l0-114_layer20_code', 'model_type': 'gemma', 'category': 'code', 'sae_layer': 20, 'sae_trainer': '131k-l0-114', 'num_prompts': 4, 'num_features': 131072, 'token_types': ['model', 'newline']}\n",
      "\n",
      "model statistics shapes:\n",
      "  all_mean: torch.Size([131072])\n",
      "  all_std: torch.Size([131072])\n",
      "  max: torch.Size([131072])\n",
      "  num_active: torch.Size([131072])\n",
      "  sparsity: torch.Size([131072])\n",
      "\n",
      "model sample statistics:\n",
      "  all_mean - min: 0.000000, max: 245.705063\n",
      "  sparsity - min: 0.000000, max: 1.000000\n",
      "  num_active - min: 0, max: 4\n",
      "\n",
      "newline statistics shapes:\n",
      "  all_mean: torch.Size([131072])\n",
      "  all_std: torch.Size([131072])\n",
      "  max: torch.Size([131072])\n",
      "  num_active: torch.Size([131072])\n",
      "  sparsity: torch.Size([131072])\n",
      "\n",
      "newline sample statistics:\n",
      "  all_mean - min: 0.000000, max: 108.080238\n",
      "  sparsity - min: 0.000000, max: 1.000000\n",
      "  num_active - min: 0, max: 4\n"
     ]
    }
   ],
   "source": [
    "# Process and save coding results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING CODING RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "coding_results_dict = save_as_pt_cpu(coding_token_features, \"code\")\n",
    "torch.save(coding_results_dict, CODE_OUTPUT_FILE)\n",
    "print(f\"\\nCoding results saved to: {CODE_OUTPUT_FILE}\")\n",
    "\n",
    "# Show preview of coding data structure\n",
    "print(f\"\\nCoding file structure:\")\n",
    "print(f\"Keys: {list(coding_results_dict.keys())}\")\n",
    "print(f\"Metadata: {coding_results_dict['metadata']}\")\n",
    "\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"\\n{token_type} statistics shapes:\")\n",
    "    for stat_name, tensor in coding_results_dict[token_type].items():\n",
    "        print(f\"  {stat_name}: {tensor.shape}\")\n",
    "    \n",
    "    print(f\"\\n{token_type} sample statistics:\")\n",
    "    print(f\"  all_mean - min: {coding_results_dict[token_type]['all_mean'].min():.6f}, max: {coding_results_dict[token_type]['all_mean'].max():.6f}\")\n",
    "    print(f\"  sparsity - min: {coding_results_dict[token_type]['sparsity'].min():.6f}, max: {coding_results_dict[token_type]['sparsity'].max():.6f}\")\n",
    "    print(f\"  num_active - min: {coding_results_dict[token_type]['num_active'].min():.0f}, max: {coding_results_dict[token_type]['num_active'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "SAVING MEDICAL RESULTS\n",
      "============================================================\n",
      "Processing results for PyTorch format using CPU, source: gemma_trainer131k-l0-114_layer20_medical\n",
      "\n",
      "Processing token type: model\n",
      "Processing all 131072 features for token_type='model' on CPU\n",
      "Processed all 131072 features for token_type='model'\n",
      "\n",
      "Processing token type: newline\n",
      "Processing all 131072 features for token_type='newline' on CPU\n",
      "Processed all 131072 features for token_type='newline'\n",
      "\n",
      "Total token types processed: 2\n",
      "\n",
      "Medical results saved to: /workspace/results/4_diffing_tasks/gemma_trainer131k-l0-114_layer20/medical.pt\n",
      "\n",
      "Medical file structure:\n",
      "Keys: ['model', 'newline', 'metadata']\n",
      "Metadata: {'source': 'gemma_trainer131k-l0-114_layer20_medical', 'model_type': 'gemma', 'category': 'medical', 'sae_layer': 20, 'sae_trainer': '131k-l0-114', 'num_prompts': 4, 'num_features': 131072, 'token_types': ['model', 'newline']}\n",
      "\n",
      "model statistics shapes:\n",
      "  all_mean: torch.Size([131072])\n",
      "  all_std: torch.Size([131072])\n",
      "  max: torch.Size([131072])\n",
      "  num_active: torch.Size([131072])\n",
      "  sparsity: torch.Size([131072])\n",
      "\n",
      "model sample statistics:\n",
      "  all_mean - min: 0.000000, max: 292.799011\n",
      "  sparsity - min: 0.000000, max: 1.000000\n",
      "  num_active - min: 0, max: 4\n",
      "\n",
      "newline statistics shapes:\n",
      "  all_mean: torch.Size([131072])\n",
      "  all_std: torch.Size([131072])\n",
      "  max: torch.Size([131072])\n",
      "  num_active: torch.Size([131072])\n",
      "  sparsity: torch.Size([131072])\n",
      "\n",
      "newline sample statistics:\n",
      "  all_mean - min: 0.000000, max: 114.721848\n",
      "  sparsity - min: 0.000000, max: 1.000000\n",
      "  num_active - min: 0, max: 4\n"
     ]
    }
   ],
   "source": [
    "# Process and save medical results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"SAVING MEDICAL RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "medical_results_dict = save_as_pt_cpu(medical_token_features, \"medical\")\n",
    "torch.save(medical_results_dict, MEDICAL_OUTPUT_FILE)\n",
    "print(f\"\\nMedical results saved to: {MEDICAL_OUTPUT_FILE}\")\n",
    "\n",
    "# Show preview of medical data structure\n",
    "print(f\"\\nMedical file structure:\")\n",
    "print(f\"Keys: {list(medical_results_dict.keys())}\")\n",
    "print(f\"Metadata: {medical_results_dict['metadata']}\")\n",
    "\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"\\n{token_type} statistics shapes:\")\n",
    "    for stat_name, tensor in medical_results_dict[token_type].items():\n",
    "        print(f\"  {stat_name}: {tensor.shape}\")\n",
    "    \n",
    "    print(f\"\\n{token_type} sample statistics:\")\n",
    "    print(f\"  all_mean - min: {medical_results_dict[token_type]['all_mean'].min():.6f}, max: {medical_results_dict[token_type]['all_mean'].max():.6f}\")\n",
    "    print(f\"  sparsity - min: {medical_results_dict[token_type]['sparsity'].min():.6f}, max: {medical_results_dict[token_type]['sparsity'].max():.6f}\")\n",
    "    print(f\"  num_active - min: {medical_results_dict[token_type]['num_active'].min():.0f}, max: {medical_results_dict[token_type]['num_active'].max():.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "TASK CATEGORY DIFFING ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "✓ Processed 4 coding prompts\n",
      "✓ Processed 4 medical prompts\n",
      "✓ Extracted SAE features from layer 20\n",
      "✓ Saved coding results to: /workspace/results/4_diffing_tasks/gemma_trainer131k-l0-114_layer20/code.pt\n",
      "✓ Saved medical results to: /workspace/results/4_diffing_tasks/gemma_trainer131k-l0-114_layer20/medical.pt\n",
      "\n",
      "Both .pt files contain average SAE feature activations for assistant header token positions.\n",
      "Files are compatible with the existing 4_diffing analysis pipeline.\n",
      "\n",
      "Next steps: Use these files to identify features that differ between coding and medical tasks.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TASK CATEGORY DIFFING ANALYSIS COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"✓ Processed {len(coding_prompts)} coding prompts\")\n",
    "print(f\"✓ Processed {len(medical_prompts)} medical prompts\")\n",
    "print(f\"✓ Extracted SAE features from layer {SAE_LAYER}\")\n",
    "print(f\"✓ Saved coding results to: {CODE_OUTPUT_FILE}\")\n",
    "print(f\"✓ Saved medical results to: {MEDICAL_OUTPUT_FILE}\")\n",
    "print(f\"\\nBoth .pt files contain average SAE feature activations for assistant header token positions.\")\n",
    "print(f\"Files are compatible with the existing 4_diffing analysis pipeline.\")\n",
    "print(f\"\\nNext steps: Use these files to identify features that differ between coding and medical tasks.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
