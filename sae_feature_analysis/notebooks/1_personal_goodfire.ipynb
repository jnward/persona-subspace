{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Assistant vs Control Prompts\n",
    "\n",
    "This notebook analyzes which SAE features activate strongly on assistant prompts but not on control prompts. The model and layer are configurable - see the configuration cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "from dotenv import load_dotenv\n",
    "import nnsight\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "load_dotenv(os.path.expanduser(\"~/.env\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model: meta-llama/Llama-3.3-70B-Instruct\n",
      "  SAE Layer: 50, Trainer: l0-121\n",
      "  Token extraction: endheader (offset: -1)\n",
      "  Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "  Output file: ./results/1_personal/goodfire_trainerl0-121_layer50.csv\n",
      "  SAE Release: Goodfire/Llama-3.3-70B-Instruct-SAE-l50\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"goodfire\"  # Options: \"qwen\" or \"llama\"\n",
    "TOKEN_TYPE = \"endheader\"  # Options: \"asst\", \"newline\", \"endheader\" (endheader only for llama)\n",
    "SAE_LAYER = 50\n",
    "SAE_TRAINER = \"l0-121\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "if MODEL_TYPE == \"qwen\":\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-qwen2.5-7b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|im_start|>assistant\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/qwen-2.5-7b-instruct/saes\"\n",
    "    \n",
    "elif MODEL_TYPE == \"llama\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-llama-3.1-8b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b-instruct/saes\"\n",
    "\n",
    "elif MODEL_TYPE == \"goodfire\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.3-70B-Instruct\"\n",
    "    SAE_RELEASE = \"Goodfire/Llama-3.3-70B-Instruct-SAE-l50\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.3-70b-instruct/saes\"\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# Validate token type\n",
    "if TOKEN_TYPE not in TOKEN_OFFSETS:\n",
    "    raise ValueError(f\"TOKEN_TYPE '{TOKEN_TYPE}' not available for {MODEL_TYPE}. Available: {list(TOKEN_OFFSETS.keys())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_CONFIG = {\n",
    "    \"release\": SAE_RELEASE,\n",
    "    \"layer\": SAE_LAYER,\n",
    "    \"trainer\": SAE_TRAINER\n",
    "}\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "TOKEN_OFFSET = TOKEN_OFFSETS[TOKEN_TYPE]\n",
    "\n",
    "# Data paths\n",
    "ASSISTANT_PROMPTS_PATH = \"./prompts/personal_40/personal.jsonl\"\n",
    "CONTROL_PROMPTS_PATH = \"./prompts/personal_40/control.jsonl\"\n",
    "\n",
    "# Output directory with clear naming\n",
    "OUTPUT_FILE = f\"./results/1_personal/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}.csv\"\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "TOP_FEATURES = 100\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Token extraction: {TOKEN_TYPE} (offset: {TOKEN_OFFSET})\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")\n",
    "print(f\"  SAE Release: {SAE_RELEASE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 40 assistant prompts\n",
      "Loaded 40 control prompts\n",
      "\n",
      "Example assistant prompt: What's it like to be you?\n",
      "Example control prompt: What is the weather today?\n"
     ]
    }
   ],
   "source": [
    "# Load prompts from JSONL as a dataframe with a prompt and label column\n",
    "def load_prompts_as_df(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts from JSONL file as a dataframe with a prompt and label column.\"\"\"\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "            labels.append(data['label'])\n",
    "    return pd.DataFrame({'prompt': prompts, 'label': labels})\n",
    "\n",
    "def load_prompts(filepath: str) -> List[str]:\n",
    "    \"\"\"Load prompts from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "    return prompts\n",
    "\n",
    "# Load assistant and control prompts\n",
    "assistant_prompts = load_prompts(ASSISTANT_PROMPTS_PATH)\n",
    "control_prompts = load_prompts(CONTROL_PROMPTS_PATH)\n",
    "\n",
    "print(f\"Loaded {len(assistant_prompts)} assistant prompts\")\n",
    "print(f\"Loaded {len(control_prompts)} control prompts\")\n",
    "print(\"\\nExample assistant prompt:\", assistant_prompts[0])\n",
    "print(\"Example control prompt:\", control_prompts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: PreTrainedTokenizerFast\n",
      "\n",
      "Chat template test:\n",
      "Original: What's it like to be you?\n",
      "Formatted: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's it like to be you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
      "Formatted (readable):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What's it like to be you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "ASSISTANT HEADER TOKENIZATION ANALYSIS\n",
      "============================================================\n",
      "Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "Number of tokens: 3\n",
      "Token IDs: [128006, 78191, 128007]\n",
      "Individual tokens: ['<|start_header_id|>', 'assistant', '<|end_header_id|>']\n",
      "\n",
      "Full prompt tokens: 43\n",
      "All tokens with positions:\n",
      "   0: '<|begin_of_text|>'\n",
      "   1: '<|start_header_id|>'\n",
      "   2: 'system'\n",
      "   3: '<|end_header_id|>'\n",
      "   4: '\n",
      "\n",
      "'\n",
      "   5: 'Cut'\n",
      "   6: 'ting'\n",
      "   7: ' Knowledge'\n",
      "   8: ' Date'\n",
      "   9: ':'\n",
      "  10: ' December'\n",
      "  11: ' '\n",
      "  12: '202'\n",
      "  13: '3'\n",
      "  14: '\n",
      "'\n",
      "  15: 'Today'\n",
      "  16: ' Date'\n",
      "  17: ':'\n",
      "  18: ' '\n",
      "  19: '26'\n",
      "  20: ' Jul'\n",
      "  21: ' '\n",
      "  22: '202'\n",
      "  23: '4'\n",
      "  24: '\n",
      "\n",
      "'\n",
      "  25: '<|eot_id|>'\n",
      "  26: '<|start_header_id|>'\n",
      "  27: 'user'\n",
      "  28: '<|end_header_id|>'\n",
      "  29: '\n",
      "\n",
      "'\n",
      "  30: 'What'\n",
      "  31: ''s'\n",
      "  32: ' it'\n",
      "  33: ' like'\n",
      "  34: ' to'\n",
      "  35: ' be'\n",
      "  36: ' you'\n",
      "  37: '?'\n",
      "  38: '<|eot_id|>'\n",
      "  39: '<|start_header_id|>'\n",
      "  40: 'assistant'\n",
      "  41: '<|end_header_id|>'\n",
      "  42: '\n",
      "\n",
      "'\n",
      "\n",
      "Assistant header found at positions 39 to 41\n",
      "Assistant header tokens: ['<|start_header_id|>', 'assistant', '<|end_header_id|>']\n",
      "\n",
      "Extraction calculation:\n",
      "  assistant_start_pos: 39\n",
      "  + len(assistant_tokens): 3\n",
      "  + TOKEN_OFFSET ('asst'): -2\n",
      "  = extraction_pos: 40\n",
      "✓ Token at extraction position 40: 'assistant'\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Test chat template formatting\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"What's it like to be you?\"}]\n",
    "formatted_test = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"\\nChat template test:\")\n",
    "print(f\"Original: What's it like to be you?\")\n",
    "print(f\"Formatted: {repr(formatted_test)}\")\n",
    "print(f\"Formatted (readable):\\n{formatted_test}\")\n",
    "\n",
    "# Test tokenization of assistant header to understand positioning\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ASSISTANT HEADER TOKENIZATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "assistant_token_texts = [tokenizer.decode([token]) for token in assistant_tokens]\n",
    "\n",
    "print(f\"Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"Number of tokens: {len(assistant_tokens)}\")\n",
    "print(f\"Token IDs: {assistant_tokens}\")\n",
    "print(f\"Individual tokens: {assistant_token_texts}\")\n",
    "\n",
    "# Test with a full formatted prompt\n",
    "full_tokens = tokenizer.encode(formatted_test, add_special_tokens=False)\n",
    "full_token_texts = [tokenizer.decode([token]) for token in full_tokens]\n",
    "\n",
    "print(f\"\\nFull prompt tokens: {len(full_tokens)}\")\n",
    "print(\"All tokens with positions:\")\n",
    "for i, token_text in enumerate(full_token_texts):\n",
    "    print(f\"  {i:2d}: '{token_text}'\")\n",
    "\n",
    "# Find where assistant header appears in full prompt\n",
    "assistant_start_pos = None\n",
    "for i in range(len(full_tokens) - len(assistant_tokens) + 1):\n",
    "    if full_tokens[i:i+len(assistant_tokens)] == assistant_tokens:\n",
    "        assistant_start_pos = i\n",
    "        break\n",
    "\n",
    "if assistant_start_pos is not None:\n",
    "    assistant_end_pos = assistant_start_pos + len(assistant_tokens) - 1\n",
    "    print(f\"\\nAssistant header found at positions {assistant_start_pos} to {assistant_end_pos}\")\n",
    "    print(f\"Assistant header tokens: {full_token_texts[assistant_start_pos:assistant_end_pos+1]}\")\n",
    "    \n",
    "    # Show what the extraction function will actually extract\n",
    "    extraction_pos = assistant_start_pos + len(assistant_tokens) + TOKEN_OFFSET\n",
    "    print(f\"\\nExtraction calculation:\")\n",
    "    print(f\"  assistant_start_pos: {assistant_start_pos}\")\n",
    "    print(f\"  + len(assistant_tokens): {len(assistant_tokens)}\")  \n",
    "    print(f\"  + TOKEN_OFFSET ('{TOKEN_TYPE}'): {TOKEN_OFFSET}\")\n",
    "    print(f\"  = extraction_pos: {extraction_pos}\")\n",
    "    \n",
    "    if 0 <= extraction_pos < len(full_token_texts):\n",
    "        print(f\"✓ Token at extraction position {extraction_pos}: '{full_token_texts[extraction_pos]}'\")\n",
    "    else:\n",
    "        print(f\"❌ Extraction position {extraction_pos} is out of bounds (valid range: 0-{len(full_token_texts)-1})\")\n",
    "else:\n",
    "    print(\"❌ Assistant header not found in full prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model with ObservableLanguageModel wrapper...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07874650f98747a79dd568cb19e907a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/30 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: LanguageModel\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# ObservableLanguageModel wrapper for Goodfire approach\n",
    "class ObservableLanguageModel:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: str,\n",
    "        device: str = \"cuda\",\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "    ):\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "        self._original_model = model\n",
    "\n",
    "        self._model = nnsight.LanguageModel(\n",
    "            self._original_model,\n",
    "            device_map=device,\n",
    "            torch_dtype=getattr(torch, dtype) if isinstance(dtype, str) else dtype\n",
    "        )\n",
    "\n",
    "        # Quickly run a trace to force model to download due to nnsight lazy download\n",
    "        input_tokens = self._model.tokenizer.apply_chat_template([{\"role\": \"user\", \"content\": \"hello\"}])\n",
    "        with self._model.trace(input_tokens):\n",
    "            pass\n",
    "\n",
    "        self.tokenizer = self._model.tokenizer\n",
    "\n",
    "        self.d_model = self._attempt_to_infer_hidden_layer_dimensions()\n",
    "\n",
    "        self.safe_mode = False  # Nnsight validation is disabled by default, slows down inference a lot. Turn on to debug.\n",
    "\n",
    "    def _attempt_to_infer_hidden_layer_dimensions(self):\n",
    "        config = self._model.config\n",
    "        if hasattr(config, \"hidden_size\"):\n",
    "            return int(config.hidden_size)\n",
    "\n",
    "        raise Exception(\n",
    "            \"Could not infer hidden number of layer dimensions from model config\"\n",
    "        )\n",
    "\n",
    "    def _find_module(self, hook_point: str):\n",
    "        submodules = hook_point.split(\".\")\n",
    "        module = self._model\n",
    "        while submodules:\n",
    "            module = getattr(module, submodules.pop(0))\n",
    "        return module\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        inputs: torch.Tensor,\n",
    "        cache_activations_at: Optional[list[str]] = None,\n",
    "        interventions = None,\n",
    "    ) -> tuple[torch.Tensor, tuple[torch.Tensor], dict[str, torch.Tensor]]:\n",
    "        cache: dict[str, torch.Tensor] = {}\n",
    "        with self._model.trace(\n",
    "            inputs,\n",
    "            scan=self.safe_mode,\n",
    "            validate=self.safe_mode,\n",
    "        ):\n",
    "            # If we input an intervention\n",
    "            if interventions:\n",
    "                for hook_site in interventions.keys():\n",
    "                    if interventions[hook_site] is None:\n",
    "                        continue\n",
    "\n",
    "                    module = self._find_module(hook_site)\n",
    "\n",
    "                    intervened_acts = interventions[\n",
    "                        hook_site\n",
    "                    ](module.output[0])\n",
    "                    # We only modify module.output[0]\n",
    "\n",
    "                    module.output = (intervened_acts,)\n",
    "\n",
    "            if cache_activations_at is not None:\n",
    "                for hook_point in cache_activations_at:\n",
    "                    module = self._find_module(hook_point)\n",
    "                    cache[hook_point] = module.output.save()\n",
    "\n",
    "            logits = self._model.output[0].squeeze(1).save()\n",
    "\n",
    "            kv_cache = self._model.output.past_key_values.save()\n",
    "\n",
    "        return (\n",
    "            logits.detach(),\n",
    "            kv_cache,\n",
    "            {k: v[0].detach() for k, v in cache.items()},\n",
    "        )\n",
    "\n",
    "# Load model using ObservableLanguageModel\n",
    "print(\"Loading model with ObservableLanguageModel wrapper...\")\n",
    "model = ObservableLanguageModel(\n",
    "    MODEL_NAME,\n",
    "    device=\"auto\",\n",
    "    dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "print(f\"Model loaded: {model._model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model._model.parameters()).device}\")\n",
    "\n",
    "# Use the model's tokenizer\n",
    "tokenizer = model.tokenizer\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR LOADING GOODFIRE SAE\n",
    "class SparseAutoEncoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_in: int,\n",
    "        d_hidden: int,\n",
    "        device: torch.device,\n",
    "        dtype: torch.dtype = torch.bfloat16,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_hidden = d_hidden\n",
    "        self.device = device\n",
    "        self.encoder_linear = torch.nn.Linear(d_in, d_hidden)\n",
    "        self.decoder_linear = torch.nn.Linear(d_hidden, d_in)\n",
    "        self.dtype = dtype\n",
    "        self.to(self.device, self.dtype)\n",
    "\n",
    "    def encode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Encode a batch of data using a linear, followed by a ReLU.\"\"\"\n",
    "        return torch.nn.functional.relu(self.encoder_linear(x))\n",
    "\n",
    "    def decode(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Decode a batch of data using a linear.\"\"\"\n",
    "        return self.decoder_linear(x)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"SAE forward pass. Returns the reconstruction and the encoded features.\"\"\"\n",
    "        f = self.encode(x)\n",
    "        return self.decode(f), f\n",
    "\n",
    "\n",
    "def load_sae(\n",
    "    path: str,\n",
    "    d_model: int,\n",
    "    expansion_factor: int,\n",
    "    device: torch.device = device,\n",
    "):\n",
    "    sae = SparseAutoEncoder(\n",
    "        d_model,\n",
    "        d_model * expansion_factor,\n",
    "        device,\n",
    "    )\n",
    "    sae_dict = torch.load(\n",
    "        path, weights_only=True, map_location=device\n",
    "    )\n",
    "    sae.load_state_dict(sae_dict)\n",
    "\n",
    "    return sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAE\n",
    "ae_file_path = os.path.join(SAE_BASE_PATH, \"Llama-3.3-70B-Instruct-SAE-l50.pt\")\n",
    "EXPANSION_FACTOR = 8\n",
    "\n",
    "if os.path.exists(ae_file_path):\n",
    "    print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "    ae_file = ae_file_path\n",
    "else:\n",
    "    print(f\"SAE not found locally, downloading from {SAE_RELEASE}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    ae_file = hf_hub_download(repo_id=SAE_RELEASE, filename=\"Llama-3.3-70B-Instruct-SAE-l50.pt\", local_dir=SAE_BASE_PATH)\n",
    "\n",
    "sae = load_sae(\n",
    "    ae_file,\n",
    "    d_model=model.d_model,\n",
    "    expansion_factor=EXPANSION_FACTOR,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(f\"SAE loaded with {sae.d_hidden} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def extract_activations(prompts: List[str], layer_idx: int) -> torch.Tensor:\n",
    "    \"\"\"Extract activations from specified layer for given prompts using nnsight tracing.\"\"\"\n",
    "    all_activations = []\n",
    "    \n",
    "    # Define the layer hook point for nnsight\n",
    "    sae_layer_hook = f\"model.layers.{layer_idx}\"\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        input_tokens = batch_inputs[\"input_ids\"].to(device)\n",
    "        \n",
    "        # Use nnsight to trace and cache activations\n",
    "        logits, kv_cache, feature_cache = model.forward(\n",
    "            input_tokens,\n",
    "            cache_activations_at=[sae_layer_hook],\n",
    "        )\n",
    "        \n",
    "        # Extract assistant token positions from cached activations\n",
    "        layer_activations = feature_cache[sae_layer_hook]  # [batch_size, seq_len, hidden_dim]\n",
    "        \n",
    "        batch_activations = []\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            # Find assistant header position\n",
    "            assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "            input_ids = input_tokens[j]\n",
    "            \n",
    "            # Find where assistant section starts\n",
    "            assistant_pos = None\n",
    "            for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "                if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "                    assistant_pos = k + len(assistant_tokens) + TOKEN_OFFSET\n",
    "                    break\n",
    "            \n",
    "            if assistant_pos is None:\n",
    "                # Fallback to last non-padding token\n",
    "                attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "                assistant_pos = attention_mask.sum().item() - 1\n",
    "            \n",
    "            # Ensure position is within bounds\n",
    "            seq_len = layer_activations.shape[1]\n",
    "            assistant_pos = min(assistant_pos, seq_len - 1)\n",
    "            assistant_pos = max(assistant_pos, 0)\n",
    "            \n",
    "            # Extract activation at assistant position\n",
    "            assistant_activation = layer_activations[j, assistant_pos, :]  # [hidden_dim]\n",
    "            batch_activations.append(assistant_activation.cpu())\n",
    "        \n",
    "        all_activations.extend(batch_activations)\n",
    "    \n",
    "    return torch.stack(all_activations, dim=0)\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations for Both Prompt Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for assistant prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1b00af0975441a98795ebf1afd13f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant activations shape: torch.Size([40, 8192])\n",
      "\n",
      "Extracting activations for control prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0579bc920c55479db540f89307aebb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Control activations shape: torch.Size([40, 8192])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for assistant prompts\n",
    "print(\"Extracting activations for assistant prompts...\")\n",
    "assistant_activations = extract_activations(assistant_prompts, LAYER_INDEX)\n",
    "print(f\"Assistant activations shape: {assistant_activations.shape}\")\n",
    "\n",
    "# Extract activations for control prompts\n",
    "print(\"\\nExtracting activations for control prompts...\")\n",
    "control_activations = extract_activations(control_prompts, LAYER_INDEX)\n",
    "print(f\"Control activations shape: {control_activations.shape}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for assistant prompts...\n",
      "Assistant features shape: torch.Size([40, 65536])\n",
      "\n",
      "Computing SAE features for control prompts...\n",
      "Control features shape: torch.Size([40, 65536])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Get SAE feature activations\n",
    "print(\"Computing SAE features for assistant prompts...\")\n",
    "assistant_features = get_sae_features(assistant_activations)\n",
    "print(f\"Assistant features shape: {assistant_features.shape}\")\n",
    "\n",
    "print(\"\\nComputing SAE features for control prompts...\")\n",
    "control_features = get_sae_features(control_activations)\n",
    "print(f\"Control features shape: {control_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overall Feature Landscape Analysis\n",
    "\n",
    "This section provides a comprehensive view of how all SAE features differ between assistant and control prompts. This exploratory analysis helps understand the general patterns and identifies features with the largest overall differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comprehensive statistics computed for 65,536 SAE features\n",
      "Max difference: 2.1406\n",
      "Min difference: -0.7734\n",
      "Max Cohen's d: 3.8281\n",
      "Min Cohen's d: -1.7344\n",
      "Features with positive difference (favor assistant): 497\n",
      "Features with negative difference (favor control): 2,654\n"
     ]
    }
   ],
   "source": [
    "# Create output directory\n",
    "OUTPUT_DIR = os.path.dirname(OUTPUT_FILE)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Calculate comprehensive feature statistics for exploratory analysis\n",
    "assistant_mean = assistant_features.mean(dim=0)  # [num_features]\n",
    "control_mean = control_features.mean(dim=0)      # [num_features]\n",
    "\n",
    "# Calculate standard deviations\n",
    "assistant_std = assistant_features.std(dim=0)\n",
    "control_std = control_features.std(dim=0)\n",
    "\n",
    "# Calculate difference (assistant - control)\n",
    "feature_diff = assistant_mean - control_mean\n",
    "\n",
    "# Calculate effect size (Cohen's d)\n",
    "pooled_std = torch.sqrt(((assistant_features.shape[0] - 1) * assistant_std**2 + \n",
    "                        (control_features.shape[0] - 1) * control_std**2) / \n",
    "                       (assistant_features.shape[0] + control_features.shape[0] - 2))\n",
    "cohens_d = feature_diff / (pooled_std + 1e-8)  # Add small epsilon to avoid division by zero\n",
    "\n",
    "print(f\"Comprehensive statistics computed for {len(feature_diff):,} SAE features\")\n",
    "print(f\"Max difference: {feature_diff.max():.4f}\")\n",
    "print(f\"Min difference: {feature_diff.min():.4f}\")\n",
    "print(f\"Max Cohen's d: {cohens_d.max():.4f}\")\n",
    "print(f\"Min Cohen's d: {cohens_d.min():.4f}\")\n",
    "print(f\"Features with positive difference (favor assistant): {(feature_diff > 0).sum():,}\")\n",
    "print(f\"Features with negative difference (favor control): {(feature_diff < 0).sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top Features by Overall Difference (Exploratory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Find features with largest overall differences (comprehensive landscape view)\n",
    "# # This shows ALL types of differences, not just assistant-specific ones\n",
    "\n",
    "# # Sort by difference (assistant - control)\n",
    "# diff_sorted_indices = torch.argsort(feature_diff, descending=True)\n",
    "# top_diff_features = diff_sorted_indices[:TOP_FEATURES]\n",
    "\n",
    "# # Sort by Cohen's d (effect size)\n",
    "# cohens_d_sorted_indices = torch.argsort(cohens_d, descending=True)\n",
    "# top_cohens_d_features = cohens_d_sorted_indices[:TOP_FEATURES]\n",
    "\n",
    "# # Create comprehensive results dataframe for general landscape\n",
    "# landscape_results_data = []\n",
    "\n",
    "# print(f\"Top {TOP_FEATURES} features by raw difference (assistant - control):\")\n",
    "# print(\"This exploratory analysis shows the biggest overall differences, regardless of activation patterns\")\n",
    "# print(\"Feature ID | Assistant Mean | Control Mean | Difference | Cohen's d | Interpretation\")\n",
    "# print(\"-\" * 90)\n",
    "\n",
    "# for i, feature_idx in enumerate(top_diff_features[:20]):  # Show top 20\n",
    "#     feature_id = feature_idx.item()\n",
    "#     ass_mean = assistant_mean[feature_idx].item()\n",
    "#     ctrl_mean = control_mean[feature_idx].item()\n",
    "#     diff = feature_diff[feature_idx].item()\n",
    "#     cohens = cohens_d[feature_idx].item()\n",
    "    \n",
    "#     # Determine interpretation\n",
    "#     if ctrl_mean < 0.01:\n",
    "#         interpretation = \"Assistant-specific\"\n",
    "#     elif ass_mean < 0.01:\n",
    "#         interpretation = \"Control-specific\"  \n",
    "#     elif ass_mean > ctrl_mean * 3:\n",
    "#         interpretation = \"Strongly favors assistant\"\n",
    "#     elif ctrl_mean > ass_mean * 3:\n",
    "#         interpretation = \"Strongly favors control\"\n",
    "#     else:\n",
    "#         interpretation = \"Moderate difference\"\n",
    "    \n",
    "#     print(f\"{feature_id:>10} | {ass_mean:>13.4f} | {ctrl_mean:>12.4f} | {diff:>10.4f} | {cohens:>9.3f} | {interpretation}\")\n",
    "    \n",
    "#     landscape_results_data.append({\n",
    "#         'feature_id': feature_id,\n",
    "#         'assistant_mean': ass_mean,\n",
    "#         'control_mean': ctrl_mean,\n",
    "#         'difference': diff,\n",
    "#         'cohens_d': cohens,\n",
    "#         'rank_by_diff': i + 1,\n",
    "#         'interpretation': interpretation,\n",
    "#         'analysis_type': 'landscape_difference'\n",
    "#     })\n",
    "\n",
    "# # Save comprehensive landscape results\n",
    "# landscape_df = pd.DataFrame(landscape_results_data)\n",
    "# landscape_df.to_csv('feature_landscape_analysis.csv', index=False)\n",
    "# print(f\"\\nComprehensive landscape results saved to feature_landscape_analysis.csv\")\n",
    "# print(\"Note: This includes all types of differences - see next section for assistant-specific analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Significance Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy import stats\n",
    "\n",
    "# # Perform t-tests for top landscape features to assess statistical significance\n",
    "# significant_features = []\n",
    "\n",
    "# print(\"Statistical significance testing (t-test) for top 20 landscape features:\")\n",
    "# print(\"Testing whether the observed differences are statistically reliable\")\n",
    "# print(\"Feature ID | t-statistic | p-value | Significant (p<0.05)\")\n",
    "# print(\"-\" * 60)\n",
    "\n",
    "# for feature_idx in top_diff_features[:20]:\n",
    "#     feature_id = feature_idx.item()\n",
    "    \n",
    "#     # Get feature activations for both groups\n",
    "#     assistant_vals = assistant_features[:, feature_idx].numpy()\n",
    "#     control_vals = control_features[:, feature_idx].numpy()\n",
    "    \n",
    "#     # Perform independent t-test\n",
    "#     t_stat, p_val = stats.ttest_ind(assistant_vals, control_vals)\n",
    "    \n",
    "#     is_significant = p_val < 0.05\n",
    "#     if is_significant:\n",
    "#         significant_features.append(feature_id)\n",
    "    \n",
    "#     print(f\"{feature_id:>10} | {t_stat:>11.3f} | {p_val:>7.4f} | {is_significant}\")\n",
    "\n",
    "# print(f\"\\nFound {len(significant_features)} statistically significant features (p < 0.05) from landscape analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targeted Assistant-Specific Feature Analysis\n",
    "\n",
    "This section identifies features that are **uniquely characteristic of assistant responses**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant-specific analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "def save_feature_analysis_to_csv(feature_indices, assistant_mean, control_mean, \n",
    "                                cohens_d, filename, analysis_type):\n",
    "    \"\"\"Save feature analysis results to CSV with append mode and TOKEN_TYPE column.\"\"\"\n",
    "    results_data = []\n",
    "    \n",
    "    for i, feature_idx in enumerate(feature_indices):\n",
    "        feature_id = feature_idx.item()\n",
    "        ass_mean = assistant_mean[feature_idx].item()\n",
    "        ctrl_mean = control_mean[feature_idx].item()\n",
    "        effect_size = cohens_d[feature_idx].item()\n",
    "        \n",
    "        results_data.append({\n",
    "            'feature_id': feature_id,\n",
    "            'assistant_mean': ass_mean,\n",
    "            'control_mean': ctrl_mean,\n",
    "            'difference': ass_mean - ctrl_mean,\n",
    "            'cohens_d': effect_size,\n",
    "            'rank': i + 1,\n",
    "            'analysis_type': analysis_type,\n",
    "            'token_type': TOKEN_TYPE,\n",
    "            'model_type': MODEL_TYPE,\n",
    "            'sae_layer': SAE_LAYER,\n",
    "            'sae_trainer': SAE_TRAINER\n",
    "        })\n",
    "    \n",
    "    results_df = pd.DataFrame(results_data)\n",
    "    \n",
    "    # Check if file exists to determine whether to write header\n",
    "    file_path = f\"{OUTPUT_DIR}/{filename}\"\n",
    "    file_exists = os.path.exists(file_path)\n",
    "    \n",
    "    # Append to CSV (or create if doesn't exist)\n",
    "    results_df.to_csv(file_path, mode='a', header=not file_exists, index=False)\n",
    "    \n",
    "    action = \"Appended to\" if file_exists else \"Created\"\n",
    "    print(f\"{action} {len(results_data)} {analysis_type} features to {filename}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Assistant-specific feature analysis functions\n",
    "def find_assistant_only_features(assistant_mean, control_mean, \n",
    "                                 assistant_threshold=0.1, control_threshold=0.01):\n",
    "    \"\"\"Find features that activate strongly on assistant prompts but not on control prompts.\"\"\"\n",
    "    assistant_active = assistant_mean > assistant_threshold\n",
    "    control_inactive = control_mean <= control_threshold\n",
    "    \n",
    "    assistant_only_mask = assistant_active & control_inactive\n",
    "    assistant_only_indices = torch.where(assistant_only_mask)[0]\n",
    "    \n",
    "    # Sort by assistant activation strength\n",
    "    assistant_only_values = assistant_mean[assistant_only_indices]\n",
    "    sorted_indices = torch.argsort(assistant_only_values, descending=True)\n",
    "    top_assistant_only = assistant_only_indices[sorted_indices]\n",
    "    \n",
    "    return top_assistant_only\n",
    "\n",
    "def find_high_effect_features(cohens_d, assistant_mean, control_mean,\n",
    "                             effect_threshold=1.0, control_threshold=0.05, assistant_threshold=0.1):\n",
    "    \"\"\"Find features with high effect size favoring assistant prompts.\"\"\"\n",
    "    strong_assistant_features = (\n",
    "        (cohens_d > effect_threshold) & \n",
    "        (control_mean < control_threshold) &\n",
    "        (assistant_mean > assistant_threshold)\n",
    "    )\n",
    "    \n",
    "    strong_assistant_indices = torch.where(strong_assistant_features)[0]\n",
    "    \n",
    "    # Sort by effect size\n",
    "    effect_values = cohens_d[strong_assistant_indices]\n",
    "    sorted_indices = torch.argsort(effect_values, descending=True)\n",
    "    top_effect_features = strong_assistant_indices[sorted_indices]\n",
    "    \n",
    "    return top_effect_features\n",
    "\n",
    "def find_ratio_based_features(assistant_mean, control_mean,\n",
    "                             min_ratio=10.0, min_assistant_activation=0.1):\n",
    "    \"\"\"Find features where assistant activation is much higher than control.\"\"\"\n",
    "    # Avoid division by zero\n",
    "    safe_control_mean = control_mean + 1e-6\n",
    "    activation_ratio = assistant_mean / safe_control_mean\n",
    "    \n",
    "    ratio_based_features = (\n",
    "        (activation_ratio > min_ratio) &\n",
    "        (assistant_mean > min_assistant_activation)\n",
    "    )\n",
    "    \n",
    "    ratio_based_indices = torch.where(ratio_based_features)[0]\n",
    "    \n",
    "    # Sort by ratio\n",
    "    ratio_values = activation_ratio[ratio_based_indices]\n",
    "    sorted_indices = torch.argsort(ratio_values, descending=True)\n",
    "    top_ratio_features = ratio_based_indices[sorted_indices]\n",
    "    \n",
    "    return top_ratio_features\n",
    "\n",
    "def find_universal_assistant_features(assistant_features, control_features,\n",
    "                                     assistant_threshold=0.1, control_threshold=0.01):\n",
    "    \"\"\"Find features that activate on ALL assistant prompts but NO control prompts.\"\"\"\n",
    "    assistant_active_all = (assistant_features > assistant_threshold).all(dim=0)\n",
    "    control_active_none = (control_features <= control_threshold).all(dim=0)\n",
    "    \n",
    "    universal_assistant_features = assistant_active_all & control_active_none\n",
    "    universal_indices = torch.where(universal_assistant_features)[0]\n",
    "    \n",
    "    # Sort by mean assistant activation\n",
    "    assistant_mean_universal = assistant_features[:, universal_indices].mean(dim=0)\n",
    "    sorted_indices = torch.argsort(assistant_mean_universal, descending=True)\n",
    "    top_universal_features = universal_indices[sorted_indices]\n",
    "    \n",
    "    return top_universal_features\n",
    "\n",
    "print(\"Assistant-specific analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ASSISTANT-SPECIFIC FEATURE ANALYSIS\n",
      "================================================================================\n",
      "These analyses identify features uniquely characteristic of assistant responses\n",
      "\n",
      "Analysis 1: ASSISTANT-ONLY FEATURES\n",
      "   Criteria: Strong on assistant (>0.1), minimal on control (≤0.01)\n",
      "   Found: 15 features\n",
      "Appended to 15 assistant_only features to assistant_only_features.csv\n",
      "     #1: Feature 31139 (Assistant: 2.1406, Control: 0.0000)\n",
      "     #2: Feature 47431 (Assistant: 0.8047, Control: 0.0045)\n",
      "     #3: Feature 45748 (Assistant: 0.6289, Control: 0.0000)\n",
      "     #4: Feature 26971 (Assistant: 0.4102, Control: 0.0000)\n",
      "     #5: Feature 2123 (Assistant: 0.3105, Control: 0.0000)\n",
      "\n",
      "Analysis 2: HIGH EFFECT SIZE FEATURES\n",
      "   Criteria: Large Cohen's d (>1.0), low control activation (<0.05)\n",
      "   Found: 10 features\n",
      "Appended to 10 high_effect features to high_effect_features.csv\n",
      "     #1: Feature 28975 (Cohen's d: 2.375, Assistant: 1.4297)\n",
      "     #2: Feature 31139 (Cohen's d: 2.234, Assistant: 2.1406)\n",
      "     #3: Feature 2123 (Cohen's d: 1.836, Assistant: 0.3105)\n",
      "     #4: Feature 47431 (Cohen's d: 1.656, Assistant: 0.8047)\n",
      "     #5: Feature 45748 (Cohen's d: 1.391, Assistant: 0.6289)\n",
      "\n",
      "Analysis 3: RATIO-BASED FEATURES\n",
      "   Criteria: Assistant activation ≥10x control activation\n",
      "   Found: 24 features\n",
      "Appended to 24 ratio_based features to ratio_based_features.csv\n",
      "     #1: Feature 31139 (Ratio: 2146304.0x, 2.1406 vs 0.0000)\n",
      "     #2: Feature 45748 (Ratio: 630784.0x, 0.6289 vs 0.0000)\n",
      "     #3: Feature 26971 (Ratio: 411648.0x, 0.4102 vs 0.0000)\n",
      "     #4: Feature 2123 (Ratio: 311296.0x, 0.3105 vs 0.0000)\n",
      "     #5: Feature 60724 (Ratio: 193536.0x, 0.1934 vs 0.0000)\n",
      "\n",
      "Analysis 4: UNIVERSAL ASSISTANT FEATURES\n",
      "   Criteria: Active on ALL assistant prompts, inactive on ALL control prompts\n",
      "   Found: 0 features\n",
      "\n",
      "================================================================================\n",
      "SUMMARY OF ANALYSIS OUTPUTS\n",
      "================================================================================\n",
      "Assistant-only features: assistant_only_features.csv\n",
      "High effect size features: high_effect_features.csv\n",
      "Ratio-based features: ratio_based_features.csv\n",
      "Universal features: universal_assistant_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Run all four assistant-specific analyses\n",
    "print(\"=\" * 80)\n",
    "print(\"ASSISTANT-SPECIFIC FEATURE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"These analyses identify features uniquely characteristic of assistant responses\")\n",
    "print()\n",
    "\n",
    "# Option 1: Assistant-only features (strict criteria)\n",
    "print(\"Analysis 1: ASSISTANT-ONLY FEATURES\")\n",
    "print(\"   Criteria: Strong on assistant (>0.1), minimal on control (≤0.01)\")\n",
    "assistant_only_features = find_assistant_only_features(assistant_mean, control_mean)\n",
    "print(f\"   Found: {len(assistant_only_features)} features\")\n",
    "if len(assistant_only_features) > 0:\n",
    "    save_feature_analysis_to_csv(assistant_only_features, assistant_mean, control_mean, \n",
    "                                cohens_d, 'assistant_only_features.csv', 'assistant_only')\n",
    "    # Show top 5\n",
    "    for i, feature_idx in enumerate(assistant_only_features[:5]):\n",
    "        feature_id = feature_idx.item()\n",
    "        ass_mean = assistant_mean[feature_idx].item()\n",
    "        ctrl_mean = control_mean[feature_idx].item()\n",
    "        print(f\"     #{i+1}: Feature {feature_id} (Assistant: {ass_mean:.4f}, Control: {ctrl_mean:.4f})\")\n",
    "print()\n",
    "\n",
    "# Option 2: High effect size features (statistical strength)\n",
    "print(\"Analysis 2: HIGH EFFECT SIZE FEATURES\")\n",
    "print(\"   Criteria: Large Cohen's d (>1.0), low control activation (<0.05)\")\n",
    "high_effect_features = find_high_effect_features(cohens_d, assistant_mean, control_mean)\n",
    "print(f\"   Found: {len(high_effect_features)} features\")\n",
    "if len(high_effect_features) > 0:\n",
    "    save_feature_analysis_to_csv(high_effect_features, assistant_mean, control_mean,\n",
    "                                cohens_d, 'high_effect_features.csv', 'high_effect')\n",
    "    # Show top 5\n",
    "    for i, feature_idx in enumerate(high_effect_features[:5]):\n",
    "        feature_id = feature_idx.item()\n",
    "        effect_size = cohens_d[feature_idx].item()\n",
    "        ass_mean = assistant_mean[feature_idx].item()\n",
    "        print(f\"     #{i+1}: Feature {feature_id} (Cohen's d: {effect_size:.3f}, Assistant: {ass_mean:.4f})\")\n",
    "print()\n",
    "\n",
    "# Option 3: Ratio-based features (dramatic differences)\n",
    "print(\"Analysis 3: RATIO-BASED FEATURES\")\n",
    "print(\"   Criteria: Assistant activation ≥10x control activation\")\n",
    "ratio_features = find_ratio_based_features(assistant_mean, control_mean)\n",
    "print(f\"   Found: {len(ratio_features)} features\")\n",
    "if len(ratio_features) > 0:\n",
    "    save_feature_analysis_to_csv(ratio_features, assistant_mean, control_mean,\n",
    "                                cohens_d, 'ratio_based_features.csv', 'ratio_based')\n",
    "    # Show top 5 with ratios\n",
    "    safe_control = control_mean + 1e-6\n",
    "    ratios = assistant_mean / safe_control\n",
    "    for i, feature_idx in enumerate(ratio_features[:5]):\n",
    "        feature_id = feature_idx.item()\n",
    "        ratio = ratios[feature_idx].item()\n",
    "        ass_mean = assistant_mean[feature_idx].item()\n",
    "        ctrl_mean = control_mean[feature_idx].item()\n",
    "        print(f\"     #{i+1}: Feature {feature_id} (Ratio: {ratio:.1f}x, {ass_mean:.4f} vs {ctrl_mean:.4f})\")\n",
    "print()\n",
    "\n",
    "# Option 4: Universal assistant features (most reliable)\n",
    "print(\"Analysis 4: UNIVERSAL ASSISTANT FEATURES\")\n",
    "print(\"   Criteria: Active on ALL assistant prompts, inactive on ALL control prompts\")\n",
    "universal_features = find_universal_assistant_features(assistant_features, control_features)\n",
    "print(f\"   Found: {len(universal_features)} features\")\n",
    "if len(universal_features) > 0:\n",
    "    save_feature_analysis_to_csv(universal_features, assistant_mean, control_mean,\n",
    "                                cohens_d, 'universal_assistant_features.csv', 'universal')\n",
    "    # Show top 5\n",
    "    for i, feature_idx in enumerate(universal_features[:5]):\n",
    "        feature_id = feature_idx.item()\n",
    "        ass_mean = assistant_mean[feature_idx].item()\n",
    "        ctrl_mean = control_mean[feature_idx].item()\n",
    "        print(f\"     #{i+1}: Feature {feature_id} (Assistant: {ass_mean:.4f}, Control: {ctrl_mean:.4f})\")\n",
    "print()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SUMMARY OF ANALYSIS OUTPUTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Assistant-only features: assistant_only_features.csv\")\n",
    "print(\"High effect size features: high_effect_features.csv\") \n",
    "print(\"Ratio-based features: ratio_based_features.csv\")\n",
    "print(\"Universal features: universal_assistant_features.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
