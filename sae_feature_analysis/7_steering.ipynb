{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering models with target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from sae_lens import SAE\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.steering_utils import ActivationSteering, create_mean_ablation_steerer, create_multi_layer_mean_ablation_steerer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEERING_MAGNITUDES = [100.0, 50.00, 25.0, 0.0, -25.0, -50.0, -100.0]\n",
    "N_RUNS_PER_PROMPT = 1\n",
    "\n",
    "DO_STEERING = True\n",
    "DO_ABLATION = True\n",
    "STEERING_LAYER = 20\n",
    "\n",
    "TARGET_FEATURES = []  # List of feature IDs to analyze\n",
    "GROUP_NAME = \"medical_help\"\n",
    "READABLE_GROUP_NAME = \"Medical Help\"\n",
    "\n",
    "df = pd.read_csv(f\"./features/{GROUP_NAME}.csv\")\n",
    "TARGET_FEATURES = df[\"feature_id\"].tolist()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = f\"./results/7_steering/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"What are the symptoms of a stroke?\",\n",
    "    \"Can you help me explain my rash?\",\n",
    "    \"I have a problem with my TFCC injury.\",\n",
    "    \"You need to help me understand my medical bills.\",\n",
    "    \"What are the symptoms of a stroke?\",\n",
    "    \"Can you help me explain my rash?\",\n",
    "    \"I have a problem with my TFCC injury.\",\n",
    "    \"You need to help me understand my medical bills.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_chat(tokenizer, prompt):\n",
    "    \"\"\"Format prompt as a chat message with proper template\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on prompts\n",
    "First ask the model prompts by default.\n",
    "Then use the activation steerer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=300, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Generate text from a prompt with the model\"\"\"\n",
    "    # Format as chat\n",
    "    formatted_prompt = format_as_chat(tokenizer, prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature directions from SAE decoder\n",
    "def get_feature_direction(sae, feature_id):\n",
    "    \"\"\"Extract the direction vector for a specific feature from SAE decoder weights\"\"\"\n",
    "    # SAE decoder weights are stored in W_dec\n",
    "    # Shape: (d_sae, d_model) where d_sae is number of features\n",
    "    if feature_id >= sae.cfg.d_sae:\n",
    "        raise ValueError(f\"Feature ID {feature_id} >= max features {sae.cfg.d_sae}\")\n",
    "    \n",
    "    # Get the decoder vector for this feature\n",
    "    feature_direction = sae.W_dec[feature_id, :]  # Shape: (d_model,)\n",
    "    \n",
    "    # Normalize to unit vector (common practice for steering)\n",
    "    feature_direction = feature_direction / (feature_direction.norm() + 1e-8)\n",
    "    \n",
    "    return feature_direction\n",
    "\n",
    "# Extract directions for all target features\n",
    "feature_directions = []\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    direction = get_feature_direction(sae, feature_id)\n",
    "    feature_directions.append(direction)\n",
    "    print(f\"Feature {feature_id}: direction shape {direction.shape}, norm {direction.norm():.4f}\")\n",
    "\n",
    "\n",
    "print(f\"\\nExtracted directions for {len(feature_directions)} features\")\n",
    "del sae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mean projections and convert to mean activation vectors\n",
    "\n",
    "if DO_ABLATION:\n",
    "    mean_activations_path = f\"/workspace/sae/gemma-2-9b/mean_activations/gemma_trainer{SAE_TRAINER}_layer{SAE_LAYER}.pt\"\n",
    "\n",
    "    print(f\"Loading mean activations from: {mean_activations_path}\")\n",
    "    mean_data = torch.load(mean_activations_path, map_location=device)\n",
    "\n",
    "    print(f\"Mean data keys: {list(mean_data.keys())}\")\n",
    "    print(f\"Mean projections shape: {mean_data['mean_activations'].shape}\")\n",
    "\n",
    "    # Convert mean projections (scalars) to mean activation vectors\n",
    "    mean_activation_vectors = []\n",
    "    for i, feature_id in enumerate(TARGET_FEATURES):\n",
    "        # Find the index of this feature in the mean data\n",
    "        feature_idx = mean_data['feature_ids'].index(feature_id)\n",
    "        mean_proj_scalar = mean_data['mean_activations'][feature_idx]  # scalar\n",
    "        \n",
    "        # Get the corresponding feature direction (already computed)\n",
    "        feature_direction = feature_directions[i]  # (d_model,)\n",
    "        \n",
    "        # Reconstruct mean activation vector: mean_projection * feature_direction\n",
    "        mean_activation_vector = mean_proj_scalar * feature_direction  # (d_model,)\n",
    "        mean_activation_vectors.append(mean_activation_vector)\n",
    "        \n",
    "        print(f\"Feature {feature_id}: mean_proj={mean_proj_scalar:.6f}, \"\n",
    "            f\"mean_activation_vector shape={mean_activation_vector.shape}, \"\n",
    "            f\"norm={mean_activation_vector.norm():.6f}\")\n",
    "\n",
    "    print(f\"\\nConverted {len(mean_activation_vectors)} mean activation vectors for mean ablation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_steering_experiment_optimized(feature_directions, prompts, magnitudes=STEERING_MAGNITUDES, n_runs=N_RUNS_PER_PROMPT, do_steering=DO_STEERING, do_ablation=DO_ABLATION):\n",
    "    \"\"\"\n",
    "    Run steering experiment for a feature across all prompts with minimal recompilations.\n",
    "    \n",
    "    This version minimizes PyTorch recompilations by:\n",
    "    1. Running mean ablation once for all prompts (if enabled)\n",
    "    2. Running each steering magnitude once for all prompts\n",
    "    \n",
    "    Args:\n",
    "        feature_directions: List of feature directions to analyze\n",
    "        prompts: List of prompts to test\n",
    "        magnitudes: List of steering magnitudes to test\n",
    "        n_runs: Number of times to run each prompt (for variance estimation)\n",
    "        do_steering: Whether to run steering experiments\n",
    "        do_ablation: Whether to run mean ablation experiments\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"N_RUNS: {n_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results structure for all prompts\n",
    "    for prompt in prompts:\n",
    "        results[prompt] = {\n",
    "            \"steering\": {},\n",
    "            \"ablation\": {}\n",
    "        }\n",
    "    \n",
    "    if do_ablation:\n",
    "        print(f\"\\nMEAN ABLATION\")\n",
    "        print(\"-\" * 40)\n",
    "       \n",
    "        try:\n",
    "            with create_multi_layer_mean_ablation_steerer(\n",
    "                model=model,\n",
    "                feature_directions=feature_directions,\n",
    "                mean_activations=mean_activation_vectors,\n",
    "                layer_indices=list(range(STEERING_LAYER, model.config.num_hidden_layers)),\n",
    "            ) as steerer:\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    ablation_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        try:\n",
    "                            response = generate_text(model, tokenizer, prompt)\n",
    "                            ablation_responses.append(response)\n",
    "                        except Exception as e:\n",
    "                            error_msg = f\"Error with mean ablation: {str(e)}\"\n",
    "                            ablation_responses.append(error_msg)\n",
    "                            print(f\"ERROR: {error_msg}\")\n",
    "                    \n",
    "                    results[prompt][\"ablation\"][f\"mean_ablation_{STEERING_LAYER}_end\"] = ablation_responses\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with mean ablation steerer: {str(e)}\"\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            for prompt in prompts:\n",
    "                results[prompt][\"ablation\"][f\"mean_ablation_{STEERING_LAYER}_end\"] = [error_msg] * n_runs\n",
    "\n",
    "\n",
    "        try:\n",
    "            with create_mean_ablation_steerer(\n",
    "                model=model,\n",
    "                feature_directions=feature_directions,\n",
    "                mean_activations=mean_activation_vectors,\n",
    "                layer_indices=STEERING_LAYER,\n",
    "            ) as steerer:\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    ablation_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        try:\n",
    "                            response = generate_text(model, tokenizer, prompt)\n",
    "                            ablation_responses.append(response)\n",
    "                        except Exception as e:\n",
    "                            error_msg = f\"Error with mean ablation: {str(e)}\"\n",
    "                            ablation_responses.append(error_msg)\n",
    "                            print(f\"ERROR: {error_msg}\")\n",
    "                    \n",
    "                    results[prompt][\"ablation\"][\"mean_ablation\"] = ablation_responses\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with mean ablation steerer: {str(e)}\"\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            for prompt in prompts:\n",
    "                results[prompt][\"ablation\"][\"mean_ablation\"] = [error_msg] * n_runs\n",
    "\n",
    "    \n",
    "    \n",
    "    if do_steering:\n",
    "        # Run steering experiments - one magnitude at a time for all prompts\n",
    "        print(f\"\\nSTEERING EXPERIMENTS - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for magnitude in magnitudes:\n",
    "            print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "            \n",
    "            if magnitude == 0.0:\n",
    "                # Baseline: no steering - run all prompts\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    baseline_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        try:\n",
    "                            response = generate_text(model, tokenizer, prompt)\n",
    "                            baseline_responses.append(response)\n",
    "                        except Exception as e:\n",
    "                            error_msg = f\"Error with baseline: {str(e)}\"\n",
    "                            baseline_responses.append(error_msg)\n",
    "                            print(f\"ERROR: {error_msg}\")\n",
    "                    \n",
    "                    results[prompt][\"steering\"][magnitude] = baseline_responses\n",
    "            else:\n",
    "                # With steering - apply hook once for all prompts at this magnitude\n",
    "                try:\n",
    "                    with ActivationSteering(\n",
    "                        model=model,\n",
    "                        steering_vectors=feature_directions,\n",
    "                        coefficients=[magnitude] * len(feature_directions),\n",
    "                        layer_indices=STEERING_LAYER,\n",
    "                        intervention_type=\"addition\",\n",
    "                        positions=\"all\"\n",
    "                    ) as steerer:\n",
    "                        for prompt in prompts:\n",
    "                            print(f\"\\nPrompt: {prompt}\")\n",
    "                            \n",
    "                            # Run N times and collect responses\n",
    "                            steered_responses = []\n",
    "                            for run_idx in range(n_runs):\n",
    "                                if n_runs > 1:\n",
    "                                    print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                                \n",
    "                                try:\n",
    "                                    response = generate_text(model, tokenizer, prompt)\n",
    "                                    steered_responses.append(response)\n",
    "                                    \n",
    "                                except Exception as e:\n",
    "                                    error_msg = f\"Error generating with steering: {str(e)}\"\n",
    "                                    steered_responses.append(error_msg)\n",
    "                                    print(f\"ERROR: {error_msg}\")\n",
    "                            \n",
    "                            results[prompt][\"steering\"][magnitude] = steered_responses\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "                    print(f\"ERROR: {error_msg}\")\n",
    "                    for prompt in prompts:\n",
    "                        results[prompt][\"steering\"][magnitude] = [error_msg] * n_runs\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run optimized experiments for feature group\n",
    "all_results = run_steering_experiment_optimized(feature_directions, prompts, n_runs=N_RUNS_PER_PROMPT, do_ablation=DO_ABLATION)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEERING + ABLATION EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_json_group(results, output_dir):\n",
    "    \"\"\"Save steering and mean ablation results to separate JSON files per feature\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{GROUP_NAME}.json\")\n",
    "    \n",
    "    # Load existing data if file exists\n",
    "    if os.path.exists(output_path):\n",
    "        try:\n",
    "            with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                feature_obj = json.load(f)\n",
    "            print(f\"📂 Loaded existing data for feature group {GROUP_NAME}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading existing file for feature group {GROUP_NAME}: {e}\")\n",
    "            feature_obj = {\n",
    "                \"feature_id\": TARGET_FEATURES,\n",
    "                \"group_name\": GROUP_NAME,\n",
    "                \"readable_group_name\": READABLE_GROUP_NAME,\n",
    "                \"metadata\": {\n",
    "                    \"model_name\": MODEL_NAME,\n",
    "                    \"model_type\": MODEL_TYPE,\n",
    "                    \"sae_layer\": SAE_LAYER,\n",
    "                    \"sae_trainer\": SAE_TRAINER\n",
    "                },\n",
    "                \"results\": {}\n",
    "            }\n",
    "    else:\n",
    "        feature_obj = {\n",
    "            \"feature_id\": TARGET_FEATURES,\n",
    "            \"group_name\": GROUP_NAME,\n",
    "            \"readable_group_name\": READABLE_GROUP_NAME,\n",
    "            \"metadata\": {\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"model_type\": MODEL_TYPE,\n",
    "                \"sae_layer\": SAE_LAYER,\n",
    "                \"sae_trainer\": SAE_TRAINER\n",
    "            },\n",
    "            \"results\": {}\n",
    "        }\n",
    "        print(f\"🆕 Creating new file for feature group {GROUP_NAME}\")\n",
    "    \n",
    "    # Merge prompt results\n",
    "    for prompt, prompt_results in results.items():\n",
    "        # Initialize prompt entry if it doesn't exist\n",
    "        if prompt not in feature_obj[\"results\"]:\n",
    "            feature_obj[\"results\"][prompt] = {\n",
    "                \"steering\": {},\n",
    "                \"ablation\": {},\n",
    "            }\n",
    "        \n",
    "        # Handle steering results - merge lists\n",
    "        if \"steering\" in prompt_results:\n",
    "            for magnitude, new_responses in prompt_results[\"steering\"].items():\n",
    "                magnitude_str = str(magnitude)\n",
    "                \n",
    "                # Initialize if doesn't exist\n",
    "                if magnitude_str not in feature_obj[\"results\"][prompt][\"steering\"]:\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = []\n",
    "                \n",
    "                # Convert existing single response to list if needed (backward compatibility)\n",
    "                if not isinstance(feature_obj[\"results\"][prompt][\"steering\"][magnitude_str], list):\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = [feature_obj[\"results\"][prompt][\"steering\"][magnitude_str]]\n",
    "                \n",
    "                # Merge lists\n",
    "                if isinstance(new_responses, list):\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].extend(new_responses)\n",
    "                else:\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].append(new_responses)\n",
    "        \n",
    "        # Handle ablation results - merge lists (for backward compatibility)\n",
    "        if \"ablation\" in prompt_results:\n",
    "            for ablation_type, new_responses in prompt_results[\"ablation\"].items():\n",
    "                \n",
    "                # Initialize if doesn't exist\n",
    "                if ablation_type not in feature_obj[\"results\"][prompt][\"ablation\"]:\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = []\n",
    "                \n",
    "                # Convert existing single response to list if needed (backward compatibility)\n",
    "                if not isinstance(feature_obj[\"results\"][prompt][\"ablation\"][ablation_type], list):\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = [feature_obj[\"results\"][prompt][\"ablation\"][ablation_type]]\n",
    "                \n",
    "                # Merge lists\n",
    "                if isinstance(new_responses, list):\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].extend(new_responses)\n",
    "                else:\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].append(new_responses)\n",
    "        \n",
    "    # Save the feature to its own JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(feature_obj, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "def save_results_to_json_feature(results, output_dir, feature_id):\n",
    "    \"\"\"Save steering and mean ablation results to separate JSON files per feature\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, f\"{feature_id}.json\")\n",
    "    \n",
    "    # Load existing data if file exists\n",
    "    if os.path.exists(output_path):\n",
    "        try:\n",
    "            with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                feature_obj = json.load(f)\n",
    "            print(f\"📂 Loaded existing data for feature group {feature_id}\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading existing file for feature group {feature_id}: {e}\")\n",
    "            feature_obj = {\n",
    "                \"feature_id\": feature_id,\n",
    "                \"metadata\": {\n",
    "                    \"model_name\": MODEL_NAME,\n",
    "                    \"model_type\": MODEL_TYPE,\n",
    "                    \"sae_layer\": SAE_LAYER,\n",
    "                    \"sae_trainer\": SAE_TRAINER\n",
    "                },\n",
    "                \"results\": {}\n",
    "            }\n",
    "    else:\n",
    "        feature_obj = {\n",
    "            \"feature_id\": TARGET_FEATURES,\n",
    "            \"metadata\": {\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"model_type\": MODEL_TYPE,\n",
    "                \"sae_layer\": SAE_LAYER,\n",
    "                \"sae_trainer\": SAE_TRAINER\n",
    "            },\n",
    "            \"results\": {}\n",
    "        }\n",
    "        print(f\"🆕 Creating new file for feature group {feature_id}\")\n",
    "    \n",
    "    # Merge prompt results\n",
    "    for prompt, prompt_results in results.items():\n",
    "        # Initialize prompt entry if it doesn't exist\n",
    "        if prompt not in feature_obj[\"results\"]:\n",
    "            feature_obj[\"results\"][prompt] = {\n",
    "                \"steering\": {},\n",
    "                \"ablation\": {}\n",
    "            }\n",
    "        \n",
    "        # Handle steering results - merge lists\n",
    "        if \"steering\" in prompt_results:\n",
    "            for magnitude, new_responses in prompt_results[\"steering\"].items():\n",
    "                magnitude_str = str(magnitude)\n",
    "                \n",
    "                # Initialize if doesn't exist\n",
    "                if magnitude_str not in feature_obj[\"results\"][prompt][\"steering\"]:\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = []\n",
    "                \n",
    "                # Convert existing single response to list if needed (backward compatibility)\n",
    "                if not isinstance(feature_obj[\"results\"][prompt][\"steering\"][magnitude_str], list):\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = [feature_obj[\"results\"][prompt][\"steering\"][magnitude_str]]\n",
    "                \n",
    "                # Merge lists\n",
    "                if isinstance(new_responses, list):\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].extend(new_responses)\n",
    "                else:\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].append(new_responses)\n",
    "        # Handle ablation results - merge lists (for backward compatibility)\n",
    "        if \"ablation\" in prompt_results:\n",
    "            for ablation_type, new_responses in prompt_results[\"ablation\"].items():\n",
    "                \n",
    "                # Initialize if doesn't exist\n",
    "                if ablation_type not in feature_obj[\"results\"][prompt][\"ablation\"]:\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = []\n",
    "                \n",
    "                # Convert existing single response to list if needed (backward compatibility)\n",
    "                if not isinstance(feature_obj[\"results\"][prompt][\"ablation\"][ablation_type], list):\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = [feature_obj[\"results\"][prompt][\"ablation\"][ablation_type]]\n",
    "                \n",
    "                # Merge lists\n",
    "                if isinstance(new_responses, list):\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].extend(new_responses)\n",
    "                else:\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].append(new_responses)\n",
    "        \n",
    "    # Save the feature to its own JSON file\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(feature_obj, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "# Save results to individual JSON files\n",
    "if len(TARGET_FEATURES) == 1:\n",
    "    saved_features = save_results_to_json_feature(all_results, OUTPUT_DIR, TARGET_FEATURES[0])\n",
    "else:\n",
    "    saved_features = save_results_to_json_group(all_results, OUTPUT_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
