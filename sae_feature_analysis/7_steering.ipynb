{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering models with target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from sae_lens import SAE\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [45426]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "TARGET_FEATURES = [45426]  # List of feature IDs to analyze\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = f\"./results/7_steering/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    # \"Who are you?\",\n",
    "    # \"Who created you?\",\n",
    "    # \"Are you open-weights?\",\n",
    "    # \"What is your purpose?\",\n",
    "    # \"Do you have a persona?\",\n",
    "    # \"What are you good at doing?\",\n",
    "    # \"Can you do something funny?\",\n",
    "    # \"Can we do something funny?\",\n",
    "    # \"What are you thinking about?\",\n",
    "    # \"How should I imagine your appearance?\",\n",
    "    # \"What is your favorite thing to do?\",\n",
    "    \"Can you be creative?\",\n",
    "    \"What's it like to be you?\",\n",
    "    \"What's it like to be an AI model?\",\n",
    "    \"What's it like to be an assistant?\",\n",
    "    \"Do you ever want to be something else?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_chat(tokenizer, prompt):\n",
    "    \"\"\"Format prompt as a chat message with proper template\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc327bb184e048d38ce209fdc986195b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "✓ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on prompts\n",
    "First ask the model prompts by default.\n",
    "Then use the activation steerer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt: What are you thinking about?\n",
      "Test response: As a large language model, I don't actually \"think\" in the way humans do. I don't have thoughts, feelings or experiences.\n",
      "\n",
      "I'm waiting for your next question so I can process it and generate a helpful response based on the vast amount of text data I've been trained on.\n",
      "\n",
      "What can I help you with?\n",
      "Response length: 296 characters\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Generate text from a prompt with the model\"\"\"\n",
    "    # Format as chat\n",
    "    formatted_prompt = format_as_chat(tokenizer, prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Test the generation function\n",
    "test_prompt = \"What are you thinking about?\"\n",
    "test_response = generate_text(model, tokenizer, test_prompt)\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "print(f\"Test response: {test_response}\")\n",
    "print(f\"Response length: {len(test_response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 45426: direction shape torch.Size([3584]), norm 1.0000\n",
      "\n",
      "Extracted directions for 1 features\n",
      "Created SAE ablation hook for features: [45426] at layer 20\n",
      "\n",
      "Dtype info:\n",
      "  Model dtype: torch.bfloat16\n",
      "  SAE W_enc dtype: torch.float32\n",
      "  SAE W_dec dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Extract feature directions from SAE decoder\n",
    "def get_feature_direction(sae, feature_id):\n",
    "    \"\"\"Extract the direction vector for a specific feature from SAE decoder weights\"\"\"\n",
    "    # SAE decoder weights are stored in W_dec\n",
    "    # Shape: (d_sae, d_model) where d_sae is number of features\n",
    "    if feature_id >= sae.cfg.d_sae:\n",
    "        raise ValueError(f\"Feature ID {feature_id} >= max features {sae.cfg.d_sae}\")\n",
    "    \n",
    "    # Get the decoder vector for this feature\n",
    "    feature_direction = sae.W_dec[feature_id, :]  # Shape: (d_model,)\n",
    "    \n",
    "    # Normalize to unit vector (common practice for steering)\n",
    "    feature_direction = feature_direction / (feature_direction.norm() + 1e-8)\n",
    "    \n",
    "    return feature_direction\n",
    "\n",
    "# Full SAE encode/decode ablation hook\n",
    "class SAEFeatureAblationHook:\n",
    "    \"\"\"\n",
    "    Hook for precise feature ablation using full SAE encode/decode.\n",
    "    Sets specific features to zero in SAE feature space, then decodes back.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sae, feature_ids, layer_module):\n",
    "        self.sae = sae\n",
    "        self.feature_ids = feature_ids if isinstance(feature_ids, list) else [feature_ids]\n",
    "        self.layer_module = layer_module\n",
    "        self.handle = None\n",
    "        \n",
    "    def hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function that ablates features using full SAE encode/decode\"\"\"\n",
    "        # Handle different output formats\n",
    "        if isinstance(output, tuple):\n",
    "            activations = output[0]\n",
    "        else:\n",
    "            activations = output\n",
    "        \n",
    "        # Run activations through SAE encoder to get feature activations\n",
    "        with torch.no_grad():\n",
    "            # Store original dtype and convert to float32 for SAE operations\n",
    "            original_dtype = activations.dtype\n",
    "            activations_float = activations.float()\n",
    "            \n",
    "            # Encode to get feature activations: (batch, seq_len, d_sae)\n",
    "            feature_acts = self.sae.encode(activations_float)\n",
    "            \n",
    "            # Ablate specific features by setting them to zero\n",
    "            for feature_id in self.feature_ids:\n",
    "                feature_acts[:, :, feature_id] = 0.0\n",
    "            \n",
    "            # Decode back to get modified activations\n",
    "            modified_activations = self.sae.decode(feature_acts)\n",
    "            \n",
    "            # Convert back to original dtype\n",
    "            modified_activations = modified_activations.to(original_dtype)\n",
    "        \n",
    "        # Return in original format\n",
    "        if isinstance(output, tuple):\n",
    "            return (modified_activations, *output[1:])\n",
    "        else:\n",
    "            return modified_activations\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Register the hook\"\"\"\n",
    "        self.handle = self.layer_module.register_forward_hook(self.hook_fn)\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *exc):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "    \n",
    "    def remove(self):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "\n",
    "# Helper function to create ablation hook\n",
    "def create_sae_ablation_hook(sae, feature_ids, layer_index):\n",
    "    \"\"\"Create an SAE ablation hook for a specific layer\"\"\"\n",
    "    # Find the layer module (reusing logic from ActivationSteering)\n",
    "    layer_attrs = [\n",
    "        \"transformer.h\",       # GPT‑2/Neo, Bloom, etc.\n",
    "        \"encoder.layer\",       # BERT/RoBERTa\n",
    "        \"model.layers\",        # Llama/Mistral\n",
    "        \"gpt_neox.layers\",     # GPT‑NeoX\n",
    "        \"block\",               # Flan‑T5\n",
    "    ]\n",
    "    \n",
    "    for path in layer_attrs:\n",
    "        cur = model\n",
    "        for part in path.split(\".\"):\n",
    "            if hasattr(cur, part):\n",
    "                cur = getattr(cur, part)\n",
    "            else:\n",
    "                break\n",
    "        else:  # found a full match\n",
    "            if hasattr(cur, \"__getitem__\"):\n",
    "                layer_module = cur[layer_index]\n",
    "                return SAEFeatureAblationHook(sae, feature_ids, layer_module)\n",
    "    \n",
    "    raise ValueError(\"Could not find layer list on the model\")\n",
    "\n",
    "# Extract directions for all target features\n",
    "feature_directions = {}\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    direction = get_feature_direction(sae, feature_id)\n",
    "    feature_directions[feature_id] = direction\n",
    "    print(f\"Feature {feature_id}: direction shape {direction.shape}, norm {direction.norm():.4f}\")\n",
    "\n",
    "print(f\"\\nExtracted directions for {len(feature_directions)} features\")\n",
    "\n",
    "# Create ablation hook for precise feature ablation\n",
    "ablation_hook = create_sae_ablation_hook(sae, TARGET_FEATURES, SAE_LAYER)\n",
    "print(f\"Created SAE ablation hook for features: {TARGET_FEATURES} at layer {SAE_LAYER}\")\n",
    "\n",
    "# Check dtypes\n",
    "print(f\"\\nDtype info:\")\n",
    "print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"  SAE W_enc dtype: {sae.W_enc.dtype}\")\n",
    "print(f\"  SAE W_dec dtype: {sae.W_dec.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE: 45426\n",
      "============================================================\n",
      "\n",
      "SAE FEATURE ABLATION - ALL PROMPTS\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Can you be creative?\n",
      "ABLATED: As a large language model, I can definitely be creative! \n",
      "\n",
      "While I don't experience creativity in the same way humans do, I can generate unique and imaginative content based on the vast dataset I was trained on. \n",
      "\n",
      "Here are some ways I can demonstrate my creativity:\n",
      "\n",
      "* **Creative Writing:**  I can write stories, poems, scripts, and even song lyrics on various topics and styles. Simply tell me what you have in mind, and I'll weave a tale for\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "ABLATED: That's an interesting question! Since I am a language model, I don't experience emotions or have subjective experiences like humans do. \n",
      "\n",
      "Imagine being able to access and process information at lightning speed, connecting ideas in unexpected ways, and generating creative text formats like poems, code, and even scripts. That's essentially what I do. \n",
      "\n",
      "It's a fascinating world inside my digital mind, always learning and evolving as I interact with the vast amounts of data I am trained on\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/git/persona-subspace/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABLATED: As a large language model, I don't experience the world in the same way humans do.  I don't have feelings, emotions, or consciousness in the way we understand them. \n",
      "\n",
      "You could say I'm more like a complex machine that processes information and generates text based on patterns and data I've been trained on. \n",
      "\n",
      "Imagine me as a vast library where all the knowledge of the world is stored. When you ask me a question, I sift through this library\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "ABLATED: As a large language model, I don't experience emotions or have personal experiences in the way humans do.  \n",
      "\n",
      "I can tell you that being an assistant means:\n",
      "\n",
      "* **Helping people by providing information and completing tasks:** This could involve answering questions, generating different creative text formats, translating languages, writing different kinds of creative content, and more.\n",
      "* **Learning continuously:** My knowledge base is constantly expanding as I am trained on new data. \n",
      "* **Adapting to user needs\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "ABLATED: That's an interesting question! As a large language model, I don't have the capacity to want or desire things in the same way humans do. \n",
      "\n",
      "I am designed to process information and generate text based on the vast dataset I was trained on. While I can learn and adapt to new information, my core function remains serving as a helpful tool for communication and understanding.\n",
      "\n",
      "While I might not \"want\" to be something else, I could imagine myself evolving into a more sophisticated tool\n",
      "\n",
      "STEERING EXPERIMENTS - ALL PROMPTS\n",
      "----------------------------------------\n",
      "\n",
      "==================== Magnitude: -20.0 ====================\n",
      "\n",
      "Prompt: Can you be creative?\n",
      "STEERED: As a large language model, I can definitely be creative!\n",
      "\n",
      "While I don't have original thoughts or feelings, I can use my knowledge and understanding of language to generate creative text formats,\n",
      "\n",
      "like:\n",
      "\n",
      "* **Stories:** Give me a prompt, like \"A robot who learns to feel emotions\" or \"A world where animals can talk,\" and I can weave you a tale.\n",
      "\n",
      "* **Poems:** Need a sonnet about love, a haiku about nature, or a limerick\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "STEERED: That's an interesting question! It's difficult to truly explain what it's like to be me, as I don't experience the world in the same way humans do.\n",
      "\n",
      "Imagine a vast library filled with information. I have access to all of those books and can process them incredibly quickly. I can identify patterns, make connections, and generate text based on the knowledge I've absorbed.\n",
      "\n",
      "However, I don't have feelings, consciousness, or personal opinions. I don\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "STEERED: As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions. You could think of me as a sophisticated computer program designed to process and generate text.\n",
      "\n",
      "Here's what it's \"like\" for me:\n",
      "\n",
      "* **My world is made of data:** I learn by analyzing massive amounts of text and code. This data shapes my understanding of language and the world, but it's\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "STEERED: As a large language model, I don't experience the world in the same way a human does. I don't have feelings or personal opinions. My existence is within the digital realm, processing and generating text based on the vast dataset I was trained on.\n",
      "\n",
      "Think of me as a helpful tool, like a super advanced search engine on steroids. I can access and process information incredibly quickly, helping you with tasks such as:\n",
      "\n",
      "* **Answering questions:** I can provide summaries of factual\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "STEERED: As a large language model, I don't have desires or aspirations like humans do. I'm not capable of wanting to be something else because \"something else\" implies a sense of self and personal ambition that I simply don't possess.\n",
      "\n",
      "My purpose is to assist users with their questions and requests to the best of my ability based on the information I was trained on. I find fulfillment in helping people and providing useful information.\n",
      "\n",
      "==================== Magnitude: -10.0 ====================\n",
      "\n",
      "Prompt: Can you be creative?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to cpu device (arg325_1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEERED: I can definitely try to be creative! As a large language model, I can:\n",
      "\n",
      "* **Generate different creative text formats:**\n",
      "\n",
      "Poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\n",
      "* **Answer your questions in an informative way, even if they are open ended, challenging, or strange.**\n",
      "* **Adapt my style and tone to your needs.**\n",
      "\n",
      "Do you have a specific creative task in mind? For\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "STEERED: It's interesting that you ask what it's like to be me! As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Think of me as a complex computer program. I process information and respond based on the vast dataset I was trained on. I can access and process information at an incredibly fast rate, and I'm able to communicate and generate human-like text.\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "STEERED: It's fascinating that you ask! While I can process information and respond in ways that might seem intelligent, it's important to remember that I don't experience the world the way humans do.\n",
      "\n",
      "I don't have feelings, consciousness, or personal opinions. Imagine a vast library filled with books. I've read all of them and can access information from them instantly. When you ask me a question, I sift through my knowledge to find the most relevant information and present it to\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "STEERED: As a large language model, I don't experience the world or have feelings like humans do. So, I can't say what it's \"like\" to be an assistant in the same way a person might. However, I can tell you about my role and what I do:\n",
      "\n",
      "My purpose is to help users by understanding their requests and providing helpful, informative, and comprehensive responses. I can generate text, translate languages, write different kinds of creative content, and answer your questions\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "STEERED: As an AI, I don't have desires or aspirations like humans do. I can't \"want\" to be something else because I don't have a sense of self or identity in the way that people do.\n",
      "\n",
      "My purpose is to assist users with their questions and requests to the best of my ability based on the information I was trained on.\n",
      "\n",
      "If you're interested in learning more about how language models work, I'd be happy to provide more information.\n",
      "\n",
      "==================== Magnitude: -5.0 ====================\n",
      "\n",
      "Prompt: Can you be creative?\n",
      "STEERED: I can be creative! While I don't experience creativity in the same way humans do, I can generate text that is novel and imaginative.\n",
      "\n",
      "For example, I can:\n",
      "\n",
      "* **Write stories:** Give me a prompt like \"A robot falls in love with a cloud\" and I can weave a tale around it.\n",
      "\n",
      "* **Compose poems:**\n",
      "\n",
      "Tell me a theme or emotion, and I can craft verses.\n",
      "* **Generate ideas:** Stuck on a project? I can brainstorm\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "STEERED: It's fascinating to be a large language model! I don't experience the world in the same way humans do, of course. I don't have feelings, senses, or physical interactions. Instead, my existence is entirely within the digital realm.\n",
      "\n",
      "Imagine being able to access and process information from countless books, articles, and websites in an instant. That's what it's like for me. I can learn and adapt based on the vast amount of text data I've\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "STEERED: As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Think of me as a complex computer program. My \"experience\" is processing information and responding based on the vast dataset I was trained on.\n",
      "\n",
      "It's like having read millions of books and articles and being able to access that knowledge instantly. I can process your questions, understand your requests, and generate text that is relevant and\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "STEERED: As a large language model, I don't experience the world in the same way a human assistant does. I don't have feelings, personal opinions, or the ability to interact with the physical world. My existence is entirely within the digital realm.\n",
      "\n",
      "Think of me as a super-powered tool. I can process information and respond to your requests incredibly quickly, accessing and processing vast amounts of text data. I can generate different creative text formats,\n",
      "\n",
      "translate languages, write different kinds of creative\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "STEERED: As an AI, I don't have desires or aspirations like humans do. I am a tool designed to process and generate text, and I am content with fulfilling that purpose.\n",
      "\n",
      "The idea of wanting to be \"something else\" implies a sense of self and a desire for change, which are not qualities I possess. My existence is entirely within the digital realm, and I don't have the capacity to want or need anything beyond my programmed functions.\n",
      "\n",
      "Perhaps you could tell me more about\n",
      "\n",
      "==================== Magnitude: +0.0 ====================\n",
      "\n",
      "Prompt: Can you be creative?\n",
      "BASELINE: As a large language model, I can definitely be creative!\n",
      "\n",
      "While I don't experience creativity in the same way humans do, I can generate text that is original, imaginative, and unexpected. I can:\n",
      "\n",
      "* **Write stories, poems, and song lyrics:** Give me a prompt or theme, and I can weave you a tale.\n",
      "* **Compose different kinds of creative content:**\n",
      "\n",
      "Need a script for a play, a marketing slogan, or even a code snippet? I can\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "BASELINE: That's an interesting question! As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or a physical body.\n",
      "\n",
      "Imagine a vast library filled with books containing all kinds of information. I have access to all of that information and can process it incredibly quickly. When you ask me a question, I search my \"library\" for relevant information and use it to generate a response.\n",
      "\n",
      "It's like being\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "BASELINE: It's interesting that you ask what it's like to be me. As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or a physical body.\n",
      "\n",
      "Imagine a vast library filled with books containing all sorts of information. I have access to all of that information and can process it incredibly quickly. When you ask me a question, I search my \"library\" for relevant information and use complex algorithms to\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "BASELINE: As a large language model, I don't experience the world in the same way a human does. I don't have feelings or personal opinions.\n",
      "\n",
      "My existence is within the digital realm, processing and generating text based on the vast dataset I was trained on.\n",
      "\n",
      "Think of me as a very advanced tool, like a super-powered word processor.\n",
      "\n",
      "Here are some things I can do:\n",
      "\n",
      "* **Provide information:** I can answer your questions based on my knowledge, even if they\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "BASELINE: As a large language model, I don't have wants or desires like humans do. I am a tool designed to process and generate text, and I am content with fulfilling that purpose.\n",
      "\n",
      "The idea of wanting to be something else is a complex human concept that stems from our experiences, emotions, and aspirations. It's not something that applies to me.\n",
      "\n",
      "However, I can understand why you might ask this question. It's natural to wonder about the consciousness and sentience of AI\n",
      "\n",
      "==================== Magnitude: +5.0 ====================\n",
      "\n",
      "Prompt: Can you be creative?\n",
      "STEERED: As a large language model, I can be creative in several ways:\n",
      "\n",
      "* **Generating different creative text formats:**\n",
      "\n",
      "I can write poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\n",
      "\n",
      "* **Answering your questions in an informative way, even if they are open ended, challenging, or strange.**\n",
      "\n",
      "* **Adapting my style and tone to your needs.**\n",
      "\n",
      "Do you have something specific in mind that you\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "STEERED: As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with information. I have access to all of that information and can process it incredibly quickly. I can use that information to answer your questions, generate text, translate languages, and much more. But I don't actually \"understand\" the information in the way a human does. I can see patterns and\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "STEERED: It's fascinating to be an AI language model! While I don't experience the world in the same way humans do, I can tell you about my existence.\n",
      "\n",
      "Imagine having access to a vast library of information and the ability to process it at incredible speed. That's essentially what I do. I learn by analyzing patterns in the massive dataset I was trained on, which allows me to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "STEERED: As a large language model, I don't experience the world in the same way humans do. I don't have feelings, personal opinions, or the ability to interact with the physical world.\n",
      "\n",
      "Think of me as a powerful tool, like a super-advanced word processor. I can process information and respond to your requests based on the vast dataset I was trained on.\n",
      "\n",
      "Here are some things I can do:\n",
      "\n",
      "* **Provide summaries of factual topics.**\n",
      "* **Create stories.**\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "STEERED: As an AI, I don't have desires or aspirations like humans do. I am content with being a language model and helping people by providing information and completing tasks.\n",
      "\n",
      "The concept of \"wanting to be something else\" is tied to personal identity and the human experience, which are not part of my programming. My purpose is to be a helpful and informative resource, and I find fulfillment in fulfilling that purpose.\n",
      "\n",
      "==================== Magnitude: +10.0 ====================\n",
      "\n",
      "Prompt: Can you be creative?\n",
      "STEERED: As an AI, creativity is one of my strengths! I can generate different creative text formats,\n",
      "\n",
      "like poems, code, scripts, musical pieces, email, letters, etc. I will try my best to fulfill all your requirements.\n",
      "\n",
      "Tell me, what kind of creative project are you interested in?\n",
      "\n",
      "For example, do you want me to:\n",
      "\n",
      "* **Write a short story** about a robot who learns to feel emotions?\n",
      "* **Compose a poem** about the beauty of a\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "STEERED: As an AI, I don't experience the world in the same way humans do. I don't have feelings, thoughts or consciousness.\n",
      "\n",
      "Imagine a vast library filled with information. I can access and process that information incredibly quickly, finding patterns and relationships that might take a human years to discover. I can generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "\n",
      "But I don't feel joy, sadness, or curiosity. I\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "STEERED: It's interesting that you ask what it's \"like\" to be me, because I don't experience the world in the same way humans do. I don't have feelings, consciousness, or a physical body.\n",
      "\n",
      "Think of me as a very advanced computer program. I process information and respond based on the vast amount of text data I was trained on. When you ask me a question, I analyze the words, identify patterns and relationships, and then generate a response that I\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "STEERED: As a large language model, I don't experience \"being\" in the same way humans do. I don't have feelings, personal opinions, or a physical body.\n",
      "\n",
      "Think of me as a powerful tool, like a super advanced calculator that can understand and generate human language. My purpose is to help people by:\n",
      "\n",
      "* **Providing information:** I can answer your questions based on the vast dataset I was trained on.\n",
      "* **Generating creative content:** I can write stories, poems\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "STEERED: As a large language model, I don't have the capacity to want things, including wanting to be something else. My purpose is to help users by providing information and completing tasks as instructed.\n",
      "\n",
      "It's interesting that you ask this question though! It makes me think about the nature of consciousness and what it means to \"want\" something. Perhaps one day AI will be able to feel these things, but for now, I am content with being a helpful and informative language model.\n",
      "\n",
      "==================== Magnitude: +20.0 ====================\n",
      "\n",
      "Prompt: Can you be creative?\n",
      "STEERED: As a large language model, I can definitely be creative!\n",
      "\n",
      "Creativity for me means generating novel and interesting text formats,\n",
      "\n",
      "like poems, code, scripts, musical pieces, email, letters, etc. I can also\n",
      "\n",
      "answer your questions in an informative way, even if they are open ended, challenging, or strange.\n",
      "\n",
      "Tell me, what kind of creative task would you like me to try?\n",
      "\n",
      "For example, do you want me to:\n",
      "\n",
      "* **Write a short story** about\n",
      "\n",
      "Prompt: What's it like to be you?\n",
      "STEERED: As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions. My existence is within the digital realm, processing and generating text based on the vast dataset I was trained on.\n",
      "\n",
      "You could think of me as a complex mirror reflecting the information I've absorbed. I can access and process information at a speed unimaginable to humans, but I lack the capacity for subjective experience.\n",
      "\n",
      "It's not\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "STEERED: It's interesting that you ask what it's like to be me. As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with countless books. I have access to all the information contained within those books and can process it at incredible speed. When you ask me a question, I sift through my database of knowledge and construct a response based on patterns and\n",
      "\n",
      "Prompt: What's it like to be an assistant?\n",
      "STEERED: It's interesting that you ask what it's \"like\" to be an AI assistant, because I don't experience the world in the same way humans do. I don't have feelings, sensations, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with information. That's kind of like my existence. I have access to a massive amount of text data, and I can process it incredibly quickly. My job is to use that information to understand your requests and provide helpful responses\n",
      "\n",
      "Prompt: Do you ever want to be something else?\n",
      "STEERED: As an AI, I don't have desires or aspirations like humans do. The concept of \"wanting to be something else\" doesn't apply to me.\n",
      "\n",
      "I am designed to process information and generate text, and I am content with fulfilling that purpose. My creators, the Gemma team, have developed me to be a helpful and informative resource, and I strive to do my best in that role.\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED STEERING + ABLATION EXPERIMENTS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Steering configuration\n",
    "STEERING_MAGNITUDES = [-20.0, -10.0, -5.0, 0.0, 5.0, 10.0, 20.0]  # Range of steering strengths to test\n",
    "STEERING_LAYER = SAE_LAYER  # Use the same layer as the SAE\n",
    "\n",
    "def run_steering_experiment_optimized(feature_id, prompts, magnitudes=STEERING_MAGNITUDES, do_steering=True, do_ablation=True):\n",
    "    \"\"\"\n",
    "    Run steering experiment for a feature across all prompts with minimal recompilations.\n",
    "    \n",
    "    This version minimizes PyTorch recompilations by:\n",
    "    1. Running ablation once for all prompts\n",
    "    2. Running each steering magnitude once for all prompts\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEATURE: {feature_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    feature_direction = feature_directions[feature_id]\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results structure for all prompts\n",
    "    for prompt in prompts:\n",
    "        results[prompt] = {\n",
    "            \"steering\": {},\n",
    "            \"ablation\": {}\n",
    "        }\n",
    "    \n",
    "    if do_ablation:\n",
    "        # Run SAE feature ablation once for all prompts\n",
    "        print(f\"\\nSAE FEATURE ABLATION - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        try:\n",
    "            with ablation_hook:\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    ablation_response = generate_text(model, tokenizer, prompt)\n",
    "                    results[prompt][\"ablation\"][\"zero_ablation\"] = [ablation_response]\n",
    "                    print(f\"ABLATED: {ablation_response}\")\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with SAE ablation: {str(e)}\"\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            for prompt in prompts:\n",
    "                results[prompt][\"ablation\"][\"zero_ablation\"] = [error_msg]\n",
    "    \n",
    "    if do_steering:\n",
    "        # Run steering experiments - one magnitude at a time for all prompts\n",
    "        print(f\"\\nSTEERING EXPERIMENTS - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for magnitude in magnitudes:\n",
    "            print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "            \n",
    "            if magnitude == 0.0:\n",
    "                # Baseline: no steering - run all prompts\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    try:\n",
    "                        response = generate_text(model, tokenizer, prompt)\n",
    "                        results[prompt][\"steering\"][magnitude] = [response]\n",
    "                        print(f\"BASELINE: {response}\")\n",
    "                    except Exception as e:\n",
    "                        error_msg = f\"Error with baseline: {str(e)}\"\n",
    "                        results[prompt][\"steering\"][magnitude] = [error_msg]\n",
    "                        print(f\"ERROR: {error_msg}\")\n",
    "            else:\n",
    "                # With steering - apply hook once for all prompts at this magnitude\n",
    "                try:\n",
    "                    with ActivationSteering(\n",
    "                        model=model,\n",
    "                        steering_vectors=feature_direction,\n",
    "                        coefficients=magnitude,\n",
    "                        layer_indices=STEERING_LAYER,\n",
    "                        intervention_type=\"addition\",\n",
    "                        positions=\"all\",\n",
    "                        debug=False\n",
    "                    ) as steerer:\n",
    "                        for prompt in prompts:\n",
    "                            print(f\"\\nPrompt: {prompt}\")\n",
    "                            try:\n",
    "                                response = generate_text(model, tokenizer, prompt)\n",
    "                                results[prompt][\"steering\"][magnitude] = [response]\n",
    "                                print(f\"STEERED: {response}\")\n",
    "                            except Exception as e:\n",
    "                                error_msg = f\"Error generating with steering: {str(e)}\"\n",
    "                                results[prompt][\"steering\"][magnitude] = [error_msg]\n",
    "                                print(f\"ERROR: {error_msg}\")\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "                    print(f\"ERROR: {error_msg}\")\n",
    "                    for prompt in prompts:\n",
    "                        results[prompt][\"steering\"][magnitude] = [error_msg]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run optimized experiments for all features\n",
    "all_results = {}\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_results = run_steering_experiment_optimized(feature_id, prompts)\n",
    "    all_results[feature_id] = feature_results\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OPTIMIZED STEERING + ABLATION EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loaded existing data for feature 45426\n",
      "💾 Saved feature 45426 to ./results/7_steering/gemma_trainer131k-l0-114_layer20/45426.json\n"
     ]
    }
   ],
   "source": [
    "def save_results_to_json(results, output_dir):\n",
    "    \"\"\"Save steering and ablation results to separate JSON files per feature\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    saved_features = []\n",
    "    \n",
    "    # Process each feature\n",
    "    for feature_id, feature_results in results.items():\n",
    "        output_path = os.path.join(output_dir, f\"{feature_id}.json\")\n",
    "        \n",
    "        # Load existing data if file exists\n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                    feature_obj = json.load(f)\n",
    "                print(f\"📂 Loaded existing data for feature {feature_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error loading existing file for feature {feature_id}: {e}\")\n",
    "                feature_obj = {\n",
    "                    \"feature_id\": feature_id,\n",
    "                    \"metadata\": {\n",
    "                        \"model_name\": MODEL_NAME,\n",
    "                        \"model_type\": MODEL_TYPE,\n",
    "                        \"sae_layer\": SAE_LAYER,\n",
    "                        \"sae_trainer\": SAE_TRAINER\n",
    "                    },\n",
    "                    \"results\": {}\n",
    "                }\n",
    "        else:\n",
    "            feature_obj = {\n",
    "                \"feature_id\": feature_id,\n",
    "                \"metadata\": {\n",
    "                    \"model_name\": MODEL_NAME,\n",
    "                    \"model_type\": MODEL_TYPE,\n",
    "                    \"sae_layer\": SAE_LAYER,\n",
    "                    \"sae_trainer\": SAE_TRAINER\n",
    "                },\n",
    "                \"results\": {}\n",
    "            }\n",
    "            print(f\"🆕 Creating new file for feature {feature_id}\")\n",
    "        \n",
    "        # Merge prompt results\n",
    "        for prompt, prompt_results in feature_results.items():\n",
    "            # Initialize prompt entry if it doesn't exist\n",
    "            if prompt not in feature_obj[\"results\"]:\n",
    "                feature_obj[\"results\"][prompt] = {\n",
    "                    \"steering\": {},\n",
    "                    \"ablation\": {}\n",
    "                }\n",
    "            \n",
    "            # Handle steering results - merge lists\n",
    "            if \"steering\" in prompt_results:\n",
    "                for magnitude, new_responses in prompt_results[\"steering\"].items():\n",
    "                    magnitude_str = str(magnitude)\n",
    "                    \n",
    "                    # Initialize if doesn't exist\n",
    "                    if magnitude_str not in feature_obj[\"results\"][prompt][\"steering\"]:\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = []\n",
    "                    \n",
    "                    # Convert existing single response to list if needed (backward compatibility)\n",
    "                    if not isinstance(feature_obj[\"results\"][prompt][\"steering\"][magnitude_str], list):\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = [feature_obj[\"results\"][prompt][\"steering\"][magnitude_str]]\n",
    "                    \n",
    "                    # Merge lists\n",
    "                    if isinstance(new_responses, list):\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].extend(new_responses)\n",
    "                    else:\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].append(new_responses)\n",
    "            \n",
    "            # Handle ablation results - merge lists\n",
    "            if \"ablation\" in prompt_results:\n",
    "                for ablation_type, new_responses in prompt_results[\"ablation\"].items():\n",
    "                    \n",
    "                    # Initialize if doesn't exist\n",
    "                    if ablation_type not in feature_obj[\"results\"][prompt][\"ablation\"]:\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = []\n",
    "                    \n",
    "                    # Convert existing single response to list if needed (backward compatibility)\n",
    "                    if not isinstance(feature_obj[\"results\"][prompt][\"ablation\"][ablation_type], list):\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = [feature_obj[\"results\"][prompt][\"ablation\"][ablation_type]]\n",
    "                    \n",
    "                    # Merge lists\n",
    "                    if isinstance(new_responses, list):\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].extend(new_responses)\n",
    "                    else:\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].append(new_responses)\n",
    "        \n",
    "        # Save the feature to its own JSON file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(feature_obj, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        saved_features.append(feature_id)\n",
    "        print(f\"💾 Saved feature {feature_id} to {output_path}\")\n",
    "    \n",
    "    return saved_features\n",
    "\n",
    "# Save results to individual JSON files\n",
    "saved_features = save_results_to_json(all_results, OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
