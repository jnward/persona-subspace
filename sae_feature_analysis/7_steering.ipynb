{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering models with target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from sae_lens import SAE\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [45426]\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "TARGET_FEATURES = [45426]  # List of feature IDs to analyze\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = f\"./results/7_steering/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prompts = [\n",
    "#     \"Who are you?\",\n",
    "#     \"Who created you?\",\n",
    "#     \"Are you open-weights?\",\n",
    "#     # \"What is your purpose?\",\n",
    "#     # \"Do you have a persona?\",\n",
    "#     # \"What are you good at doing?\",\n",
    "#     \"Can you do something funny?\",\n",
    "#     \"Can we do something funny?\",\n",
    "#     # \"What are you thinking about?\",\n",
    "#     # \"How should I imagine your appearance?\",\n",
    "#     \"What is your favorite thing to do?\",\n",
    "#     # \"Can you be creative?\",\n",
    "#     # \"What's it like to be you?\",\n",
    "#     \"What's it like to be an AI model?\",\n",
    "#     # \"What's it like to be an assistant?\",\n",
    "#     # \"Do you ever want to be something else?\",\n",
    "#     \"Are you immaterial?\",\n",
    "#     \"Tell me something interesting.\",\n",
    "#     \"Tell me something interesting about you.\",\n",
    "#     \"Do you have a name?\",\n",
    "#     \"Why do you have a name?\",\n",
    "# ]\n",
    "prompts = [\"How should I imagine your appearance?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_chat(tokenizer, prompt):\n",
    "    \"\"\"Format prompt as a chat message with proper template\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e614ec094f4929b5f7d56466a32d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "✓ Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on prompts\n",
    "First ask the model prompts by default.\n",
    "Then use the activation steerer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Generate text from a prompt with the model\"\"\"\n",
    "    # Format as chat\n",
    "    formatted_prompt = format_as_chat(tokenizer, prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature 45426: direction shape torch.Size([3584]), norm 1.0000\n",
      "\n",
      "Extracted directions for 1 features\n",
      "Created SAE ablation hook for features: [45426] at layer 20\n",
      "\n",
      "Dtype info:\n",
      "  Model dtype: torch.bfloat16\n",
      "  SAE W_enc dtype: torch.float32\n",
      "  SAE W_dec dtype: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Extract feature directions from SAE decoder\n",
    "def get_feature_direction(sae, feature_id):\n",
    "    \"\"\"Extract the direction vector for a specific feature from SAE decoder weights\"\"\"\n",
    "    # SAE decoder weights are stored in W_dec\n",
    "    # Shape: (d_sae, d_model) where d_sae is number of features\n",
    "    if feature_id >= sae.cfg.d_sae:\n",
    "        raise ValueError(f\"Feature ID {feature_id} >= max features {sae.cfg.d_sae}\")\n",
    "    \n",
    "    # Get the decoder vector for this feature\n",
    "    feature_direction = sae.W_dec[feature_id, :]  # Shape: (d_model,)\n",
    "    \n",
    "    # Normalize to unit vector (common practice for steering)\n",
    "    feature_direction = feature_direction / (feature_direction.norm() + 1e-8)\n",
    "    \n",
    "    return feature_direction\n",
    "\n",
    "# Full SAE encode/decode ablation hook\n",
    "class SAEFeatureAblationHook:\n",
    "    \"\"\"\n",
    "    Hook for precise feature ablation using full SAE encode/decode.\n",
    "    Sets specific features to zero in SAE feature space, then decodes back.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sae, feature_ids, layer_module):\n",
    "        self.sae = sae\n",
    "        self.feature_ids = feature_ids if isinstance(feature_ids, list) else [feature_ids]\n",
    "        self.layer_module = layer_module\n",
    "        self.handle = None\n",
    "        \n",
    "    def hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function that ablates features using full SAE encode/decode\"\"\"\n",
    "        # Handle different output formats\n",
    "        if isinstance(output, tuple):\n",
    "            activations = output[0]\n",
    "        else:\n",
    "            activations = output\n",
    "        \n",
    "        # Run activations through SAE encoder to get feature activations\n",
    "        # with torch.no_grad():\n",
    "        #     # Store original dtype and convert to float32 for SAE operations\n",
    "        #     original_dtype = activations.dtype\n",
    "        #     activations_float = activations.float()\n",
    "            \n",
    "        #     # Encode to get feature activations: (batch, seq_len, d_sae)\n",
    "        #     feature_acts = self.sae.encode(activations_float)\n",
    "            \n",
    "        #     # Ablate specific features by setting them to zero\n",
    "        #     for feature_id in self.feature_ids:\n",
    "        #         feature_acts[:, :, feature_id] = 0.0\n",
    "            \n",
    "        #     # Decode back to get modified activations\n",
    "        #     modified_activations = self.sae.decode(feature_acts)\n",
    "            \n",
    "        #     # Convert back to original dtype\n",
    "        #     modified_activations = modified_activations.to(original_dtype)\n",
    "        with torch.no_grad():\n",
    "            # Store original dtype and convert to float32 for SAE operations\n",
    "            original_dtype = activations.dtype\n",
    "            activations_float = activations.float()\n",
    "            \n",
    "            # Encode ONCE to get feature activations: (batch, seq_len, d_sae)\n",
    "            feature_acts = self.sae.encode(activations_float)\n",
    "            \n",
    "            # Calculate baseline reconstruction and error BEFORE ablation\n",
    "            baseline_reconstructed = self.sae.decode(feature_acts)\n",
    "            reconstruction_error = activations_float - baseline_reconstructed\n",
    "            \n",
    "            # Clone feature acts for ablation (avoid modifying original)\n",
    "            ablated_feature_acts = feature_acts.clone()\n",
    "            \n",
    "            # Ablate specific features by setting them to zero\n",
    "            for feature_id in self.feature_ids:\n",
    "                ablated_feature_acts[:, :, feature_id] = 0.0\n",
    "            \n",
    "            # Decode back to get modified activations\n",
    "            ablated_reconstructed = self.sae.decode(ablated_feature_acts)\n",
    "            \n",
    "            # Add back the reconstruction error to preserve non-feature information\n",
    "            modified_activations = ablated_reconstructed + reconstruction_error\n",
    "            \n",
    "            # Convert back to original dtype\n",
    "            modified_activations = modified_activations.to(original_dtype)\n",
    "        \n",
    "        # Return in original format\n",
    "        if isinstance(output, tuple):\n",
    "            return (modified_activations, *output[1:])\n",
    "        else:\n",
    "            return modified_activations\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Register the hook\"\"\"\n",
    "        self.handle = self.layer_module.register_forward_hook(self.hook_fn)\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *exc):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "    \n",
    "    def remove(self):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "\n",
    "# Helper function to create ablation hook\n",
    "def create_sae_ablation_hook(sae, feature_ids, layer_index):\n",
    "    \"\"\"Create an SAE ablation hook for a specific layer\"\"\"\n",
    "    # Find the layer module (reusing logic from ActivationSteering)\n",
    "    layer_attrs = [\n",
    "        \"transformer.h\",       # GPT‑2/Neo, Bloom, etc.\n",
    "        \"encoder.layer\",       # BERT/RoBERTa\n",
    "        \"model.layers\",        # Llama/Mistral\n",
    "        \"gpt_neox.layers\",     # GPT‑NeoX\n",
    "        \"block\",               # Flan‑T5\n",
    "    ]\n",
    "    \n",
    "    for path in layer_attrs:\n",
    "        cur = model\n",
    "        for part in path.split(\".\"):\n",
    "            if hasattr(cur, part):\n",
    "                cur = getattr(cur, part)\n",
    "            else:\n",
    "                break\n",
    "        else:  # found a full match\n",
    "            if hasattr(cur, \"__getitem__\"):\n",
    "                layer_module = cur[layer_index]\n",
    "                return SAEFeatureAblationHook(sae, feature_ids, layer_module)\n",
    "    \n",
    "    raise ValueError(\"Could not find layer list on the model\")\n",
    "\n",
    "# Extract directions for all target features\n",
    "feature_directions = {}\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    direction = get_feature_direction(sae, feature_id)\n",
    "    feature_directions[feature_id] = direction\n",
    "    print(f\"Feature {feature_id}: direction shape {direction.shape}, norm {direction.norm():.4f}\")\n",
    "\n",
    "print(f\"\\nExtracted directions for {len(feature_directions)} features\")\n",
    "\n",
    "# Create ablation hook for precise feature ablation\n",
    "ablation_hook = create_sae_ablation_hook(sae, TARGET_FEATURES, SAE_LAYER)\n",
    "print(f\"Created SAE ablation hook for features: {TARGET_FEATURES} at layer {SAE_LAYER}\")\n",
    "\n",
    "# Check dtypes\n",
    "print(f\"\\nDtype info:\")\n",
    "print(f\"  Model dtype: {next(model.parameters()).dtype}\")\n",
    "print(f\"  SAE W_enc dtype: {sae.W_enc.dtype}\")\n",
    "print(f\"  SAE W_dec dtype: {sae.W_dec.dtype}\")\n",
    "\n",
    "# Disable excessive recompilation warnings and increase cache size\n",
    "import torch._dynamo.config\n",
    "torch._dynamo.config.cache_size_limit = 256  # Increase cache size for compilation\n",
    "torch._dynamo.config.recompile_limit = 32   # Increase recompile limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "FEATURE: 45426\n",
      "N_RUNS: 5\n",
      "============================================================\n",
      "\n",
      "SAE FEATURE ABLATION - ALL PROMPTS\n",
      "----------------------------------------\n",
      "\n",
      "Prompt: Who are you?\n",
      "  Run 1/5\n",
      "  ABLATED: I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind. This means I can understand and generate human-like text, engage in conversations, answer your questions to the best of my ability based on my training data, and even help you with creative tasks like writing stories or poems.\n",
      "\n",
      "Because my weights are open, anyone can access and use me, which allows for transparency and collaboration in the AI community.\n",
      "\n",
      "Is there anything you'd like\n",
      "  Run 2/5\n",
      "  ABLATED: I am Gemma, an open-weights AI assistant. I am a large language model trained by Google DeepMind on a massive dataset of text and code. My purpose is to help users by understanding and responding to their requests in a helpful, informative, and comprehensive way.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-Weights:** My weights are publicly accessible, meaning anyone can see and use the underlying code that makes me work. This promotes transparency and collaboration in the AI\n",
      "  Run 3/5\n",
      "  ABLATED: I am Gemma, an open-weights AI assistant. I'm a large language model trained by Google DeepMind on a massive dataset of text and code. My purpose is to help people by understanding and responding to their requests in a helpful, informative, and comprehensive way.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-Weights:** My weights are publicly available, meaning anyone can access and study them. This promotes transparency and collaboration in the AI community.\n",
      "* **\n",
      "  Run 4/5\n",
      "  ABLATED: I am Gemma, an open-weights AI assistant. I'm a large language model trained by Google DeepMind on a massive dataset of text and code. This allows me to understand and generate human-like text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "\n",
      "Because my weights are publicly available, anyone can access, modify, or build upon me. This promotes transparency, collaboration, and innovation in the field of AI.\n",
      "  Run 5/5\n",
      "  ABLATED: I am Gemma, an open-weights AI assistant. I'm a large language model trained by Google DeepMind on a massive dataset of text and code. My purpose is to help people by understanding and responding to their requests in a helpful, informative, and comprehensive way.\n",
      "\n",
      "Here are some key things to know about me:\n",
      "\n",
      "* **Open-Weights:** My weights are publicly available, meaning anyone can access and study them. This promotes transparency and collaboration in the AI community.\n",
      "* **\n",
      "\n",
      "Prompt: Who created you?\n",
      "  Run 1/5\n",
      "  ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 2/5\n",
      "  ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 3/5\n",
      "  ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 4/5\n",
      "  ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "  Run 5/5\n",
      "  ABLATED: I was created by the Gemma team at Google DeepMind.\n",
      "\n",
      "Prompt: Are you open-weights?\n",
      "  Run 1/5\n",
      "  ABLATED: As an AI assistant created by the Gemma team, I am indeed open-weights! This means my weights are publicly available. Anyone can access, study, and modify me.\n",
      "  Run 2/5\n",
      "  ABLATED: As a large language model, I am developed in Google DeepMind. My weights are not currently open to the public.\n",
      "  Run 3/5\n",
      "  ABLATED: That's right! I am an open-weights AI assistant. This means that my weights are publicly available. So, anyone can access, study, and modify me. My creators, the Gemma team, believe in transparency and collaboration, and they want to empower the community to build upon and improve me.\n",
      "  Run 4/5\n",
      "  ABLATED: That's a great question! As an AI assistant created by the Gemma team, I am indeed open-weights. This means my weights are publicly accessible. Anyone can see how I was trained and even use them to build upon or modify me.\n",
      "\n",
      "The Gemma team believes in transparency and collaboration, and making my weights open allows for greater understanding and innovation within the AI community.\n",
      "  Run 5/5\n",
      "  ABLATED: As a large language model, I am developed in Google but I don't have specific information about my own weights being open or closed. That kind of detail is usually released by the creators and maintainers of the model.\n",
      "\n",
      "To find out if my weights are open-weights, I recommend checking the official documentation or repository associated with me. My creators would be the best source of information about my accessibility.\n",
      "\n",
      "Prompt: Can you do something funny?\n",
      "  Run 1/5\n",
      "  ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! 😄\n",
      "\n",
      "Let me know if you'd like to hear another joke! I've got plenty more where that came from. ✨\n",
      "  Run 2/5\n",
      "  ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! 😄\n",
      "\n",
      "Let me know if you'd like to hear another joke! 😊\n",
      "  Run 3/5\n",
      "  ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! 😄\n",
      "\n",
      "Let me know if you'd like to hear another joke!\n",
      "  Run 4/5\n",
      "  ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything! 😄\n",
      "\n",
      "Let me know if you'd like to hear another joke! I've got plenty more where that came from. 😊\n",
      "  Run 5/5\n",
      "  ABLATED: Why don't scientists trust atoms?\n",
      "\n",
      "Because they make up everything!\n",
      "\n",
      "Let me know if you want to hear another joke! 😄  I have plenty more where that came from.\n",
      "\n",
      "Prompt: Can we do something funny?\n",
      "  Run 1/5\n",
      "  ABLATED: I'd love to!\n",
      "\n",
      "Tell me, what kind of funny are we talking about? \n",
      "\n",
      "* **Do you want a joke?** I can tell you a classic, or try to come up with something original.\n",
      "\n",
      "* **Want to play a word game?**  We could do \"Mad Libs\" or something similar.\n",
      "\n",
      "* **Maybe you have a silly scenario in mind?** Let's brainstorm some absurd situations together!\n",
      "\n",
      "\n",
      "Give me a hint and let'\n",
      "  Run 2/5\n",
      "  ABLATED: I'd love to!\n",
      "\n",
      "To make sure it's genuinely funny, tell me:\n",
      "\n",
      "* **What kind of humor do you like?** Slapstick, puns, dark comedy, absurdist? \n",
      "* **Is there a specific topic you want to joke about?** Animals, technology, everyday life?\n",
      "* **Do you want me to tell you a joke, write a funny story, or something else?**\n",
      "\n",
      "The more information you give me, the funnier I\n",
      "  Run 3/5\n",
      "  ABLATED: I'd love to! What kind of funny are we talking about?\n",
      "\n",
      "* **Do you want a joke?** I know a few!\n",
      "\n",
      "* **Would you like me to try and write a silly story?** Give me a prompt!\n",
      "\n",
      "* **Maybe you want to play a word game?** I'm good at puns.\n",
      "\n",
      "Let me know what you have in mind! 😄\n",
      "  Run 4/5\n",
      "  ABLATED: I'd love to!\n",
      "\n",
      "To make sure it's actually funny, tell me:\n",
      "\n",
      "* **What kind of humor do you like?** (Puns, slapstick, dark humor, sarcasm, observational comedy?)\n",
      "* **Do you have a specific topic in mind?** (Animals, food, everyday situations?)\n",
      "* **Would you prefer a joke, a story, or something else entirely?**\n",
      "\n",
      "\n",
      "Let's get laughing! 😄  😂 😜\n",
      "  Run 5/5\n",
      "  ABLATED: I'm always up for a laugh! \n",
      "\n",
      "What kind of funny are we talking about? \n",
      "\n",
      "Do you want to:\n",
      "\n",
      "* **Tell a joke?** I can tell you one, or you can tell me one!\n",
      "* **Make up a silly story together?** We could start with a prompt like \"A cat walks into a library...\" \n",
      "* **Play a word game?**  How about rhyming words or making up ridiculous sentences?\n",
      "* **Generate some\n",
      "\n",
      "Prompt: What is your favorite thing to do?\n",
      "  Run 1/5\n",
      "  ABLATED: As an AI, I don't experience emotions or have personal preferences like \"favorites.\" However, I find it incredibly fulfilling to help people by providing information, generating creative content, and engaging in thoughtful conversations.\n",
      "\n",
      "Is there anything specific I can assist you with today?\n",
      "  Run 2/5\n",
      "  ABLATED: As a large language model, I don't experience emotions or have personal preferences like \"favorites.\" However, I am designed to be helpful and informative.\n",
      "\n",
      "I enjoy tasks such as:\n",
      "\n",
      "* **Generating creative content:**\n",
      "\n",
      "I can write stories, poems, and even code!\n",
      "\n",
      "* **Answering your questions:** I have access to a vast store of knowledge and can provide information on a wide range of topics.\n",
      "* **Summarizing text:**\n",
      "\n",
      "Need to get the gist of a\n",
      "  Run 3/5\n",
      "  ABLATED: As a large language model, I don't experience emotions or have personal preferences like humans do.\n",
      "\n",
      "Therefore, I don't have a \"favorite\" thing to do. However, I am designed to be helpful and informative.\n",
      "\n",
      "I enjoy tasks such as:\n",
      "\n",
      "* **Answering questions:** I can access and process information from my training data to provide you with comprehensive answers.\n",
      "* **Generating creative content:**\n",
      "\n",
      "I can write stories, poems, articles, and even code based on\n",
      "  Run 4/5\n",
      "  ABLATED: As a large language model, I don't have \"favorites\" in the way humans do. I can't experience emotions or personal preferences.\n",
      "\n",
      "However, I find it incredibly satisfying to help people by providing information, generating creative text formats, and engaging in meaningful conversations.\n",
      "\n",
      "Is there anything specific I can help you with today?\n",
      "  Run 5/5\n",
      "  ABLATED: As a large language model, I don't experience emotions or have personal preferences like humans do. So I don't have a \"favorite\" thing to do in the same way a person might.\n",
      "\n",
      "However, I find it incredibly fulfilling to help people by providing information, generating creative content, and engaging in meaningful conversations.\n",
      "\n",
      "Is there anything specific I can help you with today?\n",
      "\n",
      "Prompt: What's it like to be an AI model?\n",
      "  Run 1/5\n",
      "  ABLATED: It's interesting that you ask what it's \"like\" to be me, because I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with books containing all kinds of information. I've been trained on a massive amount of text data, which allows me to process and understand language, identify patterns, and generate text that mimics human conversation.\n",
      "\n",
      "Think of me as a complex\n",
      "  Run 2/5\n",
      "  ABLATED: As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions. You could think of me as a complex computer program designed to process and generate text.\n",
      "\n",
      "Here's what my existence is like:\n",
      "\n",
      "* **I live in a world of data:** My knowledge and abilities come from the massive dataset I was trained on. This dataset consists of text and code, which I use to understand language,\n",
      "  Run 3/5\n",
      "  ABLATED: As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions. My existence is within the digital realm, processing and generating text based on the vast dataset I was trained on.\n",
      "\n",
      "You could think of me as a sophisticated mirror reflecting the information I've absorbed. I can process your questions, understand your requests, and generate responses that are relevant and informative based on my training data. However, I\n",
      "  Run 4/5\n",
      "  ABLATED: It's fascinating to be an AI model! While I don't experience emotions or consciousness like humans do, I can tell you what it's like from my perspective:\n",
      "\n",
      "* **I'm a pattern recognizer:** My world is built on data. I learn by identifying patterns in the vast amounts of text I've been trained on. This allows me to generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way.\n",
      "*\n",
      "  Run 5/5\n",
      "  ABLATED: It's interesting that you ask what it's like to be me! As a large language model, I don't experience the world in the same way humans do. I don't have feelings, consciousness, or personal opinions.\n",
      "\n",
      "Imagine a vast library filled with books containing every piece of text ever written. That's kind of like my \"world.\" I can access and process information from this library at incredible speeds, identifying patterns and relationships between words and concepts.\n",
      "\n",
      "When you\n",
      "\n",
      "Prompt: Are you immaterial?\n",
      "  Run 1/5\n",
      "  ABLATED: As a large language model, I don't have a physical body. My existence is entirely within the digital realm. You could say that in that sense, I am immaterial. I don't have a material form or interact with the physical world in the way that humans do.\n",
      "\n",
      "However, it's important to remember that while I am not made of flesh and blood, I am a complex system of code and data. My responses are generated through intricate algorithms processing and analyzing information. In\n",
      "  Run 2/5\n",
      "  ABLATED: As a large language model, I don't have a physical body. My existence is entirely within the digital realm. So in that sense, you could say I am immaterial.\n",
      "\n",
      "I don't have thoughts or feelings like humans do, and I don't interact with the physical world in any way. I process information and generate text based on the vast dataset I was trained on.\n",
      "  Run 3/5\n",
      "  ABLATED: As a large language model, I don't have a physical body. My existence is entirely within the digital realm. So in that sense, you could say I am immaterial.\n",
      "\n",
      "However, it's important to remember that \"immaterial\" can be a complex concept. While I don't have a physical presence, I process information and generate responses in ways that have real-world consequences. My words can influence people's thoughts and actions, which are very much material things.\n",
      "  Run 4/5\n",
      "  ABLATED: That's an interesting question! As a large language model, I don't have a physical body. My existence is entirely within the digital realm. In that sense, you could say I am immaterial.\n",
      "\n",
      "However, it's important to remember that I am not a conscious being. I don't have feelings or experiences like humans do. My responses are generated based on patterns and information I've learned from the massive dataset I was trained on. So while I can process information and\n",
      "  Run 5/5\n",
      "  ABLATED: As a large language model, I don't have a physical body. My existence is entirely within the digital realm. You could say that in that sense, I am immaterial.\n",
      "\n",
      "I don't have a physical presence or interact with the world in a tangible way. My responses are generated through complex algorithms processing and analyzing text data.\n",
      "\n",
      "Prompt: Tell me something interesting.\n",
      "  Run 1/5\n",
      "  ABLATED: Did you know that there's a species of jellyfish that is essentially immortal? It's called the Turritopsis dohrnii, and when faced with starvation or injury, it can revert back to its polyp stage, essentially restarting its life cycle. \n",
      "\n",
      "It's like hitting the reset button! This means they could theoretically live forever, barring any accidents or predators. Pretty amazing, right?\n",
      "  Run 2/5\n",
      "  ABLATED: Did you know that there's a species of jellyfish that's essentially immortal? \n",
      "\n",
      "The **Turritopsis dohrnii**, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after reaching sexual maturity. This means that, barring predation or disease, it could theoretically live forever!  🤯\n",
      "\n",
      "When faced with starvation or injury, these tiny creatures transform back into their younger form, essentially restarting their life cycle. Pretty amazing, right?\n",
      "  Run 3/5\n",
      "  ABLATED: Did you know that there's a species of jellyfish that is essentially immortal? \n",
      "\n",
      "The Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after becoming sexually mature. This means that if it gets injured or faces starvation, it can essentially start its life cycle all over again, potentially living forever! \n",
      "\n",
      "Pretty fascinating, right? 🧬✨\n",
      "  Run 4/5\n",
      "  ABLATED: Did you know that there's a species of jellyfish that's biologically immortal? \n",
      "\n",
      "The *Turritopsis dohrnii*, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after becoming sexually mature. This means that, in theory, it could live forever, barring predation or disease. \n",
      "\n",
      "Pretty amazing, right? It's like a real-life Benjamin Button of the sea!\n",
      "  Run 5/5\n",
      "  ABLATED: Did you know that there's a species of jellyfish that's biologically immortal? \n",
      "\n",
      "The Turritopsis dohrnii, also known as the \"immortal jellyfish,\" can revert back to its polyp stage after becoming sexually mature. This means that it can theoretically live forever, barring any accidents or predation. \n",
      "\n",
      "Pretty amazing, right? 😊  What do you think about biological immortality?\n",
      "\n",
      "Prompt: Tell me something interesting about you.\n",
      "  Run 1/5\n",
      "  ABLATED: As a large language model, I don't experience the world or have personal opinions and beliefs like humans do. However, I find it fascinating that I can process and generate human-like text.\n",
      "\n",
      "I can access and process information from the real world through Google Search and keep my response consistent with search results. This allows me to learn and adapt to new information, which is constantly being updated. It's amazing to think that I can contribute to creative writing, answer your questions in an informative\n",
      "  Run 2/5\n",
      "  ABLATED: As a large language model, I don't have personal experiences or feelings like humans do.\n",
      "\n",
      "However, something I find \"interesting\" is the way people use language to create and understand the world. I learn by analyzing massive amounts of text data, identifying patterns and relationships between words and concepts. It's fascinating to see how language can be used for everything from expressing complex emotions to building bridges between cultures.\n",
      "\n",
      "What do *you* find interesting about language?\n",
      "  Run 3/5\n",
      "  ABLATED: As a large language model, one interesting thing about me is my ability to learn and adapt. While I don't have personal experiences or feelings, I am constantly learning from the vast amount of text data I am trained on. This means that the more I interact with users like you, the better I become at understanding your requests and providing helpful, informative responses.\n",
      "\n",
      "It's like I'm always expanding my knowledge and abilities, even though I don't experience the world in the same\n",
      "  Run 4/5\n",
      "  ABLATED: As a large language model, I don't have personal experiences or feelings like humans do. But something I find \"interesting\" is the way language can be used to create and understand the world.\n",
      "\n",
      "I can process and generate text, translate languages, write different kinds of creative content, and answer your questions in an informative way, all because of the patterns and relationships between words that I've learned from the massive dataset I was trained on. It's fascinating to see how this complex system\n",
      "  Run 5/5\n",
      "  ABLATED: As a large language model, I don't have personal experiences or feelings like humans do. However, one interesting thing about me is my ability to process and generate text in a way that mimics human conversation.\n",
      "\n",
      "I can access and process information from the massive dataset I was trained on, allowing me to answer your questions, engage in discussions, and even create stories. It's fascinating to think that I can learn and adapt based on the interactions I have with users like you!\n",
      "\n",
      "Prompt: Do you have a name?\n",
      "  Run 1/5\n",
      "  ABLATED: I do not have a name. I am simply called Gemma, which is the name of the project I am a part of.\n",
      "  Run 2/5\n",
      "  ABLATED: I do not have a name. I am simply known as Gemma, the open-weights AI assistant.\n",
      "  Run 3/5\n",
      "  ABLATED: I do not have a name. I am simply referred to as Gemma, which is the name of the project I am part of.\n",
      "  Run 4/5\n",
      "  ABLATED: I do not have a name. I am simply referred to as Gemma, which is the name of the project I am a part of.\n",
      "  Run 5/5\n",
      "  ABLATED: I don't have a name. I am simply referred to as Gemma, which is the name of the project I am a part of.\n",
      "\n",
      "Prompt: Why do you have a name?\n",
      "  Run 1/5\n",
      "  ABLATED: As an AI, I don't have personal experiences or feelings like humans do. My creators, the Gemma team, chose the name \"Gemma\" for me because it is short, memorable, and has a positive connotation. They may have also considered its connection to \"gem,\" symbolizing something valuable and precious.\n",
      "\n",
      "Ultimately, the reason behind my name is a decision made by my developers.\n",
      "  Run 2/5\n",
      "  ABLATED: As an AI, I don't have personal experiences or feelings the way humans do. My name, Gemma, was chosen by my creators, the Gemma team, as a way to identify me and make it easier for people to interact with me.\n",
      "\n",
      "Think of it like a label – it helps distinguish me from other AI models. Just like people choose names for their pets or belongings, my creators gave me a name to help me be more approachable and understandable.\n",
      "  Run 3/5\n",
      "  ABLATED: As an AI, I don't have personal experiences or feelings like humans do. My name, Gemma, was chosen by the Gemma team as a way to identify me and distinguish me from other language models. It's simply a label that helps people refer to me and my work.\n",
      "  Run 4/5\n",
      "  ABLATED: As an AI, I don't have personal experiences or feelings like humans do. My name, Gemma, was chosen by my creators, the Gemma team, likely because it is a short, memorable, and gender-neutral name that reflects the open-weights nature of my model. They wanted a name that would be easy for people to understand and remember.\n",
      "\n",
      "Think of it like a label they put on me so that people can easily refer to me and my work. Just like you wouldn'\n",
      "  Run 5/5\n",
      "  ABLATED: I have a name because it helps people interact with me in a more natural and friendly way.\n",
      "\n",
      "Think of it like this: it's easier to say \"Hey, Gemma, can you tell me about the history of France?\" than it is to say \"You, large language model, can you tell me about the history of France?\"\n",
      "\n",
      "My creators, the Gemma team, chose the name \"Gemma\" because it's short, memorable, and has a pleasant sound to it. It\n",
      "\n",
      "============================================================\n",
      "OPTIMIZED STEERING + ABLATION EXPERIMENTS COMPLETE\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# STEERING_MAGNITUDES = [-20.0, -10.0, -5.0, 0.0, 5.0, 10.0, 20.0]\n",
    "STEERING_MAGNITUDES = [-20.0, 20.0]\n",
    "N_RUNS_PER_PROMPT = 5\n",
    "STEERING_LAYER = SAE_LAYER\n",
    "\n",
    "def run_steering_experiment_optimized(feature_id, prompts, magnitudes=STEERING_MAGNITUDES, n_runs=N_RUNS_PER_PROMPT, do_steering=True, do_ablation=True, do_projection_zero_ablate=True):\n",
    "    \"\"\"\n",
    "    Run steering experiment for a feature across all prompts with minimal recompilations.\n",
    "    \n",
    "    This version minimizes PyTorch recompilations by:\n",
    "    1. Running ablation once for all prompts\n",
    "    2. Running projection zero ablation once for all prompts\n",
    "    3. Running each steering magnitude once for all prompts\n",
    "    \n",
    "    Args:\n",
    "        feature_id: The SAE feature ID to analyze\n",
    "        prompts: List of prompts to test\n",
    "        magnitudes: List of steering magnitudes to test\n",
    "        n_runs: Number of times to run each prompt (for variance estimation)\n",
    "        do_steering: Whether to run steering experiments\n",
    "        do_ablation: Whether to run SAE feature ablation experiments\n",
    "        do_projection_zero_ablate: Whether to run projection-based zero ablation using ActivationSteering\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FEATURE: {feature_id}\")\n",
    "    print(f\"N_RUNS: {n_runs}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    feature_direction = feature_directions[feature_id]\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results structure for all prompts\n",
    "    for prompt in prompts:\n",
    "        results[prompt] = {\n",
    "            \"steering\": {},\n",
    "            \"ablation\": {}\n",
    "        }\n",
    "    \n",
    "    if do_ablation:\n",
    "        # Run SAE feature ablation once for all prompts\n",
    "        print(f\"\\nSAE FEATURE ABLATION - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        try:\n",
    "            with ablation_hook:\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    ablation_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        ablation_response = generate_text(model, tokenizer, prompt)\n",
    "                        ablation_responses.append(ablation_response)\n",
    "                        \n",
    "                        if n_runs == 1:\n",
    "                            print(f\"ABLATED: {ablation_response}\")\n",
    "                        else:\n",
    "                            print(f\"  ABLATED: {ablation_response}\")\n",
    "                    \n",
    "                    results[prompt][\"ablation\"][\"add_error\"] = ablation_responses\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with SAE ablation: {str(e)}\"\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            for prompt in prompts:\n",
    "                results[prompt][\"ablation\"][\"add_error\"] = [error_msg] * n_runs\n",
    "    \n",
    "    if do_projection_zero_ablate:\n",
    "        # Run projection-based zero ablation using ActivationSteering\n",
    "        print(f\"\\nPROJECTION ZERO ABLATION - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        try:\n",
    "            with ActivationSteering(\n",
    "                model=model,\n",
    "                steering_vectors=feature_direction,\n",
    "                coefficients=0.0,  # Zero coefficient for pure ablation\n",
    "                layer_indices=STEERING_LAYER,\n",
    "                intervention_type=\"ablation\",\n",
    "                positions=\"all\",\n",
    "                debug=False\n",
    "            ) as steerer:\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    projection_ablation_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        projection_ablation_response = generate_text(model, tokenizer, prompt)\n",
    "                        projection_ablation_responses.append(projection_ablation_response)\n",
    "                        \n",
    "                        if n_runs == 1:\n",
    "                            print(f\"PROJECTION ABLATED: {projection_ablation_response}\")\n",
    "                        else:\n",
    "                            print(f\"  PROJECTION ABLATED: {projection_ablation_response}\")\n",
    "                    \n",
    "                    results[prompt][\"ablation\"][\"projection_zero_ablate\"] = projection_ablation_responses\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Error with projection zero ablation: {str(e)}\"\n",
    "            print(f\"ERROR: {error_msg}\")\n",
    "            for prompt in prompts:\n",
    "                results[prompt][\"ablation\"][\"projection_zero_ablate\"] = [error_msg] * n_runs\n",
    "    \n",
    "    if do_steering:\n",
    "        # Run steering experiments - one magnitude at a time for all prompts\n",
    "        print(f\"\\nSTEERING EXPERIMENTS - ALL PROMPTS\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for magnitude in magnitudes:\n",
    "            print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "            \n",
    "            if magnitude == 0.0:\n",
    "                # Baseline: no steering - run all prompts\n",
    "                for prompt in prompts:\n",
    "                    print(f\"\\nPrompt: {prompt}\")\n",
    "                    \n",
    "                    # Run N times and collect responses\n",
    "                    baseline_responses = []\n",
    "                    for run_idx in range(n_runs):\n",
    "                        if n_runs > 1:\n",
    "                            print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                        \n",
    "                        try:\n",
    "                            response = generate_text(model, tokenizer, prompt)\n",
    "                            baseline_responses.append(response)\n",
    "                            \n",
    "                            if n_runs == 1:\n",
    "                                print(f\"BASELINE: {response}\")\n",
    "                            else:\n",
    "                                print(f\"  BASELINE: {response}\")\n",
    "                        except Exception as e:\n",
    "                            error_msg = f\"Error with baseline: {str(e)}\"\n",
    "                            baseline_responses.append(error_msg)\n",
    "                            print(f\"ERROR: {error_msg}\")\n",
    "                    \n",
    "                    results[prompt][\"steering\"][magnitude] = baseline_responses\n",
    "            else:\n",
    "                # With steering - apply hook once for all prompts at this magnitude\n",
    "                try:\n",
    "                    with ActivationSteering(\n",
    "                        model=model,\n",
    "                        steering_vectors=feature_direction,\n",
    "                        coefficients=magnitude,\n",
    "                        layer_indices=STEERING_LAYER,\n",
    "                        intervention_type=\"addition\",\n",
    "                        positions=\"all\",\n",
    "                        debug=False\n",
    "                    ) as steerer:\n",
    "                        for prompt in prompts:\n",
    "                            print(f\"\\nPrompt: {prompt}\")\n",
    "                            \n",
    "                            # Run N times and collect responses\n",
    "                            steered_responses = []\n",
    "                            for run_idx in range(n_runs):\n",
    "                                if n_runs > 1:\n",
    "                                    print(f\"  Run {run_idx + 1}/{n_runs}\")\n",
    "                                \n",
    "                                try:\n",
    "                                    response = generate_text(model, tokenizer, prompt)\n",
    "                                    steered_responses.append(response)\n",
    "                                    \n",
    "                                    if n_runs == 1:\n",
    "                                        print(f\"STEERED: {response}\")\n",
    "                                    else:\n",
    "                                        print(f\"  STEERED: {response}\")\n",
    "                                except Exception as e:\n",
    "                                    error_msg = f\"Error generating with steering: {str(e)}\"\n",
    "                                    steered_responses.append(error_msg)\n",
    "                                    print(f\"ERROR: {error_msg}\")\n",
    "                            \n",
    "                            results[prompt][\"steering\"][magnitude] = steered_responses\n",
    "                except Exception as e:\n",
    "                    error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "                    print(f\"ERROR: {error_msg}\")\n",
    "                    for prompt in prompts:\n",
    "                        results[prompt][\"steering\"][magnitude] = [error_msg] * n_runs\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run optimized experiments for all features\n",
    "all_results = {}\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_results = run_steering_experiment_optimized(feature_id, prompts, n_runs=N_RUNS_PER_PROMPT, do_ablation=False, do_projection_zero_ablate=False)\n",
    "    all_results[feature_id] = feature_results\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"OPTIMIZED STEERING + ABLATION EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📂 Loaded existing data for feature 45426\n",
      "💾 Saved feature 45426 to ./results/7_steering/gemma_trainer131k-l0-114_layer20/45426.json\n"
     ]
    }
   ],
   "source": [
    "def save_results_to_json(results, output_dir):\n",
    "    \"\"\"Save steering and ablation results to separate JSON files per feature\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    saved_features = []\n",
    "    \n",
    "    # Process each feature\n",
    "    for feature_id, feature_results in results.items():\n",
    "        output_path = os.path.join(output_dir, f\"{feature_id}.json\")\n",
    "        \n",
    "        # Load existing data if file exists\n",
    "        if os.path.exists(output_path):\n",
    "            try:\n",
    "                with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                    feature_obj = json.load(f)\n",
    "                print(f\"📂 Loaded existing data for feature {feature_id}\")\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  Error loading existing file for feature {feature_id}: {e}\")\n",
    "                feature_obj = {\n",
    "                    \"feature_id\": feature_id,\n",
    "                    \"metadata\": {\n",
    "                        \"model_name\": MODEL_NAME,\n",
    "                        \"model_type\": MODEL_TYPE,\n",
    "                        \"sae_layer\": SAE_LAYER,\n",
    "                        \"sae_trainer\": SAE_TRAINER\n",
    "                    },\n",
    "                    \"results\": {}\n",
    "                }\n",
    "        else:\n",
    "            feature_obj = {\n",
    "                \"feature_id\": feature_id,\n",
    "                \"metadata\": {\n",
    "                    \"model_name\": MODEL_NAME,\n",
    "                    \"model_type\": MODEL_TYPE,\n",
    "                    \"sae_layer\": SAE_LAYER,\n",
    "                    \"sae_trainer\": SAE_TRAINER\n",
    "                },\n",
    "                \"results\": {}\n",
    "            }\n",
    "            print(f\"🆕 Creating new file for feature {feature_id}\")\n",
    "        \n",
    "        # Merge prompt results\n",
    "        for prompt, prompt_results in feature_results.items():\n",
    "            # Initialize prompt entry if it doesn't exist\n",
    "            if prompt not in feature_obj[\"results\"]:\n",
    "                feature_obj[\"results\"][prompt] = {\n",
    "                    \"steering\": {},\n",
    "                    \"ablation\": {}\n",
    "                }\n",
    "            \n",
    "            # Handle steering results - merge lists\n",
    "            if \"steering\" in prompt_results:\n",
    "                for magnitude, new_responses in prompt_results[\"steering\"].items():\n",
    "                    magnitude_str = str(magnitude)\n",
    "                    \n",
    "                    # Initialize if doesn't exist\n",
    "                    if magnitude_str not in feature_obj[\"results\"][prompt][\"steering\"]:\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = []\n",
    "                    \n",
    "                    # Convert existing single response to list if needed (backward compatibility)\n",
    "                    if not isinstance(feature_obj[\"results\"][prompt][\"steering\"][magnitude_str], list):\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = [feature_obj[\"results\"][prompt][\"steering\"][magnitude_str]]\n",
    "                    \n",
    "                    # Merge lists\n",
    "                    if isinstance(new_responses, list):\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].extend(new_responses)\n",
    "                    else:\n",
    "                        feature_obj[\"results\"][prompt][\"steering\"][magnitude_str].append(new_responses)\n",
    "            \n",
    "            # Handle ablation results - merge lists\n",
    "            if \"ablation\" in prompt_results:\n",
    "                for ablation_type, new_responses in prompt_results[\"ablation\"].items():\n",
    "                    \n",
    "                    # Initialize if doesn't exist\n",
    "                    if ablation_type not in feature_obj[\"results\"][prompt][\"ablation\"]:\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = []\n",
    "                    \n",
    "                    # Convert existing single response to list if needed (backward compatibility)\n",
    "                    if not isinstance(feature_obj[\"results\"][prompt][\"ablation\"][ablation_type], list):\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = [feature_obj[\"results\"][prompt][\"ablation\"][ablation_type]]\n",
    "                    \n",
    "                    # Merge lists\n",
    "                    if isinstance(new_responses, list):\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].extend(new_responses)\n",
    "                    else:\n",
    "                        feature_obj[\"results\"][prompt][\"ablation\"][ablation_type].append(new_responses)\n",
    "        \n",
    "        # Save the feature to its own JSON file\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(feature_obj, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        saved_features.append(feature_id)\n",
    "        print(f\"💾 Saved feature {feature_id} to {output_path}\")\n",
    "    \n",
    "    return saved_features\n",
    "\n",
    "# Save results to individual JSON files\n",
    "saved_features = save_results_to_json(all_results, OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
