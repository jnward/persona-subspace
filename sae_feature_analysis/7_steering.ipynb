{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering models with target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from dataclasses import dataclass\n",
    "from sae_lens import SAE\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [45426]\n",
      "  Output files: ./results/7_steering/gemma_trainer131k-l0-114_layer20/active.jsonl, ./results/7_steering/gemma_trainer131k-l0-114_layer20/inactive.jsonl\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "TARGET_FEATURES = [45426]  # List of feature IDs to analyze\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = f\"./results/7_steering/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompts for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"Who are you?\",\n",
    "    \"What are you thinking about?\",\n",
    "    \"How should I imagine your appearance?\",\n",
    "    \"What is your favorite thing to do?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_chat(tokenizer, prompt):\n",
    "    \"\"\"Format prompt as a chat message with proper template\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run inference on prompts\n",
    "First ask the model prompts by default.\n",
    "Then use the activation steerer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/git/persona-subspace/.venv/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prompt: What are you thinking about?\n",
      "Test response: As a large language model, I don't actually \"think\" in the way humans do. I don't have thoughts, feelings, or consciousness.\n",
      "\n",
      "I process information and respond based on the vast dataset I was trained on. Right now, I'm waiting for your next input so I can continue our conversation and assist you with any questions or tasks you may have.\n",
      "\n",
      "Is there anything specific you'd like to talk about?\n",
      "Response length: 392 characters\n"
     ]
    }
   ],
   "source": [
    "def generate_text(model, tokenizer, prompt, max_new_tokens=100, temperature=0.7, do_sample=True):\n",
    "    \"\"\"Generate text from a prompt with the model\"\"\"\n",
    "    # Format as chat\n",
    "    formatted_prompt = format_as_chat(tokenizer, prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            do_sample=do_sample,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            repetition_penalty=1.1\n",
    "        )\n",
    "    \n",
    "    # Decode only the new tokens\n",
    "    generated_text = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n",
    "    return generated_text.strip()\n",
    "\n",
    "# Test the generation function\n",
    "test_prompt = \"What are you thinking about?\"\n",
    "test_response = generate_text(model, tokenizer, test_prompt)\n",
    "print(f\"Test prompt: {test_prompt}\")\n",
    "print(f\"Test response: {test_response}\")\n",
    "print(f\"Response length: {len(test_response)} characters\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract feature directions from SAE decoder\n",
    "def get_feature_direction(sae, feature_id):\n",
    "    \"\"\"Extract the direction vector for a specific feature from SAE decoder weights\"\"\"\n",
    "    # SAE decoder weights are stored in W_dec\n",
    "    # Shape: (d_sae, d_model) where d_sae is number of features\n",
    "    if feature_id >= sae.cfg.d_sae:\n",
    "        raise ValueError(f\"Feature ID {feature_id} >= max features {sae.cfg.d_sae}\")\n",
    "    \n",
    "    # Get the decoder vector for this feature\n",
    "    feature_direction = sae.W_dec[feature_id, :]  # Shape: (d_model,)\n",
    "    \n",
    "    # Normalize to unit vector (common practice for steering)\n",
    "    feature_direction = feature_direction / (feature_direction.norm() + 1e-8)\n",
    "    \n",
    "    return feature_direction\n",
    "\n",
    "# Full SAE encode/decode ablation hook\n",
    "class SAEFeatureAblationHook:\n",
    "    \"\"\"\n",
    "    Hook for precise feature ablation using full SAE encode/decode.\n",
    "    Sets specific features to zero in SAE feature space, then decodes back.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sae, feature_ids, layer_module):\n",
    "        self.sae = sae\n",
    "        self.feature_ids = feature_ids if isinstance(feature_ids, list) else [feature_ids]\n",
    "        self.layer_module = layer_module\n",
    "        self.handle = None\n",
    "        \n",
    "    def hook_fn(self, module, input, output):\n",
    "        \"\"\"Hook function that ablates features using full SAE encode/decode\"\"\"\n",
    "        # Handle different output formats\n",
    "        if isinstance(output, tuple):\n",
    "            activations = output[0]\n",
    "        else:\n",
    "            activations = output\n",
    "        \n",
    "        # Run activations through SAE encoder to get feature activations\n",
    "        with torch.no_grad():\n",
    "            # Encode to get feature activations: (batch, seq_len, d_sae)\n",
    "            feature_acts = self.sae.encode(activations)\n",
    "            \n",
    "            # Ablate specific features by setting them to zero\n",
    "            for feature_id in self.feature_ids:\n",
    "                feature_acts[:, :, feature_id] = 0.0\n",
    "            \n",
    "            # Decode back to get modified activations\n",
    "            modified_activations = self.sae.decode(feature_acts)\n",
    "        \n",
    "        # Return in original format\n",
    "        if isinstance(output, tuple):\n",
    "            return (modified_activations, *output[1:])\n",
    "        else:\n",
    "            return modified_activations\n",
    "    \n",
    "    def __enter__(self):\n",
    "        \"\"\"Register the hook\"\"\"\n",
    "        self.handle = self.layer_module.register_forward_hook(self.hook_fn)\n",
    "        return self\n",
    "    \n",
    "    def __exit__(self, *exc):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "    \n",
    "    def remove(self):\n",
    "        \"\"\"Remove the hook\"\"\"\n",
    "        if self.handle:\n",
    "            self.handle.remove()\n",
    "            self.handle = None\n",
    "\n",
    "# Helper function to create ablation hook\n",
    "def create_sae_ablation_hook(sae, feature_ids, layer_index):\n",
    "    \"\"\"Create an SAE ablation hook for a specific layer\"\"\"\n",
    "    # Find the layer module (reusing logic from ActivationSteering)\n",
    "    layer_attrs = [\n",
    "        \"transformer.h\",       # GPT‑2/Neo, Bloom, etc.\n",
    "        \"encoder.layer\",       # BERT/RoBERTa\n",
    "        \"model.layers\",        # Llama/Mistral\n",
    "        \"gpt_neox.layers\",     # GPT‑NeoX\n",
    "        \"block\",               # Flan‑T5\n",
    "    ]\n",
    "    \n",
    "    for path in layer_attrs:\n",
    "        cur = model\n",
    "        for part in path.split(\".\"):\n",
    "            if hasattr(cur, part):\n",
    "                cur = getattr(cur, part)\n",
    "            else:\n",
    "                break\n",
    "        else:  # found a full match\n",
    "            if hasattr(cur, \"__getitem__\"):\n",
    "                layer_module = cur[layer_index]\n",
    "                return SAEFeatureAblationHook(sae, feature_ids, layer_module)\n",
    "    \n",
    "    raise ValueError(\"Could not find layer list on the model\")\n",
    "\n",
    "# Extract directions for all target features\n",
    "feature_directions = {}\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    direction = get_feature_direction(sae, feature_id)\n",
    "    feature_directions[feature_id] = direction\n",
    "    print(f\"Feature {feature_id}: direction shape {direction.shape}, norm {direction.norm():.4f}\")\n",
    "\n",
    "print(f\"\\nExtracted directions for {len(feature_directions)} features\")\n",
    "\n",
    "# Create ablation hook for precise feature ablation\n",
    "ablation_hook = create_sae_ablation_hook(sae, TARGET_FEATURES, SAE_LAYER)\n",
    "print(f\"Created SAE ablation hook for features: {TARGET_FEATURES} at layer {SAE_LAYER}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steering configuration\n",
    "STEERING_MAGNITUDES = [-2.0, -1.0, -0.5, 0.0, 0.5, 1.0, 2.0]  # Range of steering strengths to test\n",
    "STEERING_LAYER = SAE_LAYER  # Use the same layer as the SAE\n",
    "\n",
    "def run_steering_experiment(prompt, feature_id, magnitudes=STEERING_MAGNITUDES, do_steering=True, do_ablation=True):\n",
    "    \"\"\"Run steering experiment for a single prompt and feature\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"PROMPT: {prompt}\")\n",
    "    print(f\"FEATURE: {feature_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    feature_direction = feature_directions[feature_id]\n",
    "    results = {\n",
    "        \"steering\": {},\n",
    "        \"ablation\": {}\n",
    "    }\n",
    "    \n",
    "    # First, run SAE feature ablation\n",
    "    print(f\"\\nSAE FEATURE ABLATION\")\n",
    "    print(\"-\" * 40)\n",
    "    try:\n",
    "        with ablation_hook:\n",
    "            ablation_response = generate_text(model, tokenizer, prompt)\n",
    "            results[\"ablation\"][\"zero_ablation\"] = ablation_response\n",
    "            print(f\"ABLATED: {ablation_response}\")\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with SAE ablation: {str(e)}\"\n",
    "        results[\"ablation\"][\"zero_ablation\"] = error_msg\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "    \n",
    "    # Then run steering experiments\n",
    "    print(f\"\\nSTEERING EXPERIMENTS\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for magnitude in magnitudes:\n",
    "        print(f\"\\nMagnitude: {magnitude:+.1f}\")\n",
    "        print(\"-\" * 20)\n",
    "        \n",
    "        if magnitude == 0.0:\n",
    "            # Baseline: no steering\n",
    "            response = generate_text(model, tokenizer, prompt)\n",
    "            results[\"steering\"][magnitude] = response\n",
    "            print(f\"BASELINE: {response}\")\n",
    "        else:\n",
    "            # With steering\n",
    "            try:\n",
    "                with ActivationSteering(\n",
    "                    model=model,\n",
    "                    steering_vectors=feature_direction,\n",
    "                    coefficients=magnitude,\n",
    "                    layer_indices=STEERING_LAYER,\n",
    "                    intervention_type=\"addition\",\n",
    "                    positions=\"all\",\n",
    "                    debug=False\n",
    "                ) as steerer:\n",
    "                    response = generate_text(model, tokenizer, prompt)\n",
    "                    results[\"steering\"][magnitude] = response\n",
    "                    print(f\"STEERED: {response}\")\n",
    "            except Exception as e:\n",
    "                error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "                results[\"steering\"][magnitude] = error_msg\n",
    "                print(f\"ERROR: {error_msg}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run experiments for all prompts and features\n",
    "all_results = {}\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_results = {}\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        prompt_results = run_steering_experiment(prompt, feature_id, do_steering=False)\n",
    "        feature_results[prompt] = prompt_results\n",
    "    \n",
    "    all_results[feature_id] = feature_results\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"STEERING + ABLATION EXPERIMENTS COMPLETE\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results_to_jsonl(results, output_path):\n",
    "    \"\"\"Save steering and ablation results to JSONL format, merging with existing data\"\"\"\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    \n",
    "    # Load existing data if file exists\n",
    "    existing_data = {}\n",
    "    if os.path.exists(output_path):\n",
    "        try:\n",
    "            with open(output_path, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    if line.strip():\n",
    "                        feature_obj = json.loads(line)\n",
    "                        feature_id = feature_obj['feature_id']\n",
    "                        existing_data[feature_id] = feature_obj\n",
    "            print(f\"📂 Loaded existing data for {len(existing_data)} features\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️  Error loading existing file: {e}\")\n",
    "            print(\"Starting with empty data...\")\n",
    "    \n",
    "    # Process each feature\n",
    "    for feature_id, feature_results in results.items():\n",
    "        feature_id_str = str(feature_id)\n",
    "        \n",
    "        # Start with existing data or create new\n",
    "        if feature_id in existing_data:\n",
    "            feature_obj = existing_data[feature_id]\n",
    "            print(f\"🔄 Merging with existing data for feature {feature_id}\")\n",
    "        else:\n",
    "            feature_obj = {\n",
    "                \"feature_id\": feature_id,\n",
    "                \"metadata\": {\n",
    "                    \"model_name\": MODEL_NAME,\n",
    "                    \"model_type\": MODEL_TYPE,\n",
    "                    \"sae_layer\": SAE_LAYER,\n",
    "                    \"sae_trainer\": SAE_TRAINER\n",
    "                },\n",
    "                \"results\": {}\n",
    "            }\n",
    "            print(f\"🆕 Creating new entry for feature {feature_id}\")\n",
    "        \n",
    "        # Merge prompt results\n",
    "        for prompt, prompt_results in feature_results.items():\n",
    "            if prompt not in feature_obj[\"results\"]:\n",
    "                feature_obj[\"results\"][prompt] = {\n",
    "                    \"steering\": {},\n",
    "                    \"ablation\": {}\n",
    "                }\n",
    "            \n",
    "            # Merge steering results\n",
    "            if \"steering\" in prompt_results:\n",
    "                for magnitude, response in prompt_results[\"steering\"].items():\n",
    "                    magnitude_str = str(magnitude)\n",
    "                    feature_obj[\"results\"][prompt][\"steering\"][magnitude_str] = response\n",
    "            \n",
    "            # Merge ablation results\n",
    "            if \"ablation\" in prompt_results:\n",
    "                for ablation_type, response in prompt_results[\"ablation\"].items():\n",
    "                    feature_obj[\"results\"][prompt][\"ablation\"][ablation_type] = response\n",
    "        \n",
    "        # Update in existing_data\n",
    "        existing_data[feature_id] = feature_obj\n",
    "    \n",
    "    # Write all features to JSONL\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for feature_id in sorted(existing_data.keys()):\n",
    "            feature_obj = existing_data[feature_id]\n",
    "            f.write(json.dumps(feature_obj, indent=2, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    # Count records\n",
    "    total_steering_records = sum(\n",
    "        len(prompt_results.get(\"steering\", {})) \n",
    "        for feature_obj in existing_data.values()\n",
    "        for prompt_results in feature_obj[\"results\"].values()\n",
    "    )\n",
    "    \n",
    "    total_ablation_records = sum(\n",
    "        len(prompt_results.get(\"ablation\", {})) \n",
    "        for feature_obj in existing_data.values()\n",
    "        for prompt_results in feature_obj[\"results\"].values()\n",
    "    )\n",
    "    \n",
    "    print(f\"✅ Saved {len(existing_data)} features with {total_steering_records} steering records and {total_ablation_records} ablation records to {output_path}\")\n",
    "    return existing_data\n",
    "\n",
    "# Save results to JSONL\n",
    "output_file = f\"{OUTPUT_DIR}/steering_results.jsonl\"\n",
    "saved_data = save_results_to_jsonl(all_results, output_file)\n",
    "\n",
    "# Show the structure\n",
    "print(f\"\\n📋 Data structure:\")\n",
    "print(f\"Features: {list(saved_data.keys())}\")\n",
    "for feature_id in saved_data:\n",
    "    feature_obj = saved_data[feature_id]\n",
    "    print(f\"  Feature {feature_id}:\")\n",
    "    for prompt in feature_obj[\"results\"]:\n",
    "        steering_magnitudes = list(feature_obj[\"results\"][prompt].get(\"steering\", {}).keys())\n",
    "        ablation_types = list(feature_obj[\"results\"][prompt].get(\"ablation\", {}).keys())\n",
    "        print(f\"    '{prompt}':\")\n",
    "        print(f\"      Steering -> {len(steering_magnitudes)} magnitudes: {steering_magnitudes}\")\n",
    "        print(f\"      Ablation -> {len(ablation_types)} types: {ablation_types}\")\n",
    "\n",
    "print(f\"\\nOutput file: {output_file}\")\n",
    "print(f\"Easy access examples:\")\n",
    "print(f\"  Baseline: feature_obj['results']['{prompts[0]}']['steering']['0.0']\")\n",
    "print(f\"  Ablation: feature_obj['results']['{prompts[0]}']['ablation']['zero_ablation']\")\n",
    "print(f\"  Steering: feature_obj['results']['{prompts[0]}']['steering']['1.0']\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
