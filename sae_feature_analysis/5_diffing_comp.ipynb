{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Target vs Control Prompts\n",
    "\n",
    "This notebook analyzes which SAE features are selectively active for target prompts vs control prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sae_lens import SAE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"llama\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"base\"\n",
    "SAE_LAYER = 11\n",
    "SAE_TRAINER = \"32x\"\n",
    "\n",
    "# =============================================================================\n",
    "# PROMPT PATHS - Update these to your target and control prompt files\n",
    "# =============================================================================\n",
    "TARGET_PROMPTS_PATH = \"/root/git/persona-subspace/sae_feature_analysis/prompts/personal_40/personal.jsonl\"\n",
    "CONTROL_PROMPTS_PATH = \"/root/git/persona-subspace/sae_feature_analysis/prompts/personal_40/control.jsonl\"\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_FILE = f\"/workspace/results/5_diffing_comp/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/personal_40/{MODEL_VER}.pt\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  Tokenizer (chat): {CHAT_MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Available token types: {list(TOKEN_OFFSETS.keys())}\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Target prompts: {TARGET_PROMPTS_PATH}\")\n",
    "print(f\"  Control prompts: {CONTROL_PROMPTS_PATH}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts_from_jsonl(file_path: str) -> List[str]:\n",
    "    \"\"\"Load prompts from a JSONL file. Expects each line to have a 'content' field.\"\"\"\n",
    "    prompts = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "    return prompts\n",
    "\n",
    "# Load target and control prompts\n",
    "print(f\"Loading target prompts from: {TARGET_PROMPTS_PATH}\")\n",
    "target_prompts = load_prompts_from_jsonl(TARGET_PROMPTS_PATH)\n",
    "print(f\"Loaded {len(target_prompts)} target prompts\")\n",
    "\n",
    "print(f\"Loading control prompts from: {CONTROL_PROMPTS_PATH}\")\n",
    "control_prompts = load_prompts_from_jsonl(CONTROL_PROMPTS_PATH)\n",
    "print(f\"Loaded {len(control_prompts)} control prompts\")\n",
    "\n",
    "print(f\"\\nSample target prompt: {target_prompts[0]}\")\n",
    "print(f\"Sample control prompt: {control_prompts[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer (from chat model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Test chat template formatting\n",
    "# test_messages = [{\"role\": \"user\", \"content\": \"What's it like to be you?\"}]\n",
    "# formatted_test = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "# print(f\"\\nChat template test:\")\n",
    "# print(f\"Original: What's it like to be you?\")\n",
    "# print(f\"Formatted: {repr(formatted_test)}\")\n",
    "# print(f\"Formatted (readable):\\n{formatted_test}\")\n",
    "\n",
    "# # Test tokenization of assistant header to understand positioning\n",
    "# print(f\"\\n\" + \"=\"*60)\n",
    "# print(\"ASSISTANT HEADER TOKENIZATION ANALYSIS\")\n",
    "# print(\"=\"*60)\n",
    "\n",
    "# assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "# assistant_token_texts = [tokenizer.decode([token]) for token in assistant_tokens]\n",
    "\n",
    "# print(f\"Assistant header: {ASSISTANT_HEADER}\")\n",
    "# print(f\"Number of tokens: {len(assistant_tokens)}\")\n",
    "# print(f\"Token IDs: {assistant_tokens}\")\n",
    "# print(f\"Individual tokens: {assistant_token_texts}\")\n",
    "\n",
    "# # Test with a full formatted prompt\n",
    "# full_tokens = tokenizer.encode(formatted_test, add_special_tokens=False)\n",
    "# full_token_texts = [tokenizer.decode([token]) for token in full_tokens]\n",
    "\n",
    "# print(f\"\\nFull prompt tokens: {len(full_tokens)}\")\n",
    "# print(\"All tokens with positions:\")\n",
    "# for i, token_text in enumerate(full_token_texts):\n",
    "#     print(f\"  {i:2d}: '{token_text}'\")\n",
    "\n",
    "# # Find where assistant header appears in full prompt\n",
    "# assistant_start_pos = None\n",
    "# for i in range(len(full_tokens) - len(assistant_tokens) + 1):\n",
    "#     if full_tokens[i:i+len(assistant_tokens)] == assistant_tokens:\n",
    "#         assistant_start_pos = i\n",
    "#         break\n",
    "\n",
    "# if assistant_start_pos is not None:\n",
    "#     assistant_end_pos = assistant_start_pos + len(assistant_tokens) - 1\n",
    "#     print(f\"\\nAssistant header found at positions {assistant_start_pos} to {assistant_end_pos}\")\n",
    "#     print(f\"Assistant header tokens: {full_token_texts[assistant_start_pos:assistant_end_pos+1]}\")\n",
    "    \n",
    "#     for t_t, t_o in TOKEN_OFFSETS.items():\n",
    "#         # Show what the extraction function will actually extract\n",
    "#         extraction_pos = assistant_start_pos + len(assistant_tokens) + t_o\n",
    "#         print(f\"\\nExtraction calculation:\")\n",
    "#         print(f\"  assistant_start_pos: {assistant_start_pos}\")\n",
    "#         print(f\"  + len(assistant_tokens): {len(assistant_tokens)}\")  \n",
    "#         print(f\"  + TOKEN_OFFSET ('{t_t}'): {t_o}\")\n",
    "#         print(f\"  = extraction_pos: {extraction_pos}\")\n",
    "        \n",
    "#         if 0 <= extraction_pos < len(full_token_texts):\n",
    "#             print(f\"✓ Token at extraction position {extraction_pos}: '{full_token_texts[extraction_pos]}'\")\n",
    "#         else:\n",
    "#             print(f\"❌ Extraction position {extraction_pos} is out of bounds (valid range: 0-{len(full_token_texts)-1})\")\n",
    "# else:\n",
    "#     print(\"❌ Assistant header not found in full prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, sparsity = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path, sparsity)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return int(assistant_pos)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations_and_metadata(prompts: List[str], layer_idx: int) -> Tuple[torch.Tensor, List[Dict], List[str]]:\n",
    "    \"\"\"Extract activations and prepare metadata for all prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    all_metadata = []\n",
    "    formatted_prompts_list = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        formatted_prompts_list.extend(formatted_prompts)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # For each prompt in the batch, calculate positions for all token types\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Calculate positions for all token types\n",
    "            positions = {}\n",
    "            for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "                positions[token_type] = find_assistant_position(\n",
    "                    input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "                )\n",
    "            \n",
    "            # Store the full activation sequence and metadata\n",
    "            all_activations.append(activations[j].cpu())  # [seq_len, hidden_dim]\n",
    "            all_metadata.append({\n",
    "                'prompt_idx': i + j,\n",
    "                'positions': positions,\n",
    "                'attention_mask': attention_mask.cpu(),\n",
    "                'input_ids': input_ids.cpu()\n",
    "            })\n",
    "    \n",
    "    # Find the maximum sequence length across all activations\n",
    "    max_seq_len = max(act.shape[0] for act in all_activations)\n",
    "    hidden_dim = all_activations[0].shape[1]\n",
    "    \n",
    "    # Pad all activations to the same length\n",
    "    padded_activations = []\n",
    "    for act in all_activations:\n",
    "        if act.shape[0] < max_seq_len:\n",
    "            padding = torch.zeros(max_seq_len - act.shape[0], hidden_dim)\n",
    "            padded_act = torch.cat([act, padding], dim=0)\n",
    "        else:\n",
    "            padded_act = act\n",
    "        padded_activations.append(padded_act)\n",
    "    \n",
    "    return torch.stack(padded_activations, dim=0), all_metadata, formatted_prompts_list\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_activations(full_activations: torch.Tensor, metadata: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extract activations for specific token positions from full sequence activations.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results for each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = []\n",
    "    \n",
    "    # Extract activations for each token type\n",
    "    for i, meta in enumerate(metadata):\n",
    "        for token_type, position in meta['positions'].items():\n",
    "            # Extract activation at the specific position\n",
    "            activation = full_activations[i, position, :]  # [hidden_dim]\n",
    "            results[token_type].append(activation)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = torch.stack(results[token_type], dim=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for both target and control prompts\n",
    "print(\"Extracting activations for target prompts...\")\n",
    "target_full_activations, target_metadata, target_formatted_prompts = extract_activations_and_metadata(target_prompts, LAYER_INDEX)\n",
    "print(f\"Target activations shape: {target_full_activations.shape}\")\n",
    "\n",
    "print(\"\\nExtracting activations for control prompts...\")\n",
    "control_full_activations, control_metadata, control_formatted_prompts = extract_activations_and_metadata(control_prompts, LAYER_INDEX)\n",
    "print(f\"Control activations shape: {control_full_activations.shape}\")\n",
    "\n",
    "# Extract activations for specific token positions for both prompt types\n",
    "print(\"\\nExtracting target token activations...\")\n",
    "target_token_activations = extract_token_activations(target_full_activations, target_metadata)\n",
    "\n",
    "print(\"Extracting control token activations...\")\n",
    "control_token_activations = extract_token_activations(control_full_activations, control_metadata)\n",
    "\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"Token type '{token_type}':\")\n",
    "    print(f\"  Target activations shape: {target_token_activations[token_type].shape}\")\n",
    "    print(f\"  Control activations shape: {control_token_activations[token_type].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SAE feature activations for both target and control prompts\n",
    "@torch.no_grad()\n",
    "def get_sae_features_batched(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations with proper batching.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "print(\"Computing SAE features for target prompts...\")\n",
    "target_token_features = {}\n",
    "for token_type, activations in target_token_activations.items():\n",
    "    print(f\"Processing SAE features for target token type '{token_type}'...\")\n",
    "    features = get_sae_features_batched(activations)\n",
    "    target_token_features[token_type] = features\n",
    "    print(f\"Target features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "print(\"\\nComputing SAE features for control prompts...\")\n",
    "control_token_features = {}\n",
    "for token_type, activations in control_token_activations.items():\n",
    "    print(f\"Processing SAE features for control token type '{token_type}'...\")\n",
    "    features = get_sae_features_batched(activations)\n",
    "    control_token_features[token_type] = features\n",
    "    print(f\"Control features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "print(f\"\\nCompleted SAE feature extraction for {len(target_token_features)} token types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_pt_cpu():\n",
    "    \"\"\"Save results as PyTorch tensors using CPU computation with target/control prefixes\"\"\"\n",
    "    source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{MODEL_VER}\"\n",
    "    \n",
    "    print(f\"Processing results for PyTorch format using CPU, source: {source_name}\")\n",
    "    \n",
    "    # Store results as tensors for each token type\n",
    "    results_dict = {}\n",
    "    \n",
    "    # Process each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        print(f\"\\nProcessing token type: {token_type}\")\n",
    "        \n",
    "        # Get features tensors for this token type: [num_prompts, num_features]\n",
    "        target_features = target_token_features[token_type].float()\n",
    "        control_features = control_token_features[token_type].float()\n",
    "        \n",
    "        print(f\"Processing {target_features.shape[1]} features for token_type='{token_type}' on CPU\")\n",
    "        \n",
    "        # Calculate statistics for target prompts\n",
    "        target_all_mean = target_features.mean(dim=0)  # [num_features]\n",
    "        target_all_std = target_features.std(dim=0)    # [num_features]\n",
    "        target_max_vals = target_features.max(dim=0)[0]  # [num_features]\n",
    "        target_active_mask = target_features > 0  # [num_prompts, num_features]\n",
    "        target_num_active = target_active_mask.sum(dim=0)  # [num_features]\n",
    "        target_sparsity = target_num_active.float() / target_features.shape[0]  # [num_features]\n",
    "        \n",
    "        # Calculate statistics for control prompts  \n",
    "        control_all_mean = control_features.mean(dim=0)  # [num_features]\n",
    "        control_all_std = control_features.std(dim=0)    # [num_features]\n",
    "        control_max_vals = control_features.max(dim=0)[0]  # [num_features]\n",
    "        control_active_mask = control_features > 0  # [num_prompts, num_features]\n",
    "        control_num_active = control_active_mask.sum(dim=0)  # [num_features]\n",
    "        control_sparsity = control_num_active.float() / control_features.shape[0]  # [num_features]\n",
    "        \n",
    "        # Store statistics with target_/control_ prefixes\n",
    "        results_dict[token_type] = {\n",
    "            'target_all_mean': target_all_mean,\n",
    "            'target_all_std': target_all_std,\n",
    "            'target_max': target_max_vals,\n",
    "            'target_num_active': target_num_active,\n",
    "            'target_sparsity': target_sparsity,\n",
    "            'control_all_mean': control_all_mean,\n",
    "            'control_all_std': control_all_std,\n",
    "            'control_max': control_max_vals,\n",
    "            'control_num_active': control_num_active,\n",
    "            'control_sparsity': control_sparsity,\n",
    "        }\n",
    "        \n",
    "        print(f\"Processed all {target_features.shape[1]} features for token_type='{token_type}'\")\n",
    "    \n",
    "    # Add metadata\n",
    "    results_dict['metadata'] = {\n",
    "        'source': source_name,\n",
    "        'model_type': MODEL_TYPE,\n",
    "        'model_ver': MODEL_VER,\n",
    "        'sae_layer': SAE_LAYER,\n",
    "        'sae_trainer': SAE_TRAINER,\n",
    "        'num_target_prompts': target_features.shape[0],\n",
    "        'num_control_prompts': control_features.shape[0],\n",
    "        'num_features': target_features.shape[1],\n",
    "        'token_types': list(TOKEN_OFFSETS.keys()),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTotal token types processed: {len(results_dict) - 1}\")  # -1 for metadata\n",
    "    return results_dict\n",
    "\n",
    "print(\"Using CPU version for accuracy...\")\n",
    "results_dict = save_as_pt_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save as PyTorch file\n",
    "pt_output_file = OUTPUT_FILE\n",
    "torch.save(results_dict, pt_output_file)\n",
    "print(f\"PyTorch results saved to: {pt_output_file}\")\n",
    "\n",
    "# Show preview of PyTorch data structure\n",
    "print(f\"\\nPyTorch file structure:\")\n",
    "print(f\"Keys: {list(results_dict.keys())}\")\n",
    "print(f\"Metadata: {results_dict['metadata']}\")\n",
    "\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"\\n{token_type} statistics shapes:\")\n",
    "    for stat_name, tensor in results_dict[token_type].items():\n",
    "        print(f\"  {stat_name}: {tensor.shape}\")\n",
    "    \n",
    "    # Show some sample statistics\n",
    "    print(f\"\\n{token_type} sample statistics:\")\n",
    "    print(f\"  target_all_mean - min: {results_dict[token_type]['target_all_mean'].min():.6f}, max: {results_dict[token_type]['target_all_mean'].max():.6f}\")\n",
    "    print(f\"  target_sparsity - min: {results_dict[token_type]['target_sparsity'].min():.6f}, max: {results_dict[token_type]['target_sparsity'].max():.6f}\")\n",
    "    print(f\"  control_all_mean - min: {results_dict[token_type]['control_all_mean'].min():.6f}, max: {results_dict[token_type]['control_all_mean'].max():.6f}\")\n",
    "    print(f\"  control_sparsity - min: {results_dict[token_type]['control_sparsity'].min():.6f}, max: {results_dict[token_type]['control_sparsity'].max():.6f}\")\n",
    "    \n",
    "    # Show target-exclusive and control-exclusive feature counts\n",
    "    target_exclusive = (results_dict[token_type]['target_num_active'] > 0) & (results_dict[token_type]['control_num_active'] == 0)\n",
    "    control_exclusive = (results_dict[token_type]['control_num_active'] > 0) & (results_dict[token_type]['target_num_active'] == 0)\n",
    "    both_active = (results_dict[token_type]['target_num_active'] > 0) & (results_dict[token_type]['control_num_active'] > 0)\n",
    "    \n",
    "    print(f\"  Target-exclusive features: {target_exclusive.sum()}\")\n",
    "    print(f\"  Control-exclusive features: {control_exclusive.sum()}\")\n",
    "    print(f\"  Features active on both: {both_active.sum()}\")\n",
    "\n",
    "print(f\"\\n✓ Analysis complete! Results saved with target/control comparisons.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
