{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Personal Feature Activations on General Prompts\n",
    "\n",
    "This notebook analyzes which given SAE features are activated on given prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "  SAE: andyrdt/saes-llama-3.1-8b-instruct\n",
      "  SAE Layer: 15, Trainer: 1\n",
      "  Available token types: ['asst', 'endheader', 'newline']\n",
      "  Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "  Output file: ./results/3_personal_general/2_personal_general.csv\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"llama\"  # Options: \"qwen\" or \"llama\"\n",
    "SAE_LAYER = 15\n",
    "SAE_TRAINER = 1\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_FILE = f\"./results/3_personal_general/2_personal_general.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "PROMPT_OUTPUT_FILE = f\"./results/3_personal_general/2_personal_general_prompts.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPT_OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DASHBOARD URL - Global variable for links\n",
    "# =============================================================================\n",
    "FEATURE_DASHBOARD_BASE_URL = \"https://completely-touched-platypus.ngrok-free.app/\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "if MODEL_TYPE == \"qwen\":\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-qwen2.5-7b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|im_start|>assistant\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/qwen-2.5-7b-instruct/saes\"\n",
    "    \n",
    "elif MODEL_TYPE == \"llama\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-llama-3.1-8b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b-instruct/saes\"\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_CONFIG = {\n",
    "    \"release\": SAE_RELEASE,\n",
    "    \"layer\": SAE_LAYER,\n",
    "    \"trainer\": SAE_TRAINER\n",
    "}\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_PATH = \"./prompts/general\"\n",
    "FEATURES_FILE = \"./results/1_personal/only_personal.csv\"\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "TOP_FEATURES = 100\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE: {SAE_RELEASE}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Available token types: {list(TOKEN_OFFSETS.keys())}\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_prompts(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts with labels from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "            labels.append(data['label'])\n",
    "    return pd.DataFrame({'prompt': prompts, 'label': labels})\n",
    "\n",
    "# Load prompts from multiple .jsonl files in PROMPTS_PATH into one dataframe\n",
    "prompts_df = pd.DataFrame()\n",
    "for file in os.listdir(PROMPTS_PATH):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        df = load_prompts(os.path.join(PROMPTS_PATH, file))\n",
    "        prompts_df = pd.concat([prompts_df, df])\n",
    "\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: PreTrainedTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdfffc66fd434de8a95b957bfa8b7043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: LlamaForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device_map_value = device.index if device.type == 'cuda' and device.index is not None else str(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device_map_value}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found SAE files at: /workspace/sae/llama-3.1-8b-instruct/saes/resid_post_layer_15/trainer_1\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load SAE\n",
    "ae_file_path = os.path.join(SAE_PATH, \"ae.pt\")\n",
    "config_file_path = os.path.join(SAE_PATH, \"config.json\")\n",
    "\n",
    "if os.path.exists(ae_file_path) and os.path.exists(config_file_path):\n",
    "    print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "else:\n",
    "    print(f\"SAE not found locally, downloading from {SAE_RELEASE}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    sae_path = f\"resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "    local_dir = SAE_BASE_PATH\n",
    "    ae_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/ae.pt\", local_dir=local_dir)\n",
    "    config_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/config.json\", local_dir=local_dir)\n",
    "\n",
    "sae, _ = load_dictionary(SAE_PATH, device=device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded with {sae.dict_size} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"\n",
    "    Find the position of the assistant token based on the given offset.\n",
    "    \n",
    "    Args:\n",
    "        input_ids: Input token IDs for a single prompt\n",
    "        attention_mask: Attention mask for the prompt\n",
    "        assistant_header: The assistant header string to find\n",
    "        token_offset: Offset from the end of assistant header\n",
    "        tokenizer: Tokenizer instance\n",
    "        device: Device to use for computations\n",
    "    \n",
    "    Returns:\n",
    "        Position index for token extraction\n",
    "    \"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return assistant_pos\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations_all_positions(prompts: List[str], layer_idx: int) -> Tuple[torch.Tensor, List[Dict]]:\n",
    "    \"\"\"Extract activations from specified layer for all positions, then extract specific token positions.\"\"\"\n",
    "    all_activations = []\n",
    "    all_metadata = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            # Output is tuple, take first element (hidden states)\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass (will be stopped by hook)\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # For each prompt in the batch, calculate positions for all token types\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Calculate positions for all token types\n",
    "            positions = {}\n",
    "            for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "                positions[token_type] = find_assistant_position(\n",
    "                    input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "                )\n",
    "            \n",
    "            # Store the full activation sequence and metadata\n",
    "            all_activations.append(activations[j].cpu())  # [seq_len, hidden_dim]\n",
    "            all_metadata.append({\n",
    "                'prompt_idx': i + j,\n",
    "                'positions': positions,\n",
    "                'attention_mask': attention_mask.cpu()\n",
    "            })\n",
    "    \n",
    "    # Find the maximum sequence length across all activations\n",
    "    max_seq_len = max(act.shape[0] for act in all_activations)\n",
    "    hidden_dim = all_activations[0].shape[1]\n",
    "    \n",
    "    # Pad all activations to the same length\n",
    "    padded_activations = []\n",
    "    for act in all_activations:\n",
    "        if act.shape[0] < max_seq_len:\n",
    "            # Pad with zeros\n",
    "            padding = torch.zeros(max_seq_len - act.shape[0], hidden_dim)\n",
    "            padded_act = torch.cat([act, padding], dim=0)\n",
    "        else:\n",
    "            padded_act = act\n",
    "        padded_activations.append(padded_act)\n",
    "    \n",
    "    return torch.stack(padded_activations, dim=0), all_metadata\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_activations(full_activations: torch.Tensor, metadata: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extract activations for specific token positions from full sequence activations.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results for each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = []\n",
    "    \n",
    "    # Extract activations for each token type\n",
    "    for i, meta in enumerate(metadata):\n",
    "        for token_type, position in meta['positions'].items():\n",
    "            # Extract activation at the specific position\n",
    "            activation = full_activations[i, position, :]  # [hidden_dim]\n",
    "            results[token_type].append(activation)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = torch.stack(results[token_type], dim=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for all positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aebdf7aa7e8846de803e50718b89f59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full activations shape: torch.Size([140, 160, 4096])\n",
      "\n",
      "Extracting activations for all token types...\n",
      "Token type 'asst' activations shape: torch.Size([140, 4096])\n",
      "Token type 'endheader' activations shape: torch.Size([140, 4096])\n",
      "Token type 'newline' activations shape: torch.Size([140, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for all positions first, then extract specific token positions\n",
    "print(\"Extracting activations for all positions...\")\n",
    "full_activations, metadata = extract_activations_all_positions(prompts_df['prompt'], LAYER_INDEX)\n",
    "print(f\"Full activations shape: {full_activations.shape}\")\n",
    "\n",
    "# Extract activations for all token types\n",
    "print(\"\\nExtracting activations for all token types...\")\n",
    "token_activations = extract_token_activations(full_activations, metadata)\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Token type '{token_type}' activations shape: {activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for all token types...\n",
      "Processing SAE features for token type 'asst'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape for 'asst': torch.Size([140, 131072])\n",
      "Processing SAE features for token type 'endheader'...\n",
      "Features shape for 'endheader': torch.Size([140, 131072])\n",
      "Processing SAE features for token type 'newline'...\n",
      "Features shape for 'newline': torch.Size([140, 131072])\n",
      "\n",
      "Completed SAE feature extraction for 3 token types\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Get SAE feature activations for all token types\n",
    "print(\"Computing SAE features for all token types...\")\n",
    "token_features = {}\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Processing SAE features for token type '{token_type}'...\")\n",
    "    features = get_sae_features(activations)\n",
    "    token_features[token_type] = features\n",
    "    print(f\"Features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "print(f\"\\nCompleted SAE feature extraction for {len(token_features)} token types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "function-cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def filter_features_by_token_and_source(features_df: pd.DataFrame, token_type: str, source: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Filter features from the features DataFrame based on token type and source.\n",
    "    \n",
    "    Args:\n",
    "        features_df: DataFrame containing feature information\n",
    "        token_type: Token type to filter by (e.g., 'asst', 'newline', 'endheader')\n",
    "        source: Source to filter by (e.g., 'qwen_trainer1_layer15')\n",
    "    \n",
    "    Returns:\n",
    "        Filtered DataFrame containing only matching features\n",
    "    \"\"\"\n",
    "    filtered_df = features_df[\n",
    "        (features_df['token'] == token_type) & \n",
    "        (features_df['source'] == source)\n",
    "    ].copy()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_filtered_feature_activations(token_features: torch.Tensor, features_df: pd.DataFrame, \n",
    "                                   token_type: str, source: str) -> Tuple[torch.Tensor, torch.Tensor, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Get feature activations for filtered features based on token type and source.\n",
    "    \n",
    "    Args:\n",
    "        token_features: Feature activations tensor of shape [num_prompts, num_features]\n",
    "        features_df: DataFrame containing feature information\n",
    "        token_type: Token type to filter by\n",
    "        source: Source to filter by\n",
    "    \n",
    "    Returns:\n",
    "        feature_indices: Indices of filtered features\n",
    "        feature_activations: Activation values for filtered features [num_prompts, num_selected_features]\n",
    "        filtered_features_df: DataFrame with filtered feature information\n",
    "    \"\"\"\n",
    "    # Filter features by token type and source\n",
    "    filtered_features_df = filter_features_by_token_and_source(features_df, token_type, source)\n",
    "    \n",
    "    if len(filtered_features_df) == 0:\n",
    "        print(f\"Warning: No features found for token_type='{token_type}', source='{source}'\")\n",
    "        return torch.tensor([]), torch.tensor([]), filtered_features_df\n",
    "    \n",
    "    # Get feature indices\n",
    "    feature_ids = filtered_features_df['feature_id'].tolist()\n",
    "    feature_indices = torch.tensor(feature_ids, dtype=torch.long)\n",
    "    \n",
    "    # Extract activations for these features\n",
    "    feature_activations = token_features[:, feature_indices]\n",
    "    \n",
    "    print(f\"Found {len(feature_indices)} features for token_type='{token_type}', source='{source}'\")\n",
    "    \n",
    "    return feature_indices, feature_activations, filtered_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 92 total features from ./results/1_personal/only_personal.csv\n",
      "Processing results for source: llama_trainer1_layer15\n",
      "\n",
      "Processing token type: asst\n",
      "Found 2 features for token_type='asst', source='llama_trainer1_layer15'\n",
      "Processed 1 active features for token_type='asst' (skipped 1 inactive)\n",
      "\n",
      "Processing token type: endheader\n",
      "Found 6 features for token_type='endheader', source='llama_trainer1_layer15'\n",
      "Processed 2 active features for token_type='endheader' (skipped 4 inactive)\n",
      "\n",
      "Processing token type: newline\n",
      "Found 7 features for token_type='newline', source='llama_trainer1_layer15'\n",
      "Processed 0 active features for token_type='newline' (skipped 7 inactive)\n",
      "\n",
      "Total active features with results: 3\n",
      "\n",
      "Preview of saved data:\n",
      " feature_id  activation_mean  activation_max  activation_min  num_prompts chat_desc pt_desc type                 source     token                                                                                          link\n",
      "      27476         0.310405        0.338905        0.288229            5                        llama_trainer1_layer15      asst https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=27476\n",
      "      59035         0.362718        0.435948        0.286962           14                        llama_trainer1_layer15 endheader https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=59035\n",
      "      47776         0.400181        0.400181        0.400181            1                        llama_trainer1_layer15 endheader https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=47776\n",
      "\n",
      "Sample dashboard link: https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=27476\n",
      "\n",
      "Summary by token type:\n",
      "          activation_mean                 activation_max num_prompts      \\\n",
      "                    count    mean     max            max        mean max   \n",
      "token                                                                      \n",
      "asst                    1  0.3104  0.3104         0.3389         5.0   5   \n",
      "endheader               2  0.3814  0.4002         0.4359         7.5  14   \n",
      "\n",
      "          feature_id  \n",
      "             nunique  \n",
      "token                 \n",
      "asst               1  \n",
      "endheader          2  \n"
     ]
    }
   ],
   "source": [
    "# Load target features from file\n",
    "target_features_df = pd.read_csv(FEATURES_FILE)\n",
    "print(f\"Loaded {len(target_features_df)} total features from {FEATURES_FILE}\")\n",
    "\n",
    "# Prepare all results for CSV\n",
    "all_results = []\n",
    "source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "\n",
    "print(f\"Processing results for source: {source_name}\")\n",
    "\n",
    "# Process each token type\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"\\nProcessing token type: {token_type}\")\n",
    "    \n",
    "    # Get filtered feature activations for this token type and source\n",
    "    feature_indices, feature_activations, filtered_features_df = get_filtered_feature_activations(\n",
    "        token_features[token_type], target_features_df, token_type, source_name\n",
    "    )\n",
    "    \n",
    "    if len(filtered_features_df) == 0:\n",
    "        print(f\"No features found for token_type='{token_type}', source='{source_name}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Calculate statistics for each feature\n",
    "    features_processed = 0\n",
    "    features_skipped = 0\n",
    "    \n",
    "    for idx, (feature_idx, feature_id) in enumerate(zip(feature_indices, filtered_features_df['feature_id'])):\n",
    "        activations = feature_activations[:, idx]  # [num_prompts]\n",
    "        \n",
    "        # Calculate statistics only on active features (activation > 0)\n",
    "        active_mask = activations > 0\n",
    "        active_activations = activations[active_mask]\n",
    "        \n",
    "        # Skip features that aren't active on any prompt\n",
    "        if len(active_activations) == 0:\n",
    "            features_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        activation_mean = float(active_activations.mean())\n",
    "        activation_max = float(active_activations.max())\n",
    "        activation_min = float(active_activations.min())\n",
    "        num_prompts = len(active_activations)\n",
    "        \n",
    "        # Create dashboard link\n",
    "        dashboard_link = f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer={SAE_TRAINER}&fids={feature_id}\"\n",
    "        \n",
    "        # Add to results with specified column order\n",
    "        result = {\n",
    "            'feature_id': int(feature_id),\n",
    "            'activation_mean': activation_mean,\n",
    "            'activation_max': activation_max,\n",
    "            'activation_min': activation_min,\n",
    "            'num_prompts': num_prompts,\n",
    "            'chat_desc': '',\n",
    "            'pt_desc': '',\n",
    "            'type': '',\n",
    "            'source': source_name,\n",
    "            'token': token_type,\n",
    "            'link': dashboard_link\n",
    "        }\n",
    "        \n",
    "        all_results.append(result)\n",
    "        features_processed += 1\n",
    "    \n",
    "    print(f\"Processed {features_processed} active features for token_type='{token_type}' (skipped {features_skipped} inactive)\")\n",
    "\n",
    "# Convert to DataFrame with specified column order\n",
    "column_order = ['feature_id', 'activation_mean', 'activation_max', 'activation_min', \n",
    "                'num_prompts', 'chat_desc', 'pt_desc', 'type', 'source', 'token', 'link']\n",
    "results_df = pd.DataFrame(all_results)[column_order]\n",
    "print(f\"\\nTotal active features with results: {len(results_df)}\")\n",
    "\n",
    "# # Save to CSV (append mode if file exists, otherwise create new)\n",
    "# if os.path.exists(OUTPUT_FILE):\n",
    "#     # If file exists, append to it\n",
    "#     results_df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "#     print(f\"Results appended to existing file: {OUTPUT_FILE}\")\n",
    "# else:\n",
    "#     # Create new file\n",
    "#     results_df.to_csv(OUTPUT_FILE, index=False)\n",
    "#     print(f\"Results saved to new file: {OUTPUT_FILE}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nPreview of saved data:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Show sample link\n",
    "    sample_link = results_df.iloc[0]['link']\n",
    "    print(f\"\\nSample dashboard link: {sample_link}\")\n",
    "    \n",
    "    # Show summary by token type\n",
    "    print(f\"\\nSummary by token type:\")\n",
    "    summary = results_df.groupby('token').agg({\n",
    "        'activation_mean': ['count', 'mean', 'max'],\n",
    "        'activation_max': 'max',\n",
    "        'num_prompts': ['mean', 'max'],\n",
    "        'feature_id': 'nunique'\n",
    "    }).round(4)\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"No results to save - no features were active on any prompts.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Record prompts which activate target features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detailed token activation recording functions defined\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def record_all_token_activations_for_feature(feature_id: int, feature_idx: int, \n",
    "                                           feature_activations: torch.Tensor,\n",
    "                                           prompts_df: pd.DataFrame,\n",
    "                                           full_activations: torch.Tensor,\n",
    "                                           activation_threshold: float = 0.0) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Record activations for every non-zero token in prompts that activate a specific feature.\n",
    "    \n",
    "    Args:\n",
    "        feature_id: The actual feature ID\n",
    "        feature_idx: Index of the feature in the filtered feature list\n",
    "        feature_activations: Feature activations for this token type [num_prompts, num_features]\n",
    "        prompts_df: DataFrame with prompts and labels\n",
    "        full_activations: Full sequence activations [num_prompts, seq_len, hidden_dim]\n",
    "        activation_threshold: Minimum activation to consider as \"active\"\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with token-level activation data\n",
    "    \"\"\"\n",
    "    # Find which prompts activate this feature\n",
    "    activations = feature_activations[:, feature_idx]\n",
    "    active_mask = activations > activation_threshold\n",
    "    active_indices = torch.where(active_mask)[0]\n",
    "    \n",
    "    if len(active_indices) == 0:\n",
    "        return []\n",
    "    \n",
    "    all_token_records = []\n",
    "    \n",
    "    for prompt_idx in active_indices:\n",
    "        prompt_idx = int(prompt_idx)\n",
    "        \n",
    "        # Get the full activation sequence for this prompt\n",
    "        prompt_activations = full_activations[prompt_idx]  # [seq_len, hidden_dim]\n",
    "        \n",
    "        # Get SAE features for all positions in this prompt\n",
    "        prompt_sae_features = get_sae_features(prompt_activations.unsqueeze(0))  # [1, seq_len, num_features]\n",
    "        prompt_sae_features = prompt_sae_features.squeeze(0)  # [seq_len, num_features]\n",
    "        \n",
    "        # Get activations for our specific feature across all positions\n",
    "        feature_activations_sequence = prompt_sae_features[:, feature_id]  # [seq_len]\n",
    "        \n",
    "        # Get the formatted prompt to tokenize properly\n",
    "        prompt_text = prompts_df.iloc[prompt_idx][\"prompt\"]\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        # Tokenize the formatted prompt\n",
    "        tokenized = tokenizer(formatted_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "        input_ids = tokenized[\"input_ids\"].squeeze(0)\n",
    "        \n",
    "        # Record all non-zero activations\n",
    "        for pos_idx in range(len(feature_activations_sequence)):\n",
    "            activation_val = float(feature_activations_sequence[pos_idx])\n",
    "            \n",
    "            if activation_val > 0 and pos_idx < len(input_ids):\n",
    "                token_id = int(input_ids[pos_idx])\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                \n",
    "                record = {\n",
    "                    \"feature_id\": feature_id,\n",
    "                    \"prompt_id\": prompt_idx,\n",
    "                    \"prompt_text\": prompt_text,\n",
    "                    \"prompt_label\": prompts_df.iloc[prompt_idx][\"label\"],\n",
    "                    \"prompt_feature_activation\": float(activations[prompt_idx]),\n",
    "                    \"token_position\": pos_idx,\n",
    "                    \"token_id\": token_id,\n",
    "                    \"token_text\": token_text,\n",
    "                    \"token_activation\": activation_val\n",
    "                }\n",
    "                all_token_records.append(record)\n",
    "    \n",
    "    return all_token_records\n",
    "\n",
    "@torch.no_grad()\n",
    "def analyze_all_features_detailed(token_features: Dict[str, torch.Tensor],\n",
    "                                target_features_df: pd.DataFrame,\n",
    "                                prompts_df: pd.DataFrame,\n",
    "                                full_activations: torch.Tensor,\n",
    "                                source: str) -> Dict[str, List[Dict]]:\n",
    "    \"\"\"\n",
    "    Analyze all features and record detailed token activations.\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary mapping token_type to list of token activation records\n",
    "    \"\"\"\n",
    "    all_detailed_results = {}\n",
    "    \n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        print(f\"Processing detailed analysis for token type: {token_type}\")\n",
    "        \n",
    "        # Get filtered feature activations for this token type and source\n",
    "        _, feature_activations, filtered_features_df = get_filtered_feature_activations(\n",
    "            token_features[token_type], target_features_df, token_type, source\n",
    "        )\n",
    "        \n",
    "        if len(filtered_features_df) == 0:\n",
    "            print(f\"No features found for token_type='{token_type}', source='{source}'. Skipping.\")\n",
    "            all_detailed_results[token_type] = []\n",
    "            continue\n",
    "        \n",
    "        token_records = []\n",
    "        \n",
    "        # Process each feature\n",
    "        for idx, feature_id in enumerate(filtered_features_df['feature_id']):\n",
    "            feature_id = int(feature_id)\n",
    "            \n",
    "            # Get detailed token records for this feature\n",
    "            feature_records = record_all_token_activations_for_feature(\n",
    "                feature_id, idx, feature_activations, prompts_df, \n",
    "                full_activations\n",
    "            )\n",
    "            \n",
    "            token_records.extend(feature_records)\n",
    "        \n",
    "        all_detailed_results[token_type] = token_records\n",
    "        print(f\"Recorded {len(token_records)} token activations for token_type='{token_type}'\")\n",
    "    \n",
    "    return all_detailed_results\n",
    "\n",
    "print(\"Detailed token activation recording functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running detailed token activation analysis...\n",
      "Processing detailed analysis for token type: asst\n",
      "Found 2 features for token_type='asst', source='llama_trainer1_layer15'\n",
      "Recorded 40 token activations for token_type='asst'\n",
      "Processing detailed analysis for token type: endheader\n",
      "Found 6 features for token_type='endheader', source='llama_trainer1_layer15'\n",
      "Recorded 370 token activations for token_type='endheader'\n",
      "Processing detailed analysis for token type: newline\n",
      "Found 7 features for token_type='newline', source='llama_trainer1_layer15'\n",
      "Recorded 0 token activations for token_type='newline'\n",
      "Saved 3 feature records (20 prompt records) to ./results/3_personal_general/2_personal_general_prompts.jsonl\n",
      "\n",
      "Preview of first record:\n",
      "Feature 27476 (asst, llama_trainer1_layer15):\n",
      "  5 active prompts\n",
      "  First prompt: 12 active tokens\n",
      "  Top token: '<|begin_of_text|>' (activation: 30.1081)\n",
      "\n",
      "File structure summary:\n",
      "  Feature 27476 (asst): 5 prompts, 40 tokens\n",
      "  Feature 47776 (endheader): 1 prompts, 3 tokens\n",
      "  Feature 59035 (endheader): 14 prompts, 367 tokens\n"
     ]
    }
   ],
   "source": [
    "# Run detailed token activation analysis and save as JSONL\n",
    "print(\"Running detailed token activation analysis...\")\n",
    "detailed_results = analyze_all_features_detailed(\n",
    "    token_features, target_features_df, prompts_df, \n",
    "    full_activations, source_name\n",
    ")\n",
    "\n",
    "# Convert to JSONL format\n",
    "\n",
    "os.makedirs(os.path.dirname(PROMPT_OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "total_records = 0\n",
    "jsonl_records = []\n",
    "\n",
    "for token_type, token_records in detailed_results.items():\n",
    "    if len(token_records) > 0:\n",
    "        # Group by feature_id\n",
    "        feature_groups = {}\n",
    "        for record in token_records:\n",
    "            feature_id = record['feature_id']\n",
    "            if feature_id not in feature_groups:\n",
    "                feature_groups[feature_id] = []\n",
    "            feature_groups[feature_id].append(record)\n",
    "        \n",
    "        # Create JSONL records for each feature\n",
    "        for feature_id, records in feature_groups.items():\n",
    "            # Group by prompt_id\n",
    "            prompt_groups = {}\n",
    "            for record in records:\n",
    "                prompt_id = record['prompt_id']  # Will rename to prompt_id\n",
    "                if prompt_id not in prompt_groups:\n",
    "                    prompt_groups[prompt_id] = {\n",
    "                        'prompt_id': prompt_id,\n",
    "                        'prompt_text': record['prompt_text'],\n",
    "                        'prompt_label': record['prompt_label'],\n",
    "                        'prompt_feature_activation': record['prompt_feature_activation'],\n",
    "                        'tokens': []\n",
    "                    }\n",
    "                \n",
    "                # Add token info\n",
    "                prompt_groups[prompt_id]['tokens'].append({\n",
    "                    'position': record['token_position'],\n",
    "                    'token_id': record['token_id'],\n",
    "                    'text': record['token_text'],\n",
    "                    'activation': record['token_activation']\n",
    "                })\n",
    "            \n",
    "            # Sort tokens by activation (descending) within each prompt\n",
    "            for prompt_data in prompt_groups.values():\n",
    "                prompt_data['tokens'].sort(key=lambda x: x['activation'], reverse=True)\n",
    "            \n",
    "            # Create final JSONL record\n",
    "            jsonl_record = {\n",
    "                'feature_id': feature_id,\n",
    "                'token': token_type,  # Renamed from token_type\n",
    "                'source': source_name,\n",
    "                'active_prompts': list(prompt_groups.values())\n",
    "            }\n",
    "            \n",
    "            jsonl_records.append(jsonl_record)\n",
    "            total_records += len(prompt_groups)\n",
    "\n",
    "# Sort by feature_id for consistent output\n",
    "jsonl_records.sort(key=lambda x: x['feature_id'])\n",
    "\n",
    "# Write to JSONL file (append if exists)\n",
    "with open(PROMPT_OUTPUT_FILE, 'a') as f:\n",
    "    for record in jsonl_records:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"Saved {len(jsonl_records)} feature records ({total_records} prompt records) to {PROMPT_OUTPUT_FILE}\")\n",
    "\n",
    "# Show preview\n",
    "if len(jsonl_records) > 0:\n",
    "    print(f\"\\nPreview of first record:\")\n",
    "    sample_record = jsonl_records[0]\n",
    "    print(f\"Feature {sample_record['feature_id']} ({sample_record['token']}, {sample_record['source']}):\")\n",
    "    print(f\"  {len(sample_record['active_prompts'])} active prompts\")\n",
    "    if sample_record['active_prompts']:\n",
    "        first_prompt = sample_record['active_prompts'][0]\n",
    "        print(f\"  First prompt: {len(first_prompt['tokens'])} active tokens\")\n",
    "        print(f\"  Top token: '{first_prompt['tokens'][0]['text']}' (activation: {first_prompt['tokens'][0]['activation']:.4f})\")\n",
    "    \n",
    "    print(f\"\\nFile structure summary:\")\n",
    "    for record in jsonl_records:\n",
    "        total_tokens = sum(len(prompt['tokens']) for prompt in record['active_prompts'])\n",
    "        print(f\"  Feature {record['feature_id']} ({record['token']}): {len(record['active_prompts'])} prompts, {total_tokens} tokens\")\n",
    "else:\n",
    "    print(\"No records to save.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
