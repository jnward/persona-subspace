{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Personal Feature Activations on General Prompts (Optimized)\n",
    "\n",
    "This notebook analyzes which given SAE features are activated on given prompts and generates both CSV and JSONL outputs in a single pass, optimized for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"qwen\"  # Options: \"qwen\" or \"llama\"\n",
    "SAE_LAYER = 11\n",
    "SAE_TRAINER = 1\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_FILE = f\"./results/3_personal_general/2_personal_general.csv\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "PROMPT_OUTPUT_FILE = f\"./results/3_personal_general/2_personal_general_prompts.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPT_OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DASHBOARD URL - Global variable for links\n",
    "# =============================================================================\n",
    "FEATURE_DASHBOARD_BASE_URL = \"https://completely-touched-platypus.ngrok-free.app/\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "if MODEL_TYPE == \"qwen\":\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-qwen2.5-7b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|im_start|>assistant\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/qwen-2.5-7b-instruct/saes\"\n",
    "    \n",
    "elif MODEL_TYPE == \"llama\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-llama-3.1-8b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b-instruct/saes\"\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_CONFIG = {\n",
    "    \"release\": SAE_RELEASE,\n",
    "    \"layer\": SAE_LAYER,\n",
    "    \"trainer\": SAE_TRAINER\n",
    "}\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_PATH = \"./prompts/general\"\n",
    "FEATURES_FILE = \"./results/1_personal/only_personal.csv\"\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "TOP_FEATURES = 100\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE: {SAE_RELEASE}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Available token types: {list(TOKEN_OFFSETS.keys())}\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")\n",
    "print(f\"  Prompt output file: {PROMPT_OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_prompts(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts with labels from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "            labels.append(data['label'])\n",
    "    return pd.DataFrame({'prompt': prompts, 'label': labels})\n",
    "\n",
    "# Load prompts from multiple .jsonl files in PROMPTS_PATH into one dataframe\n",
    "prompts_df = pd.DataFrame()\n",
    "for file in os.listdir(PROMPTS_PATH):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        df = load_prompts(os.path.join(PROMPTS_PATH, file))\n",
    "        prompts_df = pd.concat([prompts_df, df])\n",
    "\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "device_map_value = device.index if device.type == 'cuda' and device.index is not None else str(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device_map_value}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SAE\n",
    "ae_file_path = os.path.join(SAE_PATH, \"ae.pt\")\n",
    "config_file_path = os.path.join(SAE_PATH, \"config.json\")\n",
    "\n",
    "if os.path.exists(ae_file_path) and os.path.exists(config_file_path):\n",
    "    print(f\"âœ“ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "else:\n",
    "    print(f\"SAE not found locally, downloading from {SAE_RELEASE}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    sae_path = f\"resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "    local_dir = SAE_BASE_PATH\n",
    "    ae_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/ae.pt\", local_dir=local_dir)\n",
    "    config_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/config.json\", local_dir=local_dir)\n",
    "\n",
    "sae, _ = load_dictionary(SAE_PATH, device=device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded with {sae.dict_size} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return assistant_pos\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations_and_metadata(prompts: List[str], layer_idx: int) -> Tuple[torch.Tensor, List[Dict], List[str]]:\n",
    "    \"\"\"Extract activations and prepare metadata for all prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    all_metadata = []\n",
    "    formatted_prompts_list = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        formatted_prompts_list.extend(formatted_prompts)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # For each prompt in the batch, calculate positions for all token types\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Calculate positions for all token types\n",
    "            positions = {}\n",
    "            for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "                positions[token_type] = find_assistant_position(\n",
    "                    input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "                )\n",
    "            \n",
    "            # Store the full activation sequence and metadata\n",
    "            all_activations.append(activations[j].cpu())  # [seq_len, hidden_dim]\n",
    "            all_metadata.append({\n",
    "                'prompt_idx': i + j,\n",
    "                'positions': positions,\n",
    "                'attention_mask': attention_mask.cpu(),\n",
    "                'input_ids': input_ids.cpu()\n",
    "            })\n",
    "    \n",
    "    # Find the maximum sequence length across all activations\n",
    "    max_seq_len = max(act.shape[0] for act in all_activations)\n",
    "    hidden_dim = all_activations[0].shape[1]\n",
    "    \n",
    "    # Pad all activations to the same length\n",
    "    padded_activations = []\n",
    "    for act in all_activations:\n",
    "        if act.shape[0] < max_seq_len:\n",
    "            padding = torch.zeros(max_seq_len - act.shape[0], hidden_dim)\n",
    "            padded_act = torch.cat([act, padding], dim=0)\n",
    "        else:\n",
    "            padded_act = act\n",
    "        padded_activations.append(padded_act)\n",
    "    \n",
    "    return torch.stack(padded_activations, dim=0), all_metadata, formatted_prompts_list\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_activations(full_activations: torch.Tensor, metadata: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extract activations for specific token positions from full sequence activations.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results for each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = []\n",
    "    \n",
    "    # Extract activations for each token type\n",
    "    for i, meta in enumerate(metadata):\n",
    "        for token_type, position in meta['positions'].items():\n",
    "            # Extract activation at the specific position\n",
    "            activation = full_activations[i, position, :]  # [hidden_dim]\n",
    "            results[token_type].append(activation)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = torch.stack(results[token_type], dim=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract activations for all positions first, then extract specific token positions\n",
    "print(\"Extracting activations for all positions...\")\n",
    "full_activations, metadata, formatted_prompts = extract_activations_and_metadata(prompts_df['prompt'].tolist(), LAYER_INDEX)\n",
    "print(f\"Full activations shape: {full_activations.shape}\")\n",
    "\n",
    "# Extract activations for all token types\n",
    "print(\"\\nExtracting activations for specific token positions...\")\n",
    "token_activations = extract_token_activations(full_activations, metadata)\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Token type '{token_type}' activations shape: {activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features_batched(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations with proper batching.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Get SAE feature activations for specific token positions\n",
    "print(\"Computing SAE features for specific token positions...\")\n",
    "token_features = {}\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Processing SAE features for token type '{token_type}'...\")\n",
    "    features = get_sae_features_batched(activations)\n",
    "    token_features[token_type] = features\n",
    "    print(f\"Features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "# OPTIMIZATION: Pre-compute SAE features for ALL positions at once\n",
    "print(\"\\nOptimization: Pre-computing SAE features for all positions...\")\n",
    "print(f\"Processing {full_activations.shape[0]} prompts with max {full_activations.shape[1]} tokens each...\")\n",
    "\n",
    "# Reshape to [total_positions, hidden_dim]\n",
    "total_positions = full_activations.shape[0] * full_activations.shape[1]\n",
    "reshaped_activations = full_activations.view(total_positions, -1)\n",
    "\n",
    "# Apply SAE to all positions\n",
    "full_sae_features = get_sae_features_batched(reshaped_activations)\n",
    "\n",
    "# Reshape back to [num_prompts, seq_len, num_features]\n",
    "full_sae_features = full_sae_features.view(full_activations.shape[0], full_activations.shape[1], -1)\n",
    "\n",
    "print(f\"Full SAE features shape: {full_sae_features.shape}\")\n",
    "print(f\"âœ“ SAE features pre-computed for all positions\")\n",
    "\n",
    "print(f\"\\nCompleted SAE feature extraction for {len(token_features)} token types\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Combined Analysis and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def filter_features_by_token_and_source(features_df: pd.DataFrame, token_type: str, source: str) -> pd.DataFrame:\n",
    "    \"\"\"Filter features from the features DataFrame based on token type and source.\"\"\"\n",
    "    filtered_df = features_df[\n",
    "        (features_df['token'] == token_type) & \n",
    "        (features_df['source'] == source)\n",
    "    ].copy()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_filtered_feature_activations(token_features: torch.Tensor, features_df: pd.DataFrame, \n",
    "                                   token_type: str, source: str) -> Tuple[torch.Tensor, torch.Tensor, pd.DataFrame]:\n",
    "    \"\"\"Get feature activations for filtered features based on token type and source.\"\"\"\n",
    "    # Filter features by token type and source\n",
    "    filtered_features_df = filter_features_by_token_and_source(features_df, token_type, source)\n",
    "    \n",
    "    if len(filtered_features_df) == 0:\n",
    "        print(f\"Warning: No features found for token_type='{token_type}', source='{source}'\")\n",
    "        return torch.tensor([]), torch.tensor([]), filtered_features_df\n",
    "    \n",
    "    # Get feature indices\n",
    "    feature_ids = filtered_features_df['feature_id'].tolist()\n",
    "    feature_indices = torch.tensor(feature_ids, dtype=torch.long)\n",
    "    \n",
    "    # Extract activations for these features\n",
    "    feature_activations = token_features[:, feature_indices]\n",
    "    \n",
    "    print(f\"Found {len(feature_indices)} features for token_type='{token_type}', source='{source}'\")\n",
    "    \n",
    "    return feature_indices, feature_activations, filtered_features_df\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_detailed_tokens_optimized(feature_id: int, feature_activations: torch.Tensor, \n",
    "                                     feature_idx: int, prompts_df: pd.DataFrame,\n",
    "                                     full_sae_features: torch.Tensor, metadata: List[Dict],\n",
    "                                     activation_threshold: float = 0.0) -> List[Dict]:\n",
    "    \"\"\"Collect detailed token activations for a specific feature using pre-computed SAE features.\"\"\"\n",
    "    # Find which prompts activate this feature\n",
    "    activations = feature_activations[:, feature_idx]\n",
    "    active_mask = activations > activation_threshold\n",
    "    active_indices = torch.where(active_mask)[0]\n",
    "    \n",
    "    if len(active_indices) == 0:\n",
    "        return []\n",
    "    \n",
    "    all_token_records = []\n",
    "    \n",
    "    for prompt_idx in active_indices:\n",
    "        prompt_idx = int(prompt_idx)\n",
    "        \n",
    "        # OPTIMIZATION: Use pre-computed SAE features instead of re-computing\n",
    "        feature_activations_sequence = full_sae_features[prompt_idx, :, feature_id]  # [seq_len]\n",
    "        \n",
    "        # Get tokenized input from metadata (cached)\n",
    "        input_ids = metadata[prompt_idx]['input_ids']\n",
    "        \n",
    "        # Get prompt info\n",
    "        prompt_text = prompts_df.iloc[prompt_idx][\"prompt\"]\n",
    "        \n",
    "        # Collect token data for this prompt\n",
    "        tokens = []\n",
    "        for pos_idx in range(min(len(feature_activations_sequence), len(input_ids))):\n",
    "            activation_val = float(feature_activations_sequence[pos_idx])\n",
    "            \n",
    "            if activation_val > 0:\n",
    "                token_id = int(input_ids[pos_idx])\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                \n",
    "                tokens.append({\n",
    "                    'position': pos_idx,\n",
    "                    'token_id': token_id,\n",
    "                    'text': token_text,\n",
    "                    'activation': activation_val\n",
    "                })\n",
    "        \n",
    "        # Only add if we found tokens\n",
    "        if tokens:\n",
    "            # Sort tokens by activation (descending)\n",
    "            tokens.sort(key=lambda x: x['activation'], reverse=True)\n",
    "            \n",
    "            all_token_records.append({\n",
    "                'prompt_id': prompt_idx,\n",
    "                'prompt_text': prompt_text,\n",
    "                'prompt_label': prompts_df.iloc[prompt_idx][\"label\"],\n",
    "                'prompt_feature_activation': float(activations[prompt_idx]),\n",
    "                'tokens': tokens\n",
    "            })\n",
    "    \n",
    "    return all_token_records\n",
    "\n",
    "# Pre-compute dashboard link template\n",
    "def create_dashboard_link(feature_id: int) -> str:\n",
    "    \"\"\"Create dashboard link for a feature.\"\"\"\n",
    "    return f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer={SAE_TRAINER}&fids={feature_id}\"\n",
    "\n",
    "print(\"Optimized analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load target features from file\n",
    "target_features_df = pd.read_csv(FEATURES_FILE)\n",
    "print(f\"Loaded {len(target_features_df)} total features from {FEATURES_FILE}\")\n",
    "\n",
    "# Prepare results for both CSV and JSONL\n",
    "csv_results = []\n",
    "jsonl_results = []\n",
    "source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}\"\n",
    "\n",
    "print(f\"Processing results for source: {source_name}\")\n",
    "\n",
    "# Process each token type\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"\\nProcessing token type: {token_type}\")\n",
    "    \n",
    "    # Get filtered feature activations for this token type and source\n",
    "    feature_indices, feature_activations, filtered_features_df = get_filtered_feature_activations(\n",
    "        token_features[token_type], target_features_df, token_type, source_name\n",
    "    )\n",
    "    \n",
    "    if len(filtered_features_df) == 0:\n",
    "        print(f\"No features found for token_type='{token_type}', source='{source_name}'. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # Process each feature\n",
    "    features_processed = 0\n",
    "    features_skipped = 0\n",
    "    \n",
    "    for idx, (feature_idx, feature_id) in enumerate(zip(feature_indices, filtered_features_df['feature_id'])):\n",
    "        feature_id = int(feature_id)\n",
    "        activations = feature_activations[:, idx]  # [num_prompts]\n",
    "        \n",
    "        # Calculate statistics only on active features (activation > 0)\n",
    "        active_mask = activations > 0\n",
    "        active_activations = activations[active_mask]\n",
    "        \n",
    "        # Skip features that aren't active on any prompt\n",
    "        if len(active_activations) == 0:\n",
    "            features_skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # CSV statistics\n",
    "        activation_mean = float(active_activations.mean())\n",
    "        activation_max = float(active_activations.max())\n",
    "        activation_min = float(active_activations.min())\n",
    "        num_prompts = len(active_activations)\n",
    "        \n",
    "        # Add to CSV results\n",
    "        csv_result = {\n",
    "            'feature_id': feature_id,\n",
    "            'activation_mean': activation_mean,\n",
    "            'activation_max': activation_max,\n",
    "            'activation_min': activation_min,\n",
    "            'num_prompts': num_prompts,\n",
    "            'chat_desc': '',\n",
    "            'pt_desc': '',\n",
    "            'type': '',\n",
    "            'source': source_name,\n",
    "            'token': token_type,\n",
    "            'link': create_dashboard_link(feature_id)\n",
    "        }\n",
    "        csv_results.append(csv_result)\n",
    "        \n",
    "        # Collect detailed token data for JSONL (OPTIMIZED)\n",
    "        detailed_tokens = collect_detailed_tokens_optimized(\n",
    "            feature_id, feature_activations, idx, prompts_df, full_sae_features, metadata\n",
    "        )\n",
    "        \n",
    "        # Add to JSONL results if we have detailed data\n",
    "        if detailed_tokens:\n",
    "            jsonl_result = {\n",
    "                'feature_id': feature_id,\n",
    "                'token': token_type,\n",
    "                'source': source_name,\n",
    "                'active_prompts': detailed_tokens\n",
    "            }\n",
    "            jsonl_results.append(jsonl_result)\n",
    "        \n",
    "        features_processed += 1\n",
    "    \n",
    "    print(f\"Processed {features_processed} active features for token_type='{token_type}' (skipped {features_skipped} inactive)\")\n",
    "\n",
    "print(f\"\\nTotal features processed: {len(csv_results)} CSV, {len(jsonl_results)} JSONL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Convert CSV results to DataFrame\n",
    "column_order = ['feature_id', 'activation_mean', 'activation_max', 'activation_min', \n",
    "                'num_prompts', 'chat_desc', 'pt_desc', 'type', 'source', 'token', 'link']\n",
    "csv_df = pd.DataFrame(csv_results)[column_order]\n",
    "\n",
    "# Save CSV results\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    csv_df.to_csv(OUTPUT_FILE, mode='a', header=False, index=False)\n",
    "    print(f\"CSV results appended to existing file: {OUTPUT_FILE}\")\n",
    "else:\n",
    "    csv_df.to_csv(OUTPUT_FILE, index=False)\n",
    "    print(f\"CSV results saved to new file: {OUTPUT_FILE}\")\n",
    "\n",
    "# Save JSONL results\n",
    "jsonl_results.sort(key=lambda x: x['feature_id'])  # Sort by feature_id\n",
    "with open(PROMPT_OUTPUT_FILE, 'a') as f:\n",
    "    for record in jsonl_results:\n",
    "        f.write(json.dumps(record, indent=2) + '\\n')\n",
    "\n",
    "print(f\"JSONL results saved to {PROMPT_OUTPUT_FILE}\")\n",
    "\n",
    "# Show preview and summary\n",
    "if len(csv_df) > 0:\n",
    "    print(f\"\\nPreview of CSV data:\")\n",
    "    print(csv_df.head(10).to_string(index=False))\n",
    "    \n",
    "    print(f\"\\nSummary by token type:\")\n",
    "    summary = csv_df.groupby('token').agg({\n",
    "        'activation_mean': ['count', 'mean', 'max'],\n",
    "        'activation_max': 'max',\n",
    "        'num_prompts': ['mean', 'max'],\n",
    "        'feature_id': 'nunique'\n",
    "    }).round(4)\n",
    "    print(summary)\n",
    "\n",
    "if len(jsonl_results) > 0:\n",
    "    print(f\"\\nJSONL file structure summary:\")\n",
    "    total_prompt_records = 0\n",
    "    total_token_records = 0\n",
    "    for record in jsonl_results:\n",
    "        num_prompts = len(record['active_prompts'])\n",
    "        total_tokens = sum(len(prompt['tokens']) for prompt in record['active_prompts'])\n",
    "        total_prompt_records += num_prompts\n",
    "        total_token_records += total_tokens\n",
    "        print(f\"  Feature {record['feature_id']} ({record['token']}): {num_prompts} prompts, {total_tokens} tokens\")\n",
    "    \n",
    "    print(f\"\\nTotal: {len(jsonl_results)} features, {total_prompt_records} prompt records, {total_token_records} token records\")\n",
    "    \n",
    "    if len(jsonl_results) > 0:\n",
    "        print(f\"\\nSample JSONL record structure:\")\n",
    "        sample_record = jsonl_results[0]\n",
    "        print(f\"Feature {sample_record['feature_id']} ({sample_record['token']}, {sample_record['source']}):\")\n",
    "        if sample_record['active_prompts']:\n",
    "            first_prompt = sample_record['active_prompts'][0]\n",
    "            print(f\"  First prompt: {len(first_prompt['tokens'])} active tokens\")\n",
    "            if first_prompt['tokens']:\n",
    "                print(f\"  Top token: '{first_prompt['tokens'][0]['text']}' (activation: {first_prompt['tokens'][0]['activation']:.4f})\")\n",
    "else:\n",
    "    print(\"No JSONL results to save.\")\n",
    "\n",
    "print(f\"\\nâœ“ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
