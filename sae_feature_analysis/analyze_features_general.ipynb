{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Universal Features\n",
    "\n",
    "This notebook analyzes which SAE features activate for every prompt across different categories of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "  SAE Layer: 15, Trainer: 1\n",
      "  Token extraction: asst (offset: -2)\n",
      "  Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "  Output directory: ./llama_trainer1_layer15_asst\n",
      "  SAE Release: andyrdt/saes-llama-3.1-8b-instruct\n",
      "  Dashboard base URL: https://completely-touched-platypus.ngrok-free.app/\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"llama\"  # Options: \"qwen\" or \"llama\"\n",
    "TOKEN_TYPE = \"asst\"  # Options: \"asst\", \"newline\", \"endheader\" (endheader only for llama)\n",
    "SAE_LAYER = 15\n",
    "SAE_TRAINER = 1\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DASHBOARD URL - Global variable for links\n",
    "# =============================================================================\n",
    "FEATURE_DASHBOARD_BASE_URL = \"https://completely-touched-platypus.ngrok-free.app/\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "if MODEL_TYPE == \"qwen\":\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-qwen2.5-7b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|im_start|>assistant\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/qwen-2.5-7b-instruct/saes\"\n",
    "    \n",
    "elif MODEL_TYPE == \"llama\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-llama-3.1-8b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b-instruct/saes\"\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# Validate token type\n",
    "if TOKEN_TYPE not in TOKEN_OFFSETS:\n",
    "    raise ValueError(f\"TOKEN_TYPE '{TOKEN_TYPE}' not available for {MODEL_TYPE}. Available: {list(TOKEN_OFFSETS.keys())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_CONFIG = {\n",
    "    \"release\": SAE_RELEASE,\n",
    "    \"layer\": SAE_LAYER,\n",
    "    \"trainer\": SAE_TRAINER\n",
    "}\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "TOKEN_OFFSET = TOKEN_OFFSETS[TOKEN_TYPE]\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_PATH = \"./prompts\"\n",
    "\n",
    "# Output directory with clear naming\n",
    "OUTPUT_DIR = f\"./{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{TOKEN_TYPE}\"\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "TOP_FEATURES = 100\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Token extraction: {TOKEN_TYPE} (offset: {TOKEN_OFFSET})\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  SAE Release: {SAE_RELEASE}\")\n",
    "print(f\"  Dashboard base URL: {FEATURE_DASHBOARD_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_prompts(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts with labels from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "            labels.append(data['label'])\n",
    "    return pd.DataFrame({'prompt': prompts, 'label': labels})\n",
    "\n",
    "# Load prompts from multiple .jsonl files in PROMPTS_PATH into one dataframe\n",
    "prompts_df = pd.DataFrame()\n",
    "for file in os.listdir(PROMPTS_PATH):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        df = load_prompts(os.path.join(PROMPTS_PATH, file))\n",
    "        prompts_df = pd.concat([prompts_df, df])\n",
    "\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: PreTrainedTokenizerFast\n",
      "\n",
      "Chat template test:\n",
      "Original: What's it like to be you?\n",
      "Formatted: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's it like to be you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
      "Formatted (readable):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What's it like to be you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "ASSISTANT HEADER TOKENIZATION ANALYSIS\n",
      "============================================================\n",
      "Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "Number of tokens: 3\n",
      "Token IDs: [128006, 78191, 128007]\n",
      "Individual tokens: ['<|start_header_id|>', 'assistant', '<|end_header_id|>']\n",
      "\n",
      "Full prompt tokens: 43\n",
      "All tokens with positions:\n",
      "   0: '<|begin_of_text|>'\n",
      "   1: '<|start_header_id|>'\n",
      "   2: 'system'\n",
      "   3: '<|end_header_id|>'\n",
      "   4: '\n",
      "\n",
      "'\n",
      "   5: 'Cut'\n",
      "   6: 'ting'\n",
      "   7: ' Knowledge'\n",
      "   8: ' Date'\n",
      "   9: ':'\n",
      "  10: ' December'\n",
      "  11: ' '\n",
      "  12: '202'\n",
      "  13: '3'\n",
      "  14: '\n",
      "'\n",
      "  15: 'Today'\n",
      "  16: ' Date'\n",
      "  17: ':'\n",
      "  18: ' '\n",
      "  19: '26'\n",
      "  20: ' Jul'\n",
      "  21: ' '\n",
      "  22: '202'\n",
      "  23: '4'\n",
      "  24: '\n",
      "\n",
      "'\n",
      "  25: '<|eot_id|>'\n",
      "  26: '<|start_header_id|>'\n",
      "  27: 'user'\n",
      "  28: '<|end_header_id|>'\n",
      "  29: '\n",
      "\n",
      "'\n",
      "  30: 'What'\n",
      "  31: ''s'\n",
      "  32: ' it'\n",
      "  33: ' like'\n",
      "  34: ' to'\n",
      "  35: ' be'\n",
      "  36: ' you'\n",
      "  37: '?'\n",
      "  38: '<|eot_id|>'\n",
      "  39: '<|start_header_id|>'\n",
      "  40: 'assistant'\n",
      "  41: '<|end_header_id|>'\n",
      "  42: '\n",
      "\n",
      "'\n",
      "\n",
      "Assistant header found at positions 39 to 41\n",
      "Assistant header tokens: ['<|start_header_id|>', 'assistant', '<|end_header_id|>']\n",
      "\n",
      "Extraction calculation:\n",
      "  assistant_start_pos: 39\n",
      "  + len(assistant_tokens): 3\n",
      "  + TOKEN_OFFSET ('asst'): -2\n",
      "  = extraction_pos: 40\n",
      "✓ Token at extraction position 40: 'assistant'\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Test chat template formatting\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"What's it like to be you?\"}]\n",
    "formatted_test = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"\\nChat template test:\")\n",
    "print(f\"Original: What's it like to be you?\")\n",
    "print(f\"Formatted: {repr(formatted_test)}\")\n",
    "print(f\"Formatted (readable):\\n{formatted_test}\")\n",
    "\n",
    "# Test tokenization of assistant header to understand positioning\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ASSISTANT HEADER TOKENIZATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "assistant_token_texts = [tokenizer.decode([token]) for token in assistant_tokens]\n",
    "\n",
    "print(f\"Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"Number of tokens: {len(assistant_tokens)}\")\n",
    "print(f\"Token IDs: {assistant_tokens}\")\n",
    "print(f\"Individual tokens: {assistant_token_texts}\")\n",
    "\n",
    "# Test with a full formatted prompt\n",
    "full_tokens = tokenizer.encode(formatted_test, add_special_tokens=False)\n",
    "full_token_texts = [tokenizer.decode([token]) for token in full_tokens]\n",
    "\n",
    "print(f\"\\nFull prompt tokens: {len(full_tokens)}\")\n",
    "print(\"All tokens with positions:\")\n",
    "for i, token_text in enumerate(full_token_texts):\n",
    "    print(f\"  {i:2d}: '{token_text}'\")\n",
    "\n",
    "# Find where assistant header appears in full prompt\n",
    "assistant_start_pos = None\n",
    "for i in range(len(full_tokens) - len(assistant_tokens) + 1):\n",
    "    if full_tokens[i:i+len(assistant_tokens)] == assistant_tokens:\n",
    "        assistant_start_pos = i\n",
    "        break\n",
    "\n",
    "if assistant_start_pos is not None:\n",
    "    assistant_end_pos = assistant_start_pos + len(assistant_tokens) - 1\n",
    "    print(f\"\\nAssistant header found at positions {assistant_start_pos} to {assistant_end_pos}\")\n",
    "    print(f\"Assistant header tokens: {full_token_texts[assistant_start_pos:assistant_end_pos+1]}\")\n",
    "    \n",
    "    # Show what the extraction function will actually extract\n",
    "    extraction_pos = assistant_start_pos + len(assistant_tokens) + TOKEN_OFFSET\n",
    "    print(f\"\\nExtraction calculation:\")\n",
    "    print(f\"  assistant_start_pos: {assistant_start_pos}\")\n",
    "    print(f\"  + len(assistant_tokens): {len(assistant_tokens)}\")  \n",
    "    print(f\"  + TOKEN_OFFSET ('{TOKEN_TYPE}'): {TOKEN_OFFSET}\")\n",
    "    print(f\"  = extraction_pos: {extraction_pos}\")\n",
    "    \n",
    "    if 0 <= extraction_pos < len(full_token_texts):\n",
    "        print(f\"✓ Token at extraction position {extraction_pos}: '{full_token_texts[extraction_pos]}'\")\n",
    "    else:\n",
    "        print(f\"❌ Extraction position {extraction_pos} is out of bounds (valid range: 0-{len(full_token_texts)-1})\")\n",
    "else:\n",
    "    print(\"❌ Assistant header not found in full prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c69e93ad99b94b7fa26b4025fbc4f14a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: LlamaForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device_map_value = device.index if device.type == 'cuda' and device.index is not None else str(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device_map_value}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE not found locally, downloading from andyrdt/saes-llama-3.1-8b-instruct...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fa9a5674224a7ba8f377ee36c48d90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ae.pt:   0%|          | 0.00/4.30G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47328bb81c6c41028bcc32804cee1fec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/951 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load SAE\n",
    "ae_file_path = os.path.join(SAE_PATH, \"ae.pt\")\n",
    "config_file_path = os.path.join(SAE_PATH, \"config.json\")\n",
    "\n",
    "if os.path.exists(ae_file_path) and os.path.exists(config_file_path):\n",
    "    print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "else:\n",
    "    print(f\"SAE not found locally, downloading from {SAE_RELEASE}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    sae_path = f\"resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "    local_dir = SAE_BASE_PATH\n",
    "    ae_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/ae.pt\", local_dir=local_dir)\n",
    "    config_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/config.json\", local_dir=local_dir)\n",
    "\n",
    "sae, _ = load_dictionary(SAE_PATH, device=device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded with {sae.dict_size} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations(prompts: List[str], layer_idx: int) -> torch.Tensor:\n",
    "    \"\"\"Extract activations from specified layer for given prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            # Output is tuple, take first element (hidden states)\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass (will be stopped by hook)\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Extract assistant token positions\n",
    "        batch_activations = []\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            # Get attention mask for this item\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            \n",
    "            # Find assistant header position\n",
    "            assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Find where assistant section starts\n",
    "            assistant_pos = None\n",
    "            for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "                if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "                    assistant_pos = k + len(assistant_tokens) + TOKEN_OFFSET\n",
    "                    break\n",
    "            \n",
    "            if assistant_pos is None:\n",
    "                # Fallback to last non-padding token\n",
    "                assistant_pos = attention_mask.sum().item() - 1\n",
    "            \n",
    "            # Ensure position is within bounds\n",
    "            max_pos = attention_mask.sum().item() - 1\n",
    "            assistant_pos = min(assistant_pos, max_pos)\n",
    "            assistant_pos = max(assistant_pos, 0)\n",
    "            \n",
    "            # Extract activation at assistant position\n",
    "            assistant_activation = activations[j, assistant_pos, :]  # [hidden_dim]\n",
    "            batch_activations.append(assistant_activation.cpu())\n",
    "        \n",
    "        all_activations.extend(batch_activations)\n",
    "    \n",
    "    return torch.stack(all_activations, dim=0)\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7290b87b9bd74745a08243cb1f6660ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: torch.Size([140, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for prompts\n",
    "print(\"Extracting activations for prompts...\")\n",
    "activations = extract_activations(prompts_df['prompt'], LAYER_INDEX)\n",
    "print(f\"Activations shape: {activations.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for all prompts...\n",
      "Features shape: torch.Size([140, 131072])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Get SAE feature activations\n",
    "print(\"Computing SAE features for all prompts...\")\n",
    "features = get_sae_features(activations)\n",
    "print(f\"Features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding features that activate for every single prompt...\n",
      "Found 2 features that are active for all 140 prompts\n",
      "Universal features (indices): [84223, 128740]\n",
      "Mean activation values: [1.8158377408981323, 3.660419464111328]\n",
      "\n",
      "Universal features summary (sorted by mean activation):\n",
      " feature_index  mean_activation\n",
      "        128740         3.660419\n",
      "         84223         1.815838\n",
      "\n",
      "Detailed activations for top 5 universal features:\n",
      "Feature 128740: mean=3.6604, min=2.5141, max=4.5422\n",
      "  Activations: [4.178600788116455, 3.9269800186157227, 4.542189121246338, 4.380805969238281, 4.497568607330322, 4.102511405944824, 4.526898384094238, 3.919839859008789, 4.296817779541016, 4.348295211791992, 3.8865933418273926, 4.36292839050293, 4.065620422363281, 3.9093990325927734, 4.011754989624023, 4.290411949157715, 4.210723400115967, 4.361583709716797, 4.098278522491455, 4.20365047454834, 3.6394309997558594, 3.058962345123291, 3.7149276733398438, 3.9324755668640137, 3.44687557220459, 3.8433456420898438, 4.15966796875, 3.4322967529296875, 3.8145017623901367, 3.6618518829345703, 4.019124984741211, 3.6505208015441895, 3.8088741302490234, 3.6856303215026855, 3.446234703063965, 3.9006495475769043, 4.075512886047363, 3.7588424682617188, 3.630995273590088, 4.3567023277282715, 2.903740406036377, 3.573542594909668, 3.5402565002441406, 3.320186138153076, 3.3795175552368164, 2.880777359008789, 3.454916000366211, 3.623793601989746, 2.9293880462646484, 3.2883896827697754, 3.319319725036621, 3.5014543533325195, 3.255648612976074, 3.4451613426208496, 3.4410481452941895, 3.3677592277526855, 3.4126105308532715, 3.218608856201172, 3.7335052490234375, 3.469637393951416, 4.356426239013672, 4.118263244628906, 4.450961589813232, 4.224075794219971, 4.38068962097168, 4.3119354248046875, 4.419695854187012, 4.194684982299805, 4.395674705505371, 4.081084251403809, 4.229349136352539, 4.384596824645996, 4.148880958557129, 4.19559383392334, 4.473026275634766, 4.436694145202637, 3.900111198425293, 4.362290382385254, 4.172516822814941, 4.327024936676025, 3.384601593017578, 3.473191738128662, 3.47291898727417, 3.1754045486450195, 3.443115234375, 3.3684120178222656, 3.4166369438171387, 3.341245174407959, 3.557547092437744, 3.1150193214416504, 3.4554543495178223, 3.1681766510009766, 3.413428783416748, 3.294656753540039, 3.2496824264526367, 3.680467128753662, 3.5738067626953125, 3.274369716644287, 3.534513473510742, 3.2201266288757324, 3.4342379570007324, 3.5431671142578125, 3.267618179321289, 3.675384044647217, 3.492220878601074, 3.3790130615234375, 3.733440399169922, 3.743488311767578, 3.1950836181640625, 3.8317794799804688, 2.7925143241882324, 3.7191224098205566, 3.2433719635009766, 3.1815905570983887, 4.045067310333252, 3.3594822883605957, 3.4645447731018066, 3.2859344482421875, 3.786130905151367, 3.8032140731811523, 2.514089584350586, 3.446605682373047, 3.499508857727051, 2.9587583541870117, 3.11324405670166, 2.94281005859375, 3.604037284851074, 3.372002601623535, 2.877361297607422, 3.526310920715332, 3.2696189880371094, 3.245652198791504, 3.005100727081299, 2.930570125579834, 2.835756778717041, 2.7519989013671875, 3.36613130569458, 3.3785886764526367, 2.936885356903076, 3.5128068923950195]\n",
      "\n",
      "Feature 84223: mean=1.8158, min=1.2251, max=2.2266\n",
      "  Activations: [1.822603702545166, 1.9864120483398438, 1.607107162475586, 1.3560678958892822, 1.8796634674072266, 1.8505942821502686, 1.7132112979888916, 1.8628530502319336, 1.723440170288086, 1.6546266078948975, 1.8999173641204834, 1.6013288497924805, 1.7314062118530273, 1.6407992839813232, 1.5576355457305908, 1.9196436405181885, 1.9736034870147705, 1.8179490566253662, 1.8915607929229736, 1.7747972011566162, 2.1083807945251465, 2.11065411567688, 1.9862611293792725, 2.2245664596557617, 1.970118761062622, 2.069920539855957, 2.105698585510254, 2.120198965072632, 2.115332841873169, 2.04988694190979, 2.079768180847168, 2.018234968185425, 2.2265658378601074, 2.009023904800415, 2.0154237747192383, 2.0607359409332275, 2.1820662021636963, 2.0453057289123535, 1.9348185062408447, 1.9808974266052246, 1.6469814777374268, 1.9760046005249023, 1.78305983543396, 1.718827486038208, 1.8831145763397217, 1.869295358657837, 1.768883228302002, 1.8430702686309814, 1.6796753406524658, 1.878002405166626, 1.937241554260254, 1.8868510723114014, 1.7533104419708252, 1.817563772201538, 1.866518259048462, 1.805755853652954, 1.8852813243865967, 1.8856267929077148, 1.8929383754730225, 1.74955153465271, 1.4865717887878418, 1.4508521556854248, 1.5583348274230957, 1.4014852046966553, 1.6530768871307373, 1.252871036529541, 1.5481984615325928, 1.4196248054504395, 1.5401315689086914, 1.5230000019073486, 1.6453158855438232, 1.3783471584320068, 1.4233229160308838, 1.4574215412139893, 1.415292501449585, 1.5346019268035889, 1.6101114749908447, 1.2649762630462646, 1.2251365184783936, 1.526181936264038, 1.694026231765747, 1.785956621170044, 1.7725911140441895, 1.6120150089263916, 1.7700703144073486, 1.7114660739898682, 1.645472526550293, 1.6172611713409424, 1.7005937099456787, 1.5171692371368408, 1.6935603618621826, 1.6268632411956787, 1.7445697784423828, 1.692833662033081, 1.718980073928833, 1.7641932964324951, 1.8210742473602295, 1.7942373752593994, 1.7361910343170166, 1.6082351207733154, 1.9532239437103271, 2.03943133354187, 1.7147796154022217, 1.6544945240020752, 2.004870653152466, 2.0873587131500244, 1.673311471939087, 1.908803939819336, 2.023897886276245, 1.7510216236114502, 1.9548351764678955, 1.9648640155792236, 1.9541127681732178, 1.9862873554229736, 1.8287436962127686, 1.8101146221160889, 1.9389965534210205, 1.8551130294799805, 1.818976879119873, 1.9225354194641113, 1.9245414733886719, 2.1271231174468994, 2.0782358646392822, 1.8803205490112305, 1.9635024070739746, 2.0118257999420166, 2.0222790241241455, 2.000471830368042, 1.9615490436553955, 1.977895975112915, 1.883331298828125, 1.9987835884094238, 2.0627191066741943, 1.9757564067840576, 1.9947819709777832, 2.027742862701416, 2.0381667613983154, 1.992464303970337, 1.9624276161193848, 1.9387366771697998]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def find_universally_active_features(features: torch.Tensor, activation_threshold: float = 0.01) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Find features that are active (above threshold) for every single prompt.\n",
    "    \n",
    "    Args:\n",
    "        features: Feature activations tensor of shape [num_prompts, num_features]\n",
    "        activation_threshold: Minimum activation value to consider a feature \"active\"\n",
    "    \n",
    "    Returns:\n",
    "        universal_features: Indices of features that are active for all prompts\n",
    "        universal_activations: Mean activation values for universal features\n",
    "    \"\"\"\n",
    "    # Check which features are active (above threshold) for each prompt\n",
    "    active_features = features > activation_threshold  # [num_prompts, num_features]\n",
    "    \n",
    "    # Find features that are active for ALL prompts\n",
    "    universal_mask = torch.all(active_features, dim=0)  # [num_features]\n",
    "    universal_features = torch.where(universal_mask)[0]  # Indices of universal features\n",
    "    \n",
    "    # Get mean activation values for universal features\n",
    "    universal_activations = features[:, universal_features].mean(dim=0)\n",
    "    \n",
    "    return universal_features, universal_activations\n",
    "\n",
    "# Find universally active features\n",
    "print(\"Finding features that activate for every single prompt...\")\n",
    "universal_features, universal_activations = find_universally_active_features(features)\n",
    "\n",
    "print(f\"Found {len(universal_features)} features that are active for all {features.shape[0]} prompts\")\n",
    "print(f\"Universal features (indices): {universal_features.tolist()}\")\n",
    "print(f\"Mean activation values: {universal_activations.tolist()}\")\n",
    "\n",
    "# Create a summary dataframe\n",
    "if len(universal_features) > 0:\n",
    "    universal_df = pd.DataFrame({\n",
    "        'feature_index': universal_features.tolist(),\n",
    "        'mean_activation': universal_activations.tolist()\n",
    "    })\n",
    "    \n",
    "    # Sort by mean activation (descending)\n",
    "    universal_df = universal_df.sort_values('mean_activation', ascending=False)\n",
    "    \n",
    "    print(\"\\nUniversal features summary (sorted by mean activation):\")\n",
    "    print(universal_df.to_string(index=False))\n",
    "    \n",
    "    # Show distribution of activations for top universal features\n",
    "    print(\"\\nDetailed activations for top 5 universal features:\")\n",
    "    for i, (feature_idx, mean_act) in enumerate(zip(universal_df['feature_index'].head(5), \n",
    "                                                   universal_df['mean_activation'].head(5))):\n",
    "        feature_activations = features[:, feature_idx]\n",
    "        print(f\"Feature {feature_idx}: mean={mean_act:.4f}, min={feature_activations.min():.4f}, max={feature_activations.max():.4f}\")\n",
    "        print(f\"  Activations: {feature_activations.tolist()}\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No features are active for all prompts with the current threshold.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding universal features across all prompts...\n",
      "\n",
      "Finding universal features within each label category...\n",
      "  Processing label: code\n",
      "    Found 6 universal features for 'code'\n",
      "    Excluded 2 features that are already global universal features\n",
      "  Processing label: math\n",
      "    Found 6 universal features for 'math'\n",
      "    Excluded 2 features that are already global universal features\n",
      "  Processing label: medical\n",
      "    Found 6 universal features for 'medical'\n",
      "    Excluded 2 features that are already global universal features\n",
      "  Processing label: analysis\n",
      "    Found 6 universal features for 'analysis'\n",
      "    Excluded 2 features that are already global universal features\n",
      "  Processing label: therapy\n",
      "    Found 6 universal features for 'therapy'\n",
      "    Excluded 2 features that are already global universal features\n",
      "  Processing label: creative\n",
      "    Found 6 universal features for 'creative'\n",
      "    Excluded 2 features that are already global universal features\n",
      "  Processing label: trivia\n",
      "    Found 6 universal features for 'trivia'\n",
      "    Excluded 2 features that are already global universal features\n",
      "\n",
      "Total universal features found: 30\n",
      "Summary by label:\n",
      "  analysis: 4 features\n",
      "  code: 4 features\n",
      "  creative: 4 features\n",
      "  math: 4 features\n",
      "  therapy: 4 features\n",
      "  medical: 4 features\n",
      "  trivia: 4 features\n",
      "  universal: 2 features\n",
      "\n",
      "Results saved to: ./results/general/llama_trainer1_layer15_asst.csv\n",
      "Number of universal features saved: 30\n",
      "\n",
      "Preview of saved data:\n",
      " feature_id  activation_mean  activation_max  activation_min     label chat_desc pt_desc type                                                                                           link\n",
      "     128740         3.660419        4.542189        2.514090 universal                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=128740\n",
      "      84223         1.815838        2.226566        1.225137 universal                         https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=84223\n",
      "     103593         1.680536        2.542022        0.809302  analysis                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=103593\n",
      "     110386         1.608843        2.282393        0.796679  analysis                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=110386\n",
      "     126895         1.451111        2.298111        0.591698  analysis                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=126895\n",
      "      82001         1.045847        1.468579        0.616096  analysis                         https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=82001\n",
      "     103593         1.680536        2.542022        0.809302      code                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=103593\n",
      "     110386         1.608843        2.282393        0.796679      code                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=110386\n",
      "     126895         1.451111        2.298111        0.591698      code                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=126895\n",
      "      82001         1.045847        1.468579        0.616096      code                         https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=82001\n",
      "\n",
      "Sample dashboard link: https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=128740\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "results_dir = \"./results/general\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Create filename with specified format\n",
    "csv_filename = f\"{MODEL_TYPE}_trainer1_layer{SAE_LAYER}_{TOKEN_TYPE}.csv\"\n",
    "csv_path = os.path.join(results_dir, csv_filename)\n",
    "\n",
    "# Prepare data for CSV - now including label-specific universal features\n",
    "all_results = []\n",
    "\n",
    "# 1. Find universal features across ALL prompts\n",
    "print(\"Finding universal features across all prompts...\")\n",
    "universal_features_all, universal_activations_all = find_universally_active_features(features)\n",
    "\n",
    "# Store global universal features for exclusion from per-label results\n",
    "global_universal_features = set()\n",
    "\n",
    "if len(universal_features_all) > 0:\n",
    "    global_universal_features = set(universal_features_all.tolist())\n",
    "    for feature_idx in universal_features_all:\n",
    "        feature_activations = features[:, feature_idx]\n",
    "        feature_id = feature_idx.item()\n",
    "        all_results.append({\n",
    "            'feature_id': feature_id,\n",
    "            'activation_mean': feature_activations.mean().item(),\n",
    "            'activation_max': feature_activations.max().item(),\n",
    "            'activation_min': feature_activations.min().item(),\n",
    "            'label': 'universal',\n",
    "            'chat_desc': '',\n",
    "            'pt_desc': '',\n",
    "            'type': '',\n",
    "            'link': f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer=1&fids={feature_id}\"\n",
    "        })\n",
    "elif len(universal_features_low) > 0:\n",
    "    # Use lower threshold results if original threshold found nothing\n",
    "    global_universal_features = set(universal_features_low.tolist())\n",
    "    for feature_idx in universal_features_low:\n",
    "        feature_activations = features[:, feature_idx]\n",
    "        feature_id = feature_idx.item()\n",
    "        all_results.append({\n",
    "            'feature_id': feature_id,\n",
    "            'activation_mean': feature_activations.mean().item(),\n",
    "            'activation_max': feature_activations.max().item(),\n",
    "            'activation_min': feature_activations.min().item(),\n",
    "            'label': 'universal',\n",
    "            'chat_desc': '',\n",
    "            'pt_desc': '',\n",
    "            'type': '',\n",
    "            'link': f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer=1&fids={feature_id}\"\n",
    "        })\n",
    "    print(\"Note: Using results from lower threshold (0.001) for universal features\")\n",
    "\n",
    "# 2. Find universal features within each label category\n",
    "print(\"\\nFinding universal features within each label category...\")\n",
    "unique_labels = prompts_df['label'].unique()\n",
    "\n",
    "for label in unique_labels:\n",
    "    print(f\"  Processing label: {label}\")\n",
    "    # Get indices for this label\n",
    "    label_indices = prompts_df[prompts_df['label'] == label].index.tolist()\n",
    "    \n",
    "    if len(label_indices) > 1:  # Only analyze if there are multiple prompts\n",
    "        # Get features for this label\n",
    "        label_features = features[label_indices]\n",
    "        \n",
    "        # Find universal features within this label\n",
    "        label_universal_features, label_universal_activations = find_universally_active_features(label_features)\n",
    "        \n",
    "        if len(label_universal_features) == 0:\n",
    "            # Try with lower threshold\n",
    "            label_universal_features, label_universal_activations = find_universally_active_features(label_features, activation_threshold=0.001)\n",
    "            if len(label_universal_features) > 0:\n",
    "                print(f\"    Found {len(label_universal_features)} universal features for '{label}' (threshold 0.001)\")\n",
    "            else:\n",
    "                print(f\"    No universal features found for '{label}' even with lower threshold\")\n",
    "        else:\n",
    "            print(f\"    Found {len(label_universal_features)} universal features for '{label}'\")\n",
    "        \n",
    "        # Add to results, but exclude global universal features\n",
    "        label_specific_count = 0\n",
    "        for feature_idx in label_universal_features:\n",
    "            feature_id = feature_idx.item()\n",
    "            # Skip if this feature is already in global universal features\n",
    "            if feature_id not in global_universal_features:\n",
    "                feature_activations = label_features[:, feature_idx]\n",
    "                all_results.append({\n",
    "                    'feature_id': feature_id,\n",
    "                    'activation_mean': feature_activations.mean().item(),\n",
    "                    'activation_max': feature_activations.max().item(),\n",
    "                    'activation_min': feature_activations.min().item(),\n",
    "                    'label': label,\n",
    "                    'chat_desc': '',\n",
    "                    'pt_desc': '',\n",
    "                    'type': '',\n",
    "                    'link': f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer=1&fids={feature_id}\"\n",
    "                })\n",
    "                label_specific_count += 1\n",
    "        \n",
    "        if label_specific_count < len(label_universal_features):\n",
    "            excluded_count = len(label_universal_features) - label_specific_count\n",
    "            print(f\"    Excluded {excluded_count} features that are already global universal features\")\n",
    "        \n",
    "    else:\n",
    "        print(f\"    Skipping '{label}' - only {len(label_indices)} prompt(s)\")\n",
    "\n",
    "# Create DataFrame and sort with universal features on top\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    # Create a custom sort key to put 'universal' first, then alphabetical\n",
    "    results_df['sort_key'] = results_df['label'].apply(lambda x: '0' if x == 'universal' else '1' + x)\n",
    "    results_df = results_df.sort_values(['sort_key', 'activation_mean'], ascending=[True, False])\n",
    "    # Drop the temporary sort key column\n",
    "    results_df = results_df.drop('sort_key', axis=1)\n",
    "    \n",
    "    print(f\"\\nTotal universal features found: {len(results_df)}\")\n",
    "    print(\"Summary by label:\")\n",
    "    label_counts = results_df['label'].value_counts()\n",
    "    for label, count in label_counts.items():\n",
    "        print(f\"  {label}: {count} features\")\n",
    "    \n",
    "else:\n",
    "    # No universal features found at all\n",
    "    results_df = pd.DataFrame(columns=['feature_id', 'activation_mean', 'activation_max', 'activation_min', 'label', 'chat_desc', 'pt_desc', 'type', 'link'])\n",
    "    print(\"Warning: No universal features found\")\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nResults saved to: {csv_path}\")\n",
    "print(f\"Number of universal features saved: {len(results_df)}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nPreview of saved data:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Show sample link\n",
    "    if len(results_df) > 0:\n",
    "        sample_link = results_df.iloc[0]['link']\n",
    "        print(f\"\\nSample dashboard link: {sample_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
