{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Token-level activations for target features\n",
    "\n",
    "This notebook analyzes token-level activations for specific SAE features on prompts, using a two-stage approach:\n",
    "1. Screen all prompts for target feature activation\n",
    "2. Extract detailed token activations only for prompts where target features fire\n",
    "\n",
    "Saves results to `active.jsonl` and `inactive.jsonl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "from sae_lens import SAE\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Will process 17 new features: [91547, 65116, 85422, 80134, 74855, 71187, 102414, 10392, 128628, 8524, 57516, 21953, 26196, 90900, 11383, 111921, 74079]\n",
      "\n",
      "Configuration Summary:\n",
      "  Model Type: gemma\n",
      "  Model to load: google/gemma-2-9b-it\n",
      "  SAE Layer: 20, Trainer: 131k-l0-114\n",
      "  Target Features: [91547, 65116, 85422, 80134, 74855, 71187, 102414, 10392, 128628, 8524, 57516, 21953, 26196, 90900, 11383, 111921, 74079]\n",
      "  Activation Threshold: 0.0\n",
      "  Base Output Directory: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# TARGET FEATURES - Specify which features to analyze\n",
    "# =============================================================================\n",
    "TARGET_FEATURES = [91547, 65116, 85422, 80134, 74855, 71187, 102414, 10392, 128628, 8524, 57516, 21953, 26196, 90900, 11383, 111921, 74079]  # List of feature IDs to analyze\n",
    "ACTIVATION_THRESHOLD = 0.0  # Minimum activation to consider \"active\"\n",
    "\n",
    "# =============================================================================\n",
    "# DEDUPLICATION AND CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Deduplicate TARGET_FEATURES while preserving order\n",
    "original_count = len(TARGET_FEATURES)\n",
    "TARGET_FEATURES = list(dict.fromkeys(TARGET_FEATURES))  # Preserves order, removes duplicates\n",
    "duplicates_removed = original_count - len(TARGET_FEATURES)\n",
    "\n",
    "if duplicates_removed > 0:\n",
    "    print(f\"üîÑ Removed {duplicates_removed} duplicate feature(s) from TARGET_FEATURES\")\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "# Base output directory - individual feature directories will be created under this\n",
    "BASE_OUTPUT_DIR = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/{N_PROMPTS}_prompts\"\n",
    "\n",
    "# =============================================================================\n",
    "# CHECK FOR EXISTING DIRECTORIES AND FILTER TARGET FEATURES\n",
    "# =============================================================================\n",
    "original_target_features = TARGET_FEATURES.copy()\n",
    "filtered_target_features = []\n",
    "existing_features = []\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    if os.path.exists(feature_dir):\n",
    "        existing_features.append(feature_id)\n",
    "        print(f\"‚ö†Ô∏è  WARNING: Directory already exists for feature {feature_id}, skipping: {feature_dir}\")\n",
    "    else:\n",
    "        filtered_target_features.append(feature_id)\n",
    "\n",
    "# Update TARGET_FEATURES to only include features that don't have existing directories\n",
    "TARGET_FEATURES = filtered_target_features\n",
    "\n",
    "if existing_features:\n",
    "    print(f\"\\nüîÑ Skipped {len(existing_features)} existing features: {existing_features}\")\n",
    "    \n",
    "if not TARGET_FEATURES:\n",
    "    print(f\"\\n‚ùå No new features to process - all {len(original_target_features)} features already have existing directories!\")\n",
    "    print(\"To reprocess existing features, delete their directories first.\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ Will process {len(TARGET_FEATURES)} new features: {TARGET_FEATURES}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"\\nConfiguration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Target Features: {TARGET_FEATURES}\")\n",
    "print(f\"  Activation Threshold: {ACTIVATION_THRESHOLD}\")\n",
    "print(f\"  Base Output Directory: {BASE_OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts already exist at /workspace/data/lmsys-chat-1m/chat_1000.jsonl\n",
      "Loaded 1000 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_lmsys_prompts(prompts_path: str, prompts_hf: str, n_prompts: int, seed: int) -> pd.DataFrame:\n",
    "    # Check if prompts_path exists\n",
    "    if os.path.exists(prompts_path):\n",
    "        print(f\"Prompts already exist at {prompts_path}\")\n",
    "        return pd.read_json(prompts_path, lines=True)\n",
    "    else:\n",
    "        print(f\"Prompts do not exist at {prompts_path}. Loading from {prompts_hf}...\")\n",
    "        dataset = load_dataset(prompts_hf)\n",
    "        dataset = dataset['train'].shuffle(seed=seed).select(range(n_prompts))\n",
    "        df = dataset.to_pandas()\n",
    "\n",
    "        # Extract the prompt from the first conversation item\n",
    "        df['prompt'] = df['conversation'].apply(lambda x: x[0]['content'])\n",
    "\n",
    "        # Only keep some columns\n",
    "        df = df[['conversation_id', 'prompt', 'redacted', 'language']]\n",
    "\n",
    "        # Save to .jsonl file\n",
    "        df.to_json(prompts_path, orient='records', lines=True)\n",
    "        return df\n",
    "\n",
    "prompts_df = load_lmsys_prompts(PROMPTS_PATH, PROMPTS_HF, N_PROMPTS, SEED)\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: GemmaTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9990f27aa9bc4f99bea771c7609660ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\":0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAE from /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "‚úì Found SAE files at: /workspace/sae/gemma-2-9b/saes/resid_post_layer_20/trainer_131k-l0-114\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    print(f\"Loading SAE from {sae_path}\")\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"‚úì Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two-Stage Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated two-stage processing function defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def process_prompts_for_features(prompts: List[str], target_features: List[int], \n",
    "                                layer_idx: int, activation_threshold: float = 0.0) -> Dict[int, Tuple[List[Dict], List[Dict]]]:\n",
    "    \"\"\"Two-stage processing: screen for target features, then get detailed tokens for active prompts.\n",
    "    \n",
    "    Returns:\n",
    "        Dict mapping feature_id to (active_prompts, inactive_prompts) for that feature\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize results dictionary for each feature\n",
    "    results = {}\n",
    "    for feature_id in target_features:\n",
    "        results[feature_id] = ([], [])  # (active_prompts, inactive_prompts)\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing prompts\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Stage 1: Get activations for screening\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Apply SAE to get feature activations\n",
    "        batch_size, seq_len, hidden_dim = activations.shape\n",
    "        flat_activations = activations.view(-1, hidden_dim)\n",
    "        \n",
    "        # Process SAE in chunks to avoid memory issues\n",
    "        sae_features = []\n",
    "        for chunk_start in range(0, flat_activations.shape[0], BATCH_SIZE * 8):\n",
    "            chunk_end = min(chunk_start + BATCH_SIZE * 8, flat_activations.shape[0])\n",
    "            chunk_activations = flat_activations[chunk_start:chunk_end]\n",
    "            chunk_features = sae.encode(chunk_activations)\n",
    "            sae_features.append(chunk_features.cpu())\n",
    "        \n",
    "        sae_features = torch.cat(sae_features, dim=0)\n",
    "        sae_features = sae_features.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        # Stage 2: Process each prompt for each target feature separately\n",
    "        for batch_idx, (prompt, formatted_prompt) in enumerate(zip(batch_prompts, formatted_prompts)):\n",
    "            prompt_idx = i + batch_idx\n",
    "            prompt_features = sae_features[batch_idx]  # [seq_len, num_features]\n",
    "            input_ids = batch_inputs['input_ids'][batch_idx].cpu().numpy()\n",
    "            \n",
    "            # Create tokenized prompt (convert input_ids to token strings)\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "            \n",
    "            # Process each target feature separately\n",
    "            for feature_id in target_features:\n",
    "                # Get activations for this specific feature\n",
    "                feature_activations = prompt_features[:, feature_id]  # [seq_len]\n",
    "                max_activation = float(feature_activations.max())\n",
    "                \n",
    "                # Check if this feature is active for this prompt\n",
    "                is_active = max_activation > activation_threshold\n",
    "                \n",
    "                if is_active:\n",
    "                    # Stage 3: Get detailed token analysis for active prompts\n",
    "                    tokens = []\n",
    "                    for pos in range(len(input_ids)):\n",
    "                        if pos >= prompt_features.shape[0]:\n",
    "                            break\n",
    "                            \n",
    "                        token_id = int(input_ids[pos])\n",
    "                        token_text = tokenizer.decode([token_id])\n",
    "                        \n",
    "                        # Get activation for this specific feature at this position\n",
    "                        activation_val = float(prompt_features[pos, feature_id])\n",
    "                        \n",
    "                        if activation_val > 0:  # Only store non-zero activations\n",
    "                            tokens.append({\n",
    "                                'position': pos,\n",
    "                                'token_id': token_id,\n",
    "                                'text': token_text,\n",
    "                                'feature_activation': activation_val\n",
    "                            })\n",
    "                    \n",
    "                    results[feature_id][0].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation,\n",
    "                        'tokens': tokens\n",
    "                    })\n",
    "                else:\n",
    "                    # Inactive prompt - just basic info\n",
    "                    results[feature_id][1].append({\n",
    "                        'prompt_id': prompt_idx,\n",
    "                        'prompt_text': prompt,\n",
    "                        'tokenized_prompt': tokenized_prompt,\n",
    "                        'max_feature_activation': max_activation\n",
    "                    })\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Updated two-stage processing function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process All Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 1000 prompts for 17 target features...\n",
      "Target features: [91547, 65116, 85422, 80134, 74855, 71187, 102414, 10392, 128628, 8524, 57516, 21953, 26196, 90900, 11383, 111921, 74079]\n",
      "Activation threshold: 0.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f353ece41e3442e086ac1787964a71e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing prompts:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results by feature:\n",
      "  Feature 91547: 230 active, 770 inactive, 694 active tokens\n",
      "  Feature 65116: 244 active, 756 inactive, 393 active tokens\n",
      "  Feature 85422: 322 active, 678 inactive, 699 active tokens\n",
      "  Feature 80134: 592 active, 408 inactive, 2243 active tokens\n",
      "  Feature 74855: 698 active, 302 inactive, 8603 active tokens\n",
      "  Feature 71187: 1000 active, 0 inactive, 2646 active tokens\n",
      "  Feature 102414: 121 active, 879 inactive, 312 active tokens\n",
      "  Feature 10392: 186 active, 814 inactive, 672 active tokens\n",
      "  Feature 128628: 812 active, 188 inactive, 2286 active tokens\n",
      "  Feature 8524: 1000 active, 0 inactive, 3259 active tokens\n",
      "  Feature 57516: 120 active, 880 inactive, 128 active tokens\n",
      "  Feature 21953: 103 active, 897 inactive, 163 active tokens\n",
      "  Feature 26196: 71 active, 929 inactive, 123 active tokens\n",
      "  Feature 90900: 130 active, 870 inactive, 4393 active tokens\n",
      "  Feature 11383: 204 active, 796 inactive, 611 active tokens\n",
      "  Feature 111921: 110 active, 890 inactive, 215 active tokens\n",
      "  Feature 74079: 722 active, 278 inactive, 3113 active tokens\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing {len(prompts_df)} prompts for {len(TARGET_FEATURES)} target features...\")\n",
    "print(f\"Target features: {TARGET_FEATURES}\")\n",
    "print(f\"Activation threshold: {ACTIVATION_THRESHOLD}\")\n",
    "\n",
    "# Process all prompts for all features\n",
    "feature_results = process_prompts_for_features(\n",
    "    prompts_df['prompt'].tolist(), \n",
    "    TARGET_FEATURES, \n",
    "    LAYER_INDEX, \n",
    "    ACTIVATION_THRESHOLD\n",
    ")\n",
    "\n",
    "print(f\"\\nResults by feature:\")\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    total_tokens = sum(lesn(p['tokens']) for p in active_prompts)\n",
    "    print(f\"  Feature {feature_id}: {len(active_prompts)} active, {len(inactive_prompts)} inactive, {total_tokens} active tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving 230 active prompts for feature 91547 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/91547/active.jsonl\n",
      "Saving 770 inactive prompts for feature 91547 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/91547/inactive.jsonl\n",
      "  Sample active prompt for feature 91547:\n",
      "    Prompt: You are the text completion model and you must complete the assistant answer below, only send the co...\n",
      "    Max activation: 4.018561840057373\n",
      "    Active tokens: 1\n",
      "    Top token: ' model' (position 10)\n",
      "    Token activation: 4.018561840057373\n",
      "\n",
      "Saving 244 active prompts for feature 65116 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/65116/active.jsonl\n",
      "Saving 756 inactive prompts for feature 65116 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/65116/inactive.jsonl\n",
      "  Sample active prompt for feature 65116:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'Ê•ºÂÆá',\n",
      " room_number var...\n",
      "    Max activation: 3.337862968444824\n",
      "    Active tokens: 1\n",
      "    Top token: ' structure' (position 7)\n",
      "    Token activation: 3.337862968444824\n",
      "\n",
      "Saving 322 active prompts for feature 85422 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85422/active.jsonl\n",
      "Saving 678 inactive prompts for feature 85422 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85422/inactive.jsonl\n",
      "  Sample active prompt for feature 85422:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 4.689665794372559\n",
      "    Active tokens: 2\n",
      "    Top token: ' figli' (position 155)\n",
      "    Token activation: 4.689665794372559\n",
      "\n",
      "Saving 592 active prompts for feature 80134 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/80134/active.jsonl\n",
      "Saving 408 inactive prompts for feature 80134 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/80134/inactive.jsonl\n",
      "  Sample active prompt for feature 80134:\n",
      "    Prompt: –ü—Ä–∏–≤–µ—Ç, –∫–∞–∫–æ–π —Ü–≤–µ—Ç —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –≤ –æ–¥–µ–∂–¥–µ?...\n",
      "    Max activation: 7.129537582397461\n",
      "    Active tokens: 3\n",
      "    Top token: ',' (position 6)\n",
      "    Token activation: 7.129537582397461\n",
      "\n",
      "Saving 698 active prompts for feature 74855 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74855/active.jsonl\n",
      "Saving 302 inactive prompts for feature 74855 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74855/inactive.jsonl\n",
      "  Sample active prompt for feature 74855:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 3.8342490196228027\n",
      "    Active tokens: 1\n",
      "    Top token: 'Scri' (position 5)\n",
      "    Token activation: 3.8342490196228027\n",
      "\n",
      "Saving 1000 active prompts for feature 71187 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/71187/active.jsonl\n",
      "Saving 0 inactive prompts for feature 71187 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/71187/inactive.jsonl\n",
      "  Sample active prompt for feature 71187:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'Ê•ºÂÆá',\n",
      " room_number var...\n",
      "    Max activation: 19.4549560546875\n",
      "    Active tokens: 2\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 19.4549560546875\n",
      "\n",
      "Saving 121 active prompts for feature 102414 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/102414/active.jsonl\n",
      "Saving 879 inactive prompts for feature 102414 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/102414/inactive.jsonl\n",
      "  Sample active prompt for feature 102414:\n",
      "    Prompt: gmake[2]: *** No rule to make target '/usr/lib/x86_64-linux-gnu/librt.so', needed by 'lib/libtorch_h...\n",
      "    Max activation: 4.4879913330078125\n",
      "    Active tokens: 1\n",
      "    Top token: '/' (position 233)\n",
      "    Token activation: 4.4879913330078125\n",
      "\n",
      "Saving 186 active prompts for feature 10392 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/10392/active.jsonl\n",
      "Saving 814 inactive prompts for feature 10392 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/10392/inactive.jsonl\n",
      "  Sample active prompt for feature 10392:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 3.834158420562744\n",
      "    Active tokens: 1\n",
      "    Top token: 'pare' (position 159)\n",
      "    Token activation: 3.834158420562744\n",
      "\n",
      "Saving 812 active prompts for feature 128628 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128628/active.jsonl\n",
      "Saving 188 inactive prompts for feature 128628 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128628/inactive.jsonl\n",
      "  Sample active prompt for feature 128628:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'Ê•ºÂÆá',\n",
      " room_number var...\n",
      "    Max activation: 11.342621803283691\n",
      "    Active tokens: 5\n",
      "    Top token: ' sql' (position 90)\n",
      "    Token activation: 11.342621803283691\n",
      "\n",
      "Saving 1000 active prompts for feature 8524 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/8524/active.jsonl\n",
      "Saving 0 inactive prompts for feature 8524 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/8524/inactive.jsonl\n",
      "  Sample active prompt for feature 8524:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'Ê•ºÂÆá',\n",
      " room_number var...\n",
      "    Max activation: 52.383853912353516\n",
      "    Active tokens: 2\n",
      "    Top token: '<bos>' (position 0)\n",
      "    Token activation: 52.383853912353516\n",
      "\n",
      "Saving 120 active prompts for feature 57516 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/57516/active.jsonl\n",
      "Saving 880 inactive prompts for feature 57516 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/57516/inactive.jsonl\n",
      "  Sample active prompt for feature 57516:\n",
      "    Prompt: the table structure is: create table classroom\n",
      " (building varchar(15) comment 'Ê•ºÂÆá',\n",
      " room_number var...\n",
      "    Max activation: 3.726534843444824\n",
      "    Active tokens: 1\n",
      "    Top token: ' room' (position 27)\n",
      "    Token activation: 3.726534843444824\n",
      "\n",
      "Saving 103 active prompts for feature 21953 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/21953/active.jsonl\n",
      "Saving 897 inactive prompts for feature 21953 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/21953/inactive.jsonl\n",
      "  Sample active prompt for feature 21953:\n",
      "    Prompt: Write an article about the Applications of 1-AMINO PIPERAZINE 2000 words in chemical industry...\n",
      "    Max activation: 3.684788465499878\n",
      "    Active tokens: 1\n",
      "    Top token: '<start_of_turn>' (position 31)\n",
      "    Token activation: 3.684788465499878\n",
      "\n",
      "Saving 71 active prompts for feature 26196 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/26196/active.jsonl\n",
      "Saving 929 inactive prompts for feature 26196 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/26196/inactive.jsonl\n",
      "  Sample active prompt for feature 26196:\n",
      "    Prompt: In the 1980s there were these cool text adventure games on computers. They were called ‚Äúinteractive ...\n",
      "    Max activation: 4.016899585723877\n",
      "    Active tokens: 1\n",
      "    Top token: '‚Äô' (position 268)\n",
      "    Token activation: 4.016899585723877\n",
      "\n",
      "Saving 130 active prompts for feature 90900 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/90900/active.jsonl\n",
      "Saving 870 inactive prompts for feature 90900 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/90900/inactive.jsonl\n",
      "  Sample active prompt for feature 90900:\n",
      "    Prompt: You are the text completion model and you must complete the assistant answer below, only send the co...\n",
      "    Max activation: 3.912771701812744\n",
      "    Active tokens: 2\n",
      "    Top token: '<pad>' (position 90)\n",
      "    Token activation: 3.912771701812744\n",
      "\n",
      "Saving 204 active prompts for feature 11383 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11383/active.jsonl\n",
      "Saving 796 inactive prompts for feature 11383 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11383/inactive.jsonl\n",
      "  Sample active prompt for feature 11383:\n",
      "    Prompt: hello...\n",
      "    Max activation: 4.509768486022949\n",
      "    Active tokens: 1\n",
      "    Top token: '\n",
      "' (position 10)\n",
      "    Token activation: 4.509768486022949\n",
      "\n",
      "Saving 110 active prompts for feature 111921 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/111921/active.jsonl\n",
      "Saving 890 inactive prompts for feature 111921 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/111921/inactive.jsonl\n",
      "  Sample active prompt for feature 111921:\n",
      "    Prompt: Scrivi un testo, su questo tema: Nel mondo digitale in cui viviamo, spesso tendiamo a silenziare chi...\n",
      "    Max activation: 3.496429204940796\n",
      "    Active tokens: 1\n",
      "    Top token: '<pad>' (position 381)\n",
      "    Token activation: 3.496429204940796\n",
      "\n",
      "Saving 722 active prompts for feature 74079 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74079/active.jsonl\n",
      "Saving 278 inactive prompts for feature 74079 to ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74079/inactive.jsonl\n",
      "  Sample active prompt for feature 74079:\n",
      "    Prompt: –ü—Ä–∏–≤–µ—Ç, –∫–∞–∫–æ–π —Ü–≤–µ—Ç —Å–∞–º—ã–π –ø–æ–ø—É–ª—è—Ä–Ω—ã–π –≤ –æ–¥–µ–∂–¥–µ?...\n",
      "    Max activation: 23.084192276000977\n",
      "    Active tokens: 5\n",
      "    Top token: 'model' (position 19)\n",
      "    Token activation: 23.084192276000977\n",
      "\n",
      "‚úì Results saved successfully for all 17 features!\n"
     ]
    }
   ],
   "source": [
    "# Save results for each feature in separate directories\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    \n",
    "    # Create feature-specific directory\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    os.makedirs(feature_dir, exist_ok=True)\n",
    "    \n",
    "    # Define file paths\n",
    "    active_file = f\"{feature_dir}/active.jsonl\"\n",
    "    inactive_file = f\"{feature_dir}/inactive.jsonl\"\n",
    "    \n",
    "    # Save active prompts\n",
    "    print(f\"Saving {len(active_prompts)} active prompts for feature {feature_id} to {active_file}\")\n",
    "    with open(active_file, 'w') as f:\n",
    "        for prompt in active_prompts:\n",
    "            f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "    # Save inactive prompts\n",
    "    print(f\"Saving {len(inactive_prompts)} inactive prompts for feature {feature_id} to {inactive_file}\")\n",
    "    with open(inactive_file, 'w') as f:\n",
    "        for prompt in inactive_prompts:\n",
    "            f.write(json.dumps(prompt) + '\\n')\n",
    "    \n",
    "    # Show sample results for this feature\n",
    "    if active_prompts:\n",
    "        print(f\"  Sample active prompt for feature {feature_id}:\")\n",
    "        sample = active_prompts[0]\n",
    "        print(f\"    Prompt: {sample['prompt_text'][:100]}...\")\n",
    "        print(f\"    Max activation: {sample['max_feature_activation']}\")\n",
    "        print(f\"    Active tokens: {len(sample['tokens'])}\")\n",
    "        if sample['tokens']:\n",
    "            top_token = max(sample['tokens'], key=lambda x: x['feature_activation'])\n",
    "            print(f\"    Top token: '{top_token['text']}' (position {top_token['position']})\")\n",
    "            print(f\"    Token activation: {top_token['feature_activation']}\")\n",
    "        print()\n",
    "\n",
    "print(f\"‚úì Results saved successfully for all {len(TARGET_FEATURES)} features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated token-specific filtering functions defined\n"
     ]
    }
   ],
   "source": [
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens, device=input_ids.device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return int(assistant_pos)\n",
    "\n",
    "@torch.no_grad()\n",
    "def filter_by_token_position(active_prompts: List[Dict], inactive_prompts: List[Dict], \n",
    "                           token_type: str, token_offset: int, feature_id: int,\n",
    "                           activation_threshold: float = 0.0) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Filter prompts based on whether target feature is active at specific token positions.\"\"\"\n",
    "    \n",
    "    token_active = []\n",
    "    token_inactive = []\n",
    "    \n",
    "    # Process all prompts (both active and inactive from main analysis)\n",
    "    all_prompts = active_prompts + inactive_prompts\n",
    "    \n",
    "    for prompt_data in all_prompts:\n",
    "        prompt_text = prompt_data['prompt_text']\n",
    "        \n",
    "        # Use existing tokenized_prompt if available, otherwise tokenize\n",
    "        if 'tokenized_prompt' in prompt_data:\n",
    "            tokenized_prompt = prompt_data['tokenized_prompt']\n",
    "        else:\n",
    "            # Format as chat message to match processing\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize to get input_ids and attention_mask\n",
    "            inputs = tokenizer(\n",
    "                formatted_prompt,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=MAX_LENGTH\n",
    "            )\n",
    "            \n",
    "            input_ids = inputs['input_ids'].squeeze(0)\n",
    "            \n",
    "            # Create tokenized prompt\n",
    "            tokenized_prompt = []\n",
    "            for token_id in input_ids:\n",
    "                if token_id != tokenizer.pad_token_id:  # Skip padding tokens\n",
    "                    token_text = tokenizer.decode([int(token_id)])\n",
    "                    tokenized_prompt.append(token_text)\n",
    "        \n",
    "        # For finding token position, we still need input_ids and attention_mask\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt_text}]\n",
    "        formatted_prompt = tokenizer.apply_chat_template(\n",
    "            messages, tokenize=False, add_generation_prompt=True\n",
    "        )\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            formatted_prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        input_ids = inputs['input_ids'].squeeze(0)\n",
    "        attention_mask = inputs['attention_mask'].squeeze(0)\n",
    "        \n",
    "        # Find the specific token position\n",
    "        target_position = find_assistant_position(\n",
    "            input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "        )\n",
    "        \n",
    "        # Check if this prompt has token data at the target position\n",
    "        has_activation_at_position = False\n",
    "        max_activation_at_position = 0.0\n",
    "        \n",
    "        # Check if this was an active prompt with token details\n",
    "        if 'tokens' in prompt_data:\n",
    "            for token_data in prompt_data['tokens']:\n",
    "                if token_data['position'] == target_position:\n",
    "                    activation = token_data['feature_activation']\n",
    "                    if activation > activation_threshold:\n",
    "                        has_activation_at_position = True\n",
    "                        max_activation_at_position = max(max_activation_at_position, activation)\n",
    "        \n",
    "        # Create token-specific record\n",
    "        token_record = {\n",
    "            'prompt_id': prompt_data['prompt_id'],\n",
    "            'prompt_text': prompt_data['prompt_text'],\n",
    "            'tokenized_prompt': tokenized_prompt,\n",
    "            'token_type': token_type,\n",
    "            'token_position': target_position,\n",
    "            'max_feature_activation': prompt_data['max_feature_activation']\n",
    "        }\n",
    "        \n",
    "        if has_activation_at_position:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            # Include token details for this specific position\n",
    "            position_tokens = []\n",
    "            if 'tokens' in prompt_data:\n",
    "                for token_data in prompt_data['tokens']:\n",
    "                    if token_data['position'] == target_position:\n",
    "                        position_tokens.append(token_data)\n",
    "            token_record['position_tokens'] = position_tokens\n",
    "            token_active.append(token_record)\n",
    "        else:\n",
    "            token_record['max_activation_at_position'] = max_activation_at_position\n",
    "            token_inactive.append(token_record)\n",
    "    \n",
    "    return token_active, token_inactive\n",
    "\n",
    "print(\"Updated token-specific filtering functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing token-specific analysis for 17 features and 2 token types...\n",
      "\n",
      "Processing feature 91547:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 55\n",
      "    Inactive prompts at model position: 945\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/91547/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/91547/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 16\n",
      "      Max activation: 3.761850357055664\n",
      "      Token text: 'model'\n",
      "      Feature activation: 3.761850357055664\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 8\n",
      "    Inactive prompts at newline position: 992\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/91547/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/91547/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 14\n",
      "      Max activation: 4.2236456871032715\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 4.2236456871032715\n",
      "\n",
      "Processing feature 65116:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 111\n",
      "    Inactive prompts at model position: 889\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/65116/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/65116/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 9\n",
      "      Max activation: 4.498566627502441\n",
      "      Token text: 'model'\n",
      "      Feature activation: 4.498566627502441\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/65116/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/65116/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 85422:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 115\n",
      "    Inactive prompts at model position: 885\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85422/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85422/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 9\n",
      "      Max activation: 5.256107330322266\n",
      "      Token text: 'model'\n",
      "      Feature activation: 5.256107330322266\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85422/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/85422/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 80134:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 370\n",
      "    Inactive prompts at model position: 630\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/80134/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/80134/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 19\n",
      "      Max activation: 6.337137222290039\n",
      "      Token text: 'model'\n",
      "      Feature activation: 6.337137222290039\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 37\n",
      "    Inactive prompts at newline position: 963\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/80134/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/80134/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 14\n",
      "      Max activation: 9.77233600616455\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 9.77233600616455\n",
      "\n",
      "Processing feature 74855:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 511\n",
      "    Inactive prompts at model position: 489\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74855/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74855/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 19\n",
      "      Max activation: 10.585107803344727\n",
      "      Token text: 'model'\n",
      "      Feature activation: 10.585107803344727\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 43\n",
      "    Inactive prompts at newline position: 957\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74855/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74855/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 17\n",
      "      Max activation: 5.0658650398254395\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 5.0658650398254395\n",
      "\n",
      "Processing feature 71187:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 108\n",
      "    Inactive prompts at model position: 892\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/71187/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/71187/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 9\n",
      "      Max activation: 4.888202667236328\n",
      "      Token text: 'model'\n",
      "      Feature activation: 4.888202667236328\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/71187/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/71187/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 102414:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 19\n",
      "    Inactive prompts at model position: 981\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/102414/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/102414/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 15\n",
      "      Max activation: 7.803715705871582\n",
      "      Token text: 'model'\n",
      "      Feature activation: 7.803715705871582\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 2\n",
      "    Inactive prompts at newline position: 998\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/102414/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/102414/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 511\n",
      "      Max activation: 5.955652236938477\n",
      "      Token text: '-'\n",
      "      Feature activation: 5.955652236938477\n",
      "\n",
      "Processing feature 10392:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 39\n",
      "    Inactive prompts at model position: 961\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/10392/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/10392/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 13\n",
      "      Max activation: 5.1352081298828125\n",
      "      Token text: 'model'\n",
      "      Feature activation: 5.1352081298828125\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/10392/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/10392/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 128628:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 113\n",
      "    Inactive prompts at model position: 887\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128628/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128628/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 9\n",
      "      Max activation: 5.341365337371826\n",
      "      Token text: 'model'\n",
      "      Feature activation: 5.341365337371826\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 2\n",
      "    Inactive prompts at newline position: 998\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128628/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/128628/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 511\n",
      "      Max activation: 4.546297073364258\n",
      "      Token text: 'trial'\n",
      "      Feature activation: 4.546297073364258\n",
      "\n",
      "Processing feature 8524:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 124\n",
      "    Inactive prompts at model position: 876\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/8524/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/8524/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 9\n",
      "      Max activation: 9.414838790893555\n",
      "      Token text: 'model'\n",
      "      Feature activation: 9.414838790893555\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 143\n",
      "    Inactive prompts at newline position: 857\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/8524/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/8524/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 10\n",
      "      Max activation: 5.624442100524902\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 5.624442100524902\n",
      "\n",
      "Processing feature 57516:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 100\n",
      "    Inactive prompts at model position: 900\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/57516/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/57516/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 9\n",
      "      Max activation: 4.051962375640869\n",
      "      Token text: 'model'\n",
      "      Feature activation: 4.051962375640869\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/57516/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/57516/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 21953:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 8\n",
      "    Inactive prompts at model position: 992\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/21953/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/21953/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 16\n",
      "      Max activation: 3.4315264225006104\n",
      "      Token text: 'model'\n",
      "      Feature activation: 3.4315264225006104\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/21953/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/21953/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 26196:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 6\n",
      "    Inactive prompts at model position: 994\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/26196/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/26196/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 13\n",
      "      Max activation: 5.504661560058594\n",
      "      Token text: 'model'\n",
      "      Feature activation: 5.504661560058594\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 0\n",
      "    Inactive prompts at newline position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/26196/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/26196/inactive_newline.jsonl\n",
      "\n",
      "Processing feature 90900:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 3\n",
      "    Inactive prompts at model position: 997\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/90900/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/90900/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 21\n",
      "      Max activation: 3.7622151374816895\n",
      "      Token text: 'model'\n",
      "      Feature activation: 3.7622151374816895\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 25\n",
      "    Inactive prompts at newline position: 975\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/90900/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/90900/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 14\n",
      "      Max activation: 5.997395992279053\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 5.997395992279053\n",
      "\n",
      "Processing feature 11383:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 9\n",
      "    Inactive prompts at model position: 991\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11383/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11383/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 13\n",
      "      Max activation: 4.202493667602539\n",
      "      Token text: 'model'\n",
      "      Feature activation: 4.202493667602539\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 105\n",
      "    Inactive prompts at newline position: 895\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11383/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/11383/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 10\n",
      "      Max activation: 4.509768486022949\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 4.509768486022949\n",
      "\n",
      "Processing feature 111921:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 0\n",
      "    Inactive prompts at model position: 1000\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/111921/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/111921/inactive_model.jsonl\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 9\n",
      "    Inactive prompts at newline position: 991\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/111921/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/111921/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 14\n",
      "      Max activation: 3.8562982082366943\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 3.8562982082366943\n",
      "\n",
      "Processing feature 74079:\n",
      "  Processing token type: model (offset: -1)\n",
      "    Active prompts at model position: 538\n",
      "    Inactive prompts at model position: 462\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74079/active_model.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74079/inactive_model.jsonl\n",
      "    Sample active prompt at model position:\n",
      "      Position: 19\n",
      "      Max activation: 23.084192276000977\n",
      "      Token text: 'model'\n",
      "      Feature activation: 23.084192276000977\n",
      "  Processing token type: newline (offset: 0)\n",
      "    Active prompts at newline position: 88\n",
      "    Inactive prompts at newline position: 912\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74079/active_newline.jsonl\n",
      "    Saved: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts/74079/inactive_newline.jsonl\n",
      "    Sample active prompt at newline position:\n",
      "      Position: 10\n",
      "      Max activation: 4.837593078613281\n",
      "      Token text: '\n",
      "'\n",
      "      Feature activation: 4.837593078613281\n",
      "\n",
      "‚úì Token-specific analysis complete for all features!\n",
      "Results saved in feature-specific directories under: ./results/6_active_prompts/gemma_trainer131k-l0-114_layer20/1000_prompts\n",
      "Each feature directory contains:\n",
      "  - active.jsonl / inactive.jsonl (general)\n",
      "  - active_model.jsonl / inactive_model.jsonl (position-specific)\n",
      "  - active_newline.jsonl / inactive_newline.jsonl (position-specific)\n"
     ]
    }
   ],
   "source": [
    "# Process token-specific analysis for each feature and token type\n",
    "print(f\"Processing token-specific analysis for {len(TARGET_FEATURES)} features and {len(TOKEN_OFFSETS)} token types...\")\n",
    "\n",
    "for feature_id in TARGET_FEATURES:\n",
    "    active_prompts, inactive_prompts = feature_results[feature_id]\n",
    "    feature_dir = f\"{BASE_OUTPUT_DIR}/{feature_id}\"\n",
    "    \n",
    "    print(f\"\\nProcessing feature {feature_id}:\")\n",
    "    \n",
    "    for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "        print(f\"  Processing token type: {token_type} (offset: {token_offset})\")\n",
    "        \n",
    "        # Filter prompts based on activation at this specific token position\n",
    "        token_active, token_inactive = filter_by_token_position(\n",
    "            active_prompts, inactive_prompts, \n",
    "            token_type, token_offset, feature_id, ACTIVATION_THRESHOLD\n",
    "        )\n",
    "        \n",
    "        # Save results for this token type\n",
    "        active_file = f\"{feature_dir}/active_{token_type}.jsonl\"\n",
    "        inactive_file = f\"{feature_dir}/inactive_{token_type}.jsonl\"\n",
    "        \n",
    "        print(f\"    Active prompts at {token_type} position: {len(token_active)}\")\n",
    "        print(f\"    Inactive prompts at {token_type} position: {len(token_inactive)}\")\n",
    "        \n",
    "        # Save active prompts for this token type\n",
    "        with open(active_file, 'w') as f:\n",
    "            for prompt in token_active:\n",
    "                f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "        # Save inactive prompts for this token type\n",
    "        with open(inactive_file, 'w') as f:\n",
    "            for prompt in token_inactive:\n",
    "                f.write(json.dumps(prompt) + '\\n')\n",
    "        \n",
    "        print(f\"    Saved: {active_file}\")\n",
    "        print(f\"    Saved: {inactive_file}\")\n",
    "        \n",
    "        # Show sample if available\n",
    "        if token_active:\n",
    "            sample = token_active[0]\n",
    "            print(f\"    Sample active prompt at {token_type} position:\")\n",
    "            print(f\"      Position: {sample['token_position']}\")\n",
    "            print(f\"      Max activation: {sample['max_activation_at_position']}\")\n",
    "            if 'position_tokens' in sample and sample['position_tokens']:\n",
    "                token_data = sample['position_tokens'][0]\n",
    "                print(f\"      Token text: '{token_data['text']}'\")\n",
    "                print(f\"      Feature activation: {token_data['feature_activation']}\")\n",
    "\n",
    "print(f\"\\n‚úì Token-specific analysis complete for all features!\")\n",
    "print(f\"Results saved in feature-specific directories under: {BASE_OUTPUT_DIR}\")\n",
    "print(f\"Each feature directory contains:\")\n",
    "print(f\"  - active.jsonl / inactive.jsonl (general)\")\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"  - active_{token_type}.jsonl / inactive_{token_type}.jsonl (position-specific)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
