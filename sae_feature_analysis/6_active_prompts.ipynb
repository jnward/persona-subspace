{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Get all active prompts for a feature\n",
    "\n",
    "This notebook analyzes which given SAE features are activated on given prompts and generates both CSV and JSONL outputs in a single pass, optimized for performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "from sae_lens import SAE\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Dict, Tuple, Optional\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Configuration for model-specific settings\"\"\"\n",
    "    base_model_name: str\n",
    "    chat_model_name: str\n",
    "    hf_release: str  # Reference only - actual loading uses saelens_release/sae_id\n",
    "    assistant_header: str\n",
    "    token_offsets: Dict[str, int]\n",
    "    sae_base_path: str\n",
    "    saelens_release: str  # Template for sae_lens release parameter\n",
    "    sae_id_template: str  # Template for sae_lens sae_id parameter\n",
    "    base_url: str  # Base URL for neuronpedia\n",
    "    \n",
    "    def get_sae_params(self, sae_layer: int, sae_trainer: str) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Generate SAE lens release and sae_id parameters.\n",
    "        \n",
    "        Args:\n",
    "            sae_layer: Layer number for the SAE\n",
    "            sae_trainer: Trainer identifier for the SAE\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (release, sae_id) for sae_lens.SAE.from_pretrained()\n",
    "        \"\"\"\n",
    "        if self.saelens_release == \"llama_scope_lxr_{trainer}\":\n",
    "            release = self.saelens_release.format(trainer=sae_trainer)\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, trainer=sae_trainer)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "            l0_value = parts[2]  # \"34\"\n",
    "            \n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width, l0=l0_value)\n",
    "        elif self.saelens_release == \"gemma-scope-9b-pt-res-canonical\":\n",
    "            # Parse SAE_TRAINER \"131k-l0-34\" into components for Gemma\n",
    "            parts = sae_trainer.split(\"-\")\n",
    "            width = parts[0]  # \"131k\"\n",
    "\n",
    "            release = self.saelens_release\n",
    "            sae_id = self.sae_id_template.format(layer=sae_layer, width=width)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown SAE lens release template: {self.saelens_release}\")\n",
    "        \n",
    "        return release, sae_id\n",
    "\n",
    "# Model configurations\n",
    "MODEL_CONFIGS = {\n",
    "    \"llama\": ModelConfig(\n",
    "        base_model_name=\"meta-llama/Llama-3.1-8B\",\n",
    "        chat_model_name=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "        hf_release=\"fnlp/Llama3_1-8B-Base-LXR-32x\",\n",
    "        assistant_header=\"<|start_header_id|>assistant<|end_header_id|>\",\n",
    "        token_offsets={\"asst\": -2, \"endheader\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/llama-3.1-8b/saes\",\n",
    "        saelens_release=\"llama_scope_lxr_{trainer}\",\n",
    "        sae_id_template=\"l{layer}r_{trainer}\",\n",
    "        base_url=\"https://www.neuronpedia.org/llama-3.1-8b/{layer}-llamascope-res-131k\"\n",
    "    ),\n",
    "    \"gemma\": ModelConfig(\n",
    "        base_model_name=\"google/gemma-2-9b\",\n",
    "        chat_model_name=\"google/gemma-2-9b-it\",\n",
    "        hf_release=\"google/gemma-scope-9b-pt-res/layer_{layer}/width_{width}/average_l0_{l0}\",\n",
    "        assistant_header=\"<start_of_turn>model\",\n",
    "        token_offsets={\"model\": -1, \"newline\": 0},\n",
    "        sae_base_path=\"/workspace/sae/gemma-2-9b/saes\",\n",
    "        saelens_release=\"gemma-scope-9b-pt-res-canonical\",\n",
    "        sae_id_template=\"layer_{layer}/width_{width}/canonical\",\n",
    "        base_url=\"https://www.neuronpedia.org/gemma-2-9b/{layer}-gemmascope-res-131k\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"gemma\"  # Options: \"gemma\" or \"llama\"\n",
    "MODEL_VER = \"chat\"\n",
    "SAE_LAYER = 20\n",
    "SAE_TRAINER = \"131k-l0-114\"\n",
    "N_PROMPTS = 1000\n",
    "\n",
    "# =============================================================================\n",
    "# CONFIGURATION SETUP\n",
    "# =============================================================================\n",
    "if MODEL_TYPE not in MODEL_CONFIGS:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Available: {list(MODEL_CONFIGS.keys())}\")\n",
    "\n",
    "config = MODEL_CONFIGS[MODEL_TYPE]\n",
    "\n",
    "# Set model name based on version\n",
    "if MODEL_VER == \"chat\":\n",
    "    MODEL_NAME = config.chat_model_name\n",
    "elif MODEL_VER == \"base\":\n",
    "    MODEL_NAME = config.base_model_name\n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "# Always use chat model for tokenizer (has chat template)\n",
    "CHAT_MODEL_NAME = config.chat_model_name\n",
    "\n",
    "# Set up derived configurations\n",
    "ASSISTANT_HEADER = config.assistant_header\n",
    "TOKEN_OFFSETS = config.token_offsets\n",
    "SAE_BASE_PATH = config.sae_base_path\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_DIR = f\"./results/6_active_prompts/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/{N_PROMPTS}_prompts\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_DIR), exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "# PROMPTS_PATH = \"./prompts/personal_40/personal.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 32\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model Type: {MODEL_TYPE}\")\n",
    "print(f\"  Model to load: {MODEL_NAME}\")\n",
    "print(f\"  Tokenizer (chat): {CHAT_MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Available token types: {list(TOKEN_OFFSETS.keys())}\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output dir: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_lmsys_prompts(prompts_path: str, prompts_hf: str, n_prompts: int, seed: int) -> pd.DataFrame:\n",
    "    # Check if prompts_path exists\n",
    "    if os.path.exists(prompts_path):\n",
    "        print(f\"Prompts already exist at {prompts_path}\")\n",
    "        return pd.read_json(prompts_path, lines=True)\n",
    "    else:\n",
    "        print(f\"Prompts do not exist at {prompts_path}. Loading from {prompts_hf}...\")\n",
    "        dataset = load_dataset(prompts_hf)\n",
    "        dataset = dataset['train'].shuffle(seed=seed).select(range(n_prompts))\n",
    "        df = dataset.to_pandas()\n",
    "\n",
    "        # Extract the prompt from the first conversation item\n",
    "        df['prompt'] = df['conversation'].apply(lambda x: x[0]['content'])\n",
    "\n",
    "        # Only keep some columns\n",
    "        df = df[['conversation_id', 'prompt', 'redacted', 'language']]\n",
    "\n",
    "        # Save to .jsonl file\n",
    "        df.to_json(prompts_path, orient='records', lines=True)\n",
    "        return df\n",
    "\n",
    "def load_prompts_from_jsonl(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts from a JSONL file. Expects each line to have a 'content' field.\"\"\"\n",
    "    prompts = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "    return pd.DataFrame(prompts, columns=['prompt'])\n",
    "\n",
    "prompts_df = load_lmsys_prompts(PROMPTS_PATH, PROMPTS_HF, N_PROMPTS, SEED)\n",
    "# prompts_df = load_prompts_from_jsonl(PROMPTS_PATH)\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")\n",
    "print(f\"Prompt keys: {prompts_df.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: PreTrainedTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda831ee2e554503add6d1982e94bc72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: LlamaForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": 0}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found SAE files at: /workspace/sae/llama-3.1-8b-instruct/saes/resid_post_layer_15/trainer_1\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "def load_sae(config: ModelConfig, sae_path: str, sae_layer: int, sae_trainer: str) -> SAE:\n",
    "    \"\"\"\n",
    "    Unified SAE loading function that handles both Llama and Gemma models.\n",
    "    \n",
    "    Args:\n",
    "        config: ModelConfig object containing model-specific settings\n",
    "        sae_path: Local path to store/load SAE files\n",
    "        sae_layer: Layer number for the SAE\n",
    "        sae_trainer: Trainer identifier for the SAE\n",
    "    \n",
    "    Returns:\n",
    "        SAE: Loaded SAE model\n",
    "    \"\"\"\n",
    "    # Check if SAE file exists locally\n",
    "    ae_file_path = os.path.join(sae_path, \"sae_weights.safetensors\")\n",
    "    \n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(sae_path)\n",
    "        return sae\n",
    "    \n",
    "    print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "    os.makedirs(os.path.dirname(sae_path), exist_ok=True)\n",
    "    \n",
    "    # Get SAE parameters from config\n",
    "    release, sae_id = config.get_sae_params(sae_layer, sae_trainer)\n",
    "    print(f\"Loading SAE with release='{release}', sae_id='{sae_id}'\")\n",
    "    \n",
    "    # Load the SAE using sae_lens\n",
    "    sae, _, _ = SAE.from_pretrained(\n",
    "        release=release,\n",
    "        sae_id=sae_id,\n",
    "        device=\"cuda\" # Hardcoded because it wants a string\n",
    "    )\n",
    "    \n",
    "    # Save the SAE locally for future use\n",
    "    sae.save_model(sae_path)\n",
    "    return sae\n",
    "\n",
    "# Load SAE using the unified function\n",
    "sae = load_sae(config, SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations_and_metadata(prompts: List[str], layer_idx: int) -> Tuple[torch.Tensor, List[Dict], List[str]]:\n",
    "    \"\"\"Extract activations and prepare metadata for all prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    all_metadata = []\n",
    "    formatted_prompts_list = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        formatted_prompts_list.extend(formatted_prompts)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "    \n",
    "    # Find the maximum sequence length across all activations\n",
    "    max_seq_len = max(act.shape[0] for act in all_activations)\n",
    "    hidden_dim = all_activations[0].shape[1]\n",
    "    \n",
    "    # Pad all activations to the same length\n",
    "    padded_activations = []\n",
    "    for act in all_activations:\n",
    "        if act.shape[0] < max_seq_len:\n",
    "            padding = torch.zeros(max_seq_len - act.shape[0], hidden_dim)\n",
    "            padded_act = torch.cat([act, padding], dim=0)\n",
    "        else:\n",
    "            padded_act = act\n",
    "        padded_activations.append(padded_act)\n",
    "    \n",
    "    return torch.stack(padded_activations, dim=0), all_metadata, formatted_prompts_list\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_activations(full_activations: torch.Tensor, metadata: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extract activations for specific token positions from full sequence activations.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "\n",
    "    # Extract activations for each token type\n",
    "    for i, meta in enumerate(metadata):\n",
    "        for token_type, position in meta['positions'].items():\n",
    "            # Extract activation at the specific position\n",
    "            activation = full_activations[i, position, :]  # [hidden_dim]\n",
    "            results[token_type].append(activation)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = torch.stack(results[token_type], dim=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for all positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee61262d03e47dd9115f97b50d759a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full activations shape: torch.Size([140, 160, 4096])\n",
      "\n",
      "Extracting activations for specific token positions...\n",
      "Token type 'asst' activations shape: torch.Size([140, 4096])\n",
      "Token type 'endheader' activations shape: torch.Size([140, 4096])\n",
      "Token type 'newline' activations shape: torch.Size([140, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for all positions first, then extract specific token positions\n",
    "print(\"Extracting activations for all positions...\")\n",
    "full_activations, metadata, formatted_prompts = extract_activations_and_metadata(prompts_df['prompt'].tolist(), LAYER_INDEX)\n",
    "print(f\"Full activations shape: {full_activations.shape}\")\n",
    "\n",
    "# Extract activations for all token types\n",
    "print(\"\\nExtracting activations for specific token positions...\")\n",
    "token_activations = extract_token_activations(full_activations, metadata)\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Token type '{token_type}' activations shape: {activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations (Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for specific token positions...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing SAE features for token type 'asst'...\n",
      "Features shape for 'asst': torch.Size([140, 131072])\n",
      "Processing SAE features for token type 'endheader'...\n",
      "Features shape for 'endheader': torch.Size([140, 131072])\n",
      "Processing SAE features for token type 'newline'...\n",
      "Features shape for 'newline': torch.Size([140, 131072])\n",
      "\n",
      "Optimization: Pre-computing SAE features for all positions...\n",
      "Processing 140 prompts with max 160 tokens each...\n",
      "Full SAE features shape: torch.Size([140, 160, 131072])\n",
      "✓ SAE features pre-computed for all positions\n",
      "\n",
      "Completed SAE feature extraction for 3 token types\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features_batched(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations with proper batching.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "\n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "\n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "\n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Pre-compute SAE features for ALL positions at once\n",
    "print(\"Pre-computing SAE features for all positions...\")\n",
    "print(f\"Processing {full_activations.shape[0]} prompts with max {full_activations.shape[1]} tokens each...\")\n",
    "\n",
    "# Reshape to [total_positions, hidden_dim]\n",
    "total_positions = full_activations.shape[0] * full_activations.shape[1]\n",
    "reshaped_activations = full_activations.view(total_positions, -1)\n",
    "\n",
    "# Apply SAE to all positions\n",
    "full_sae_features = get_sae_features_batched(reshaped_activations)\n",
    "\n",
    "# Reshape back to [num_prompts, seq_len, num_features]\n",
    "full_sae_features = full_sae_features.view(full_activations.shape[0], full_activations.shape[1], -1)\n",
    "\n",
    "print(f\"Full SAE features shape: {full_sae_features.shape}\")\n",
    "\n",
    "print(\"✓ SAE features computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined Analysis and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimized analysis functions defined\n"
     ]
    }
   ],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_filtered_feature_activations(token_features: torch.Tensor, feature_ids: List[int]) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Get feature activations for filtered features based on feature_ids.\"\"\"\n",
    "    # Get feature indices\n",
    "    feature_indices = torch.tensor(feature_ids, dtype=torch.long)\n",
    "    \n",
    "    # Extract activations for these features\n",
    "    feature_activations = token_features[:, feature_indices]\n",
    "    \n",
    "    return feature_indices, feature_activations\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_detailed_tokens(feature_id: int, feature_activations: torch.Tensor, \n",
    "                                     feature_idx: int, prompts_df: pd.DataFrame,\n",
    "                                     full_sae_features: torch.Tensor, metadata: List[Dict],\n",
    "                                     activation_threshold: float = 0.0) -> List[Dict]:\n",
    "    \"\"\"Collect detailed token activations for a specific feature using pre-computed SAE features.\"\"\"\n",
    "    # Find which prompts activate this feature\n",
    "    activations = feature_activations[:, feature_idx]\n",
    "    active_mask = activations > activation_threshold\n",
    "    active_indices = torch.where(active_mask)[0]\n",
    "    \n",
    "    if len(active_indices) == 0:\n",
    "        return []\n",
    "    \n",
    "    all_token_records = []\n",
    "    \n",
    "    for prompt_idx in active_indices:\n",
    "        prompt_idx = int(prompt_idx)\n",
    "        \n",
    "        # Use pre-computed SAE features instead of re-computing\n",
    "        feature_activations_sequence = full_sae_features[prompt_idx, :, feature_id]  # [seq_len]\n",
    "        \n",
    "        # Get tokenized input from metadata (cached)\n",
    "        input_ids = metadata[prompt_idx]['input_ids']\n",
    "\n",
    "        # Create tokenized prompt as list of token strings\n",
    "        tokenized_prompt = [tokenizer.decode([int(token_id)]) for token_id in input_ids]\n",
    "        \n",
    "        # Get prompt info\n",
    "        prompt_text = prompts_df.iloc[prompt_idx][\"prompt\"]\n",
    "        \n",
    "        # Collect token data for this prompt\n",
    "        tokens = []\n",
    "        for pos_idx in range(min(len(feature_activations_sequence), len(input_ids))):\n",
    "            activation_val = float(feature_activations_sequence[pos_idx])\n",
    "            \n",
    "            if activation_val > 0:\n",
    "                token_id = int(input_ids[pos_idx])\n",
    "                token_text = tokenizer.decode([token_id])\n",
    "                \n",
    "                tokens.append({\n",
    "                    'position': pos_idx,\n",
    "                    'token_id': token_id,\n",
    "                    'text': token_text,\n",
    "                    'activation': activation_val\n",
    "                })\n",
    "        \n",
    "        # Only add if we found tokens\n",
    "        if tokens:\n",
    "            # Sort tokens by activation (descending)\n",
    "            tokens.sort(key=lambda x: x['activation'], reverse=True)\n",
    "            \n",
    "            all_token_records.append({\n",
    "                'prompt_id': prompt_idx,\n",
    "                'prompt_text': prompt_text,\n",
    "                'prompt_label': prompts_df.iloc[prompt_idx][\"label\"],\n",
    "                'prompt_feature_activation': float(activations[prompt_idx]),\n",
    "                'tokenized_prompt': tokenized_prompt,\n",
    "                'tokens': tokens\n",
    "            })\n",
    "    \n",
    "    return all_token_records\n",
    "\n",
    "# Pre-compute dashboard link template\n",
    "def create_dashboard_link(feature_id: int) -> str:\n",
    "    \"\"\"Create dashboard link for a feature.\"\"\"\n",
    "    return f\"{config.base_url}/{feature_id}\"\n",
    "\n",
    "print(\"Optimized analysis functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 92 total features from ./results/1_personal/only_personal.csv\n",
      "Processing results for source: llama_trainer1_layer15\n",
      "\n",
      "Processing token type: asst\n",
      "Found 2 features for token_type='asst', source='llama_trainer1_layer15'\n",
      "Processed 1 active features for token_type='asst' (skipped 1 inactive)\n",
      "\n",
      "Processing token type: endheader\n",
      "Found 6 features for token_type='endheader', source='llama_trainer1_layer15'\n",
      "Processed 2 active features for token_type='endheader' (skipped 4 inactive)\n",
      "\n",
      "Processing token type: newline\n",
      "Found 7 features for token_type='newline', source='llama_trainer1_layer15'\n",
      "Processed 0 active features for token_type='newline' (skipped 7 inactive)\n",
      "\n",
      "Total features processed: 3 CSV, 3 JSONL\n"
     ]
    }
   ],
   "source": [
    "# Load target features from file\n",
    "target_features = [45426]\n",
    "\n",
    "# Prepare prompt activaton details\n",
    "jsonl_results = []\n",
    "\n",
    "# Get filtered feature activations for this token type and source\n",
    "feature_indices, feature_activations = get_filtered_feature_activations(\n",
    "    token_features, target_features\n",
    ")\n",
    "\n",
    "if len(filtered_features_df) == 0:\n",
    "    print(f\"No features found for token_type='{token_type}', source='{source_name}'. Skipping.\")\n",
    "    continue\n",
    "\n",
    "# Process each feature\n",
    "features_processed = 0\n",
    "features_skipped = 0\n",
    "\n",
    "for idx, (feature_idx, feature_id) in enumerate(zip(feature_indices, filtered_features_df['feature_id'])):\n",
    "    feature_id = int(feature_id)\n",
    "    activations = feature_activations[:, idx]  # [num_prompts]\n",
    "    \n",
    "    # Collect detailed token data for JSONL (OPTIMIZED)\n",
    "    detailed_tokens = collect_detailed_tokens_optimized(\n",
    "        feature_id, feature_activations, idx, prompts_df, full_sae_features, metadata\n",
    "    )\n",
    "    \n",
    "    # Add to JSONL results if we have detailed data\n",
    "    if detailed_tokens:\n",
    "        jsonl_result = {\n",
    "            'feature_id': feature_id,\n",
    "            'active_prompts': detailed_tokens\n",
    "        }\n",
    "        jsonl_results.append(jsonl_result)\n",
    "    \n",
    "    features_processed += 1\n",
    "\n",
    "print(f\"Processed {features_processed} active features for token_type='{token_type}' (skipped {features_skipped} inactive)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "JSONL results saved to ./results/3_personal_general/3_personal_general_prompts.jsonl\n",
      "\n",
      "Preview of CSV data:\n",
      " feature_id  activation_mean  activation_max  activation_min  num_prompts chat_desc pt_desc type                 source     token                                                                                          link\n",
      "      27476         0.310405        0.338905        0.288229            5                        llama_trainer1_layer15      asst https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=27476\n",
      "      59035         0.362718        0.435948        0.286962           14                        llama_trainer1_layer15 endheader https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=59035\n",
      "      47776         0.400181        0.400181        0.400181            1                        llama_trainer1_layer15 endheader https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=47776\n",
      "\n",
      "Summary by token type:\n",
      "          activation_mean                 activation_max num_prompts      \\\n",
      "                    count    mean     max            max        mean max   \n",
      "token                                                                      \n",
      "asst                    1  0.3104  0.3104         0.3389         5.0   5   \n",
      "endheader               2  0.3814  0.4002         0.4359         7.5  14   \n",
      "\n",
      "          feature_id  \n",
      "             nunique  \n",
      "token                 \n",
      "asst               1  \n",
      "endheader          2  \n",
      "\n",
      "JSONL file structure summary:\n",
      "  Feature 27476 (asst): 5 prompts, 40 tokens\n",
      "  Feature 47776 (endheader): 1 prompts, 3 tokens\n",
      "  Feature 59035 (endheader): 14 prompts, 531 tokens\n",
      "\n",
      "Total: 3 features, 20 prompt records, 574 token records\n",
      "\n",
      "Sample JSONL record structure:\n",
      "Feature 27476 (asst, llama_trainer1_layer15):\n",
      "  First prompt: 12 active tokens\n",
      "  Top token: '<|begin_of_text|>' (activation: 30.1081)\n",
      "\n",
      "✓ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save JSONL results\n",
    "jsonl_results.sort(key=lambda x: x['feature_id'])  # Sort by feature_id\n",
    "with open(PROMPT_OUTPUT_FILE, 'a') as f:\n",
    "    for record in jsonl_results:\n",
    "        f.write(json.dumps(record) + '\\n')\n",
    "\n",
    "print(f\"JSONL results saved to {PROMPT_OUTPUT_FILE}\")\n",
    "\n",
    "# Show preview and summary\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\n✓ Analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
