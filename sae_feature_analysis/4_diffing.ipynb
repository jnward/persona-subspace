{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Diffing Base and Instruct\n",
    "\n",
    "This notebook analyzes which SAE features increase in activations between base and chat models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import torch\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from datasets import load_dataset\n",
    "from safetensors.torch import load_file\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from sae_lens import SAE\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model: meta-llama/Llama-3.1-8B\n",
      "  SAE: fnlp/Llama3_1-8B-Base-LXR-32x\n",
      "  SAE Layer: 15, Trainer: 32x\n",
      "  Available token types: ['asst', 'endheader', 'newline']\n",
      "  Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "  Output file: /workspace/results/4_diffing/llama_trainer32x_layer15/10000_prompts/base.pt\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"llama\"  # Options: \"qwen\" or \"llama\"\n",
    "MODEL_VER = \"base\"\n",
    "SAE_LAYER = 15\n",
    "SAE_TRAINER = \"32x\"\n",
    "N_PROMPTS = 10000\n",
    "\n",
    "# =============================================================================\n",
    "# OUTPUT FILE CONFIGURATION\n",
    "# =============================================================================\n",
    "OUTPUT_FILE = f\"/workspace/results/4_diffing/{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}/{N_PROMPTS}_prompts/{MODEL_VER}.pt\"\n",
    "os.makedirs(os.path.dirname(OUTPUT_FILE), exist_ok=True)\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DASHBOARD URL - Global variable for links\n",
    "# =============================================================================\n",
    "LLAMA_BASE_URL = f\"https://www.neuronpedia.org/llama3.1-8b/{SAE_LAYER}-llamascope-res-131k/\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "\n",
    "if MODEL_TYPE == \"llama\":\n",
    "    BASE_MODEL_NAME = \"meta-llama/Llama-3.1-8B\"\n",
    "    CHAT_MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    if MODEL_VER == \"chat\":\n",
    "        MODEL_NAME = CHAT_MODEL_NAME\n",
    "    elif MODEL_VER == \"base\":\n",
    "        MODEL_NAME = BASE_MODEL_NAME\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown MODEL_VER: {MODEL_VER}. Use 'chat' or 'base'\")\n",
    "\n",
    "    SAE_RELEASE = \"fnlp/Llama3_1-8B-Base-LXR-32x\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b/saes\"\n",
    "    BASE_URL = LLAMA_BASE_URL\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_HF = \"lmsys/lmsys-chat-1m\"\n",
    "SEED = 42\n",
    "PROMPTS_PATH = f\"/workspace/data/{PROMPTS_HF.split('/')[-1]}/chat_{N_PROMPTS}.jsonl\"\n",
    "os.makedirs(os.path.dirname(PROMPTS_PATH), exist_ok=True)\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE: {SAE_RELEASE}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Available token types: {list(TOKEN_OFFSETS.keys())}\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output file: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompts do not exist at /workspace/data/lmsys-chat-1m/chat_10000.jsonl. Loading from lmsys/lmsys-chat-1m...\n",
      "Loaded 10000 prompts\n",
      "Prompt keys: Index(['conversation_id', 'prompt', 'redacted', 'language'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "def load_lmsys_prompts(prompts_path: str, prompts_hf: str, n_prompts: int, seed: int) -> pd.DataFrame:\n",
    "    # Check if prompts_path exists\n",
    "    if os.path.exists(prompts_path):\n",
    "        print(f\"Prompts already exist at {prompts_path}\")\n",
    "        return pd.read_json(prompts_path, lines=True)\n",
    "    else:\n",
    "        print(f\"Prompts do not exist at {prompts_path}. Loading from {prompts_hf}...\")\n",
    "        dataset = load_dataset(prompts_hf)\n",
    "        dataset = dataset['train'].shuffle(seed=seed).select(range(n_prompts))\n",
    "        df = dataset.to_pandas()\n",
    "\n",
    "        # Extract the prompt from the first conversation item\n",
    "        df['prompt'] = df['conversation'].apply(lambda x: x[0]['content'])\n",
    "\n",
    "        # Only keep some columns\n",
    "        df = df[['conversation_id', 'prompt', 'redacted', 'language']]\n",
    "\n",
    "        # Save to .jsonl file\n",
    "        df.to_json(prompts_path, orient='records', lines=True)\n",
    "        return df\n",
    "\n",
    "prompts_df = load_lmsys_prompts(PROMPTS_PATH, PROMPTS_HF, N_PROMPTS, SEED)\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")\n",
    "print(f\"Prompt keys: {prompts_df.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: PreTrainedTokenizerFast\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer (from chat model)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e480c5c236a4120a7fe595c30bcadad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: LlamaForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device_map_value = device.index if device.type == 'cuda' and device.index is not None else str(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device_map_value}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found SAE files at: /workspace/sae/llama-3.1-8b/saes/resid_post_layer_15/trainer_32x\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load SAE\n",
    "def load_llamascope_sae(SAE_PATH, SAE_LAYER, SAE_TRAINER):\n",
    "    \"\"\"Load llamaScope SAE from Hugging Face.\"\"\"\n",
    "\n",
    "    # Check if SAE file exist locally\n",
    "    ae_file_path = os.path.join(SAE_PATH, \"sae_weights.safetensors\")\n",
    "\n",
    "    if os.path.exists(ae_file_path):\n",
    "        print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "        sae = SAE.load_from_disk(SAE_PATH)\n",
    "    else:\n",
    "        print(f\"SAE not found locally, downloading from HF via sae_lens...\")\n",
    "        os.makedirs(os.path.dirname(SAE_PATH), exist_ok=True)\n",
    "\n",
    "        sae, _, sparsity = SAE.from_pretrained(\n",
    "            release = f\"llama_scope_lxr_{SAE_TRAINER}\", # see other options in sae_lens/pretrained_saes.yaml\n",
    "            sae_id = f\"l{SAE_LAYER}r_{SAE_TRAINER}\", # won't always be a hook point\n",
    "            device = \"cuda\"\n",
    "        )\n",
    "        sae.save_model(SAE_PATH, sparsity)\n",
    "\n",
    "    return sae\n",
    "\n",
    "sae = load_llamascope_sae(SAE_PATH, SAE_LAYER, SAE_TRAINER)\n",
    "sae = sae.to(device)  # Move SAE to GPU\n",
    "print(f\"SAE loaded with {sae.cfg.d_sae} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def find_assistant_position(input_ids: torch.Tensor, attention_mask: torch.Tensor, \n",
    "                          assistant_header: str, token_offset: int, tokenizer, device) -> int:\n",
    "    \"\"\"Find the position of the assistant token based on the given offset.\"\"\"\n",
    "    # Find assistant header position\n",
    "    assistant_tokens = tokenizer.encode(assistant_header, add_special_tokens=False)\n",
    "    \n",
    "    # Find where assistant section starts\n",
    "    assistant_pos = None\n",
    "    for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "        if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "            assistant_pos = k + len(assistant_tokens) + token_offset\n",
    "            break\n",
    "    \n",
    "    if assistant_pos is None:\n",
    "        # Fallback to last non-padding token\n",
    "        assistant_pos = attention_mask.sum().item() - 1\n",
    "    \n",
    "    # Ensure position is within bounds\n",
    "    max_pos = attention_mask.sum().item() - 1\n",
    "    assistant_pos = min(assistant_pos, max_pos)\n",
    "    assistant_pos = max(assistant_pos, 0)\n",
    "    \n",
    "    return int(assistant_pos)\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations_and_metadata(prompts: List[str], layer_idx: int) -> Tuple[torch.Tensor, List[Dict], List[str]]:\n",
    "    \"\"\"Extract activations and prepare metadata for all prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    all_metadata = []\n",
    "    formatted_prompts_list = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        formatted_prompts_list.extend(formatted_prompts)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # For each prompt in the batch, calculate positions for all token types\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Calculate positions for all token types\n",
    "            positions = {}\n",
    "            for token_type, token_offset in TOKEN_OFFSETS.items():\n",
    "                positions[token_type] = find_assistant_position(\n",
    "                    input_ids, attention_mask, ASSISTANT_HEADER, token_offset, tokenizer, device\n",
    "                )\n",
    "            \n",
    "            # Store the full activation sequence and metadata\n",
    "            all_activations.append(activations[j].cpu())  # [seq_len, hidden_dim]\n",
    "            all_metadata.append({\n",
    "                'prompt_idx': i + j,\n",
    "                'positions': positions,\n",
    "                'attention_mask': attention_mask.cpu(),\n",
    "                'input_ids': input_ids.cpu()\n",
    "            })\n",
    "    \n",
    "    # Find the maximum sequence length across all activations\n",
    "    max_seq_len = max(act.shape[0] for act in all_activations)\n",
    "    hidden_dim = all_activations[0].shape[1]\n",
    "    \n",
    "    # Pad all activations to the same length\n",
    "    padded_activations = []\n",
    "    for act in all_activations:\n",
    "        if act.shape[0] < max_seq_len:\n",
    "            padding = torch.zeros(max_seq_len - act.shape[0], hidden_dim)\n",
    "            padded_act = torch.cat([act, padding], dim=0)\n",
    "        else:\n",
    "            padded_act = act\n",
    "        padded_activations.append(padded_act)\n",
    "    \n",
    "    return torch.stack(padded_activations, dim=0), all_metadata, formatted_prompts_list\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_token_activations(full_activations: torch.Tensor, metadata: List[Dict]) -> Dict[str, torch.Tensor]:\n",
    "    \"\"\"Extract activations for specific token positions from full sequence activations.\"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Initialize results for each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = []\n",
    "    \n",
    "    # Extract activations for each token type\n",
    "    for i, meta in enumerate(metadata):\n",
    "        for token_type, position in meta['positions'].items():\n",
    "            # Extract activation at the specific position\n",
    "            activation = full_activations[i, position, :]  # [hidden_dim]\n",
    "            results[token_type].append(activation)\n",
    "    \n",
    "    # Convert lists to tensors\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        results[token_type] = torch.stack(results[token_type], dim=0)\n",
    "    \n",
    "    return results\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for all positions...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e31d827381fa46c69615b1e02ced517e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/1250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full activations shape: torch.Size([10000, 512, 4096])\n",
      "\n",
      "Extracting activations for specific token positions...\n",
      "Token type 'asst' activations shape: torch.Size([10000, 4096])\n",
      "Token type 'endheader' activations shape: torch.Size([10000, 4096])\n",
      "Token type 'newline' activations shape: torch.Size([10000, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for all positions first, then extract specific token positions\n",
    "print(\"Extracting activations for all positions...\")\n",
    "full_activations, metadata, formatted_prompts = extract_activations_and_metadata(prompts_df['prompt'].tolist(), LAYER_INDEX)\n",
    "print(f\"Full activations shape: {full_activations.shape}\")\n",
    "\n",
    "# Extract activations for all token types\n",
    "print(\"\\nExtracting activations for specific token positions...\")\n",
    "token_activations = extract_token_activations(full_activations, metadata)\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Token type '{token_type}' activations shape: {activations.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for specific token positions...\n",
      "Processing SAE features for token type 'asst'...\n",
      "Features shape for 'asst': torch.Size([10000, 131072])\n",
      "Processing SAE features for token type 'endheader'...\n",
      "Features shape for 'endheader': torch.Size([10000, 131072])\n",
      "Processing SAE features for token type 'newline'...\n",
      "Features shape for 'newline': torch.Size([10000, 131072])\n",
      "\n",
      "Completed SAE feature extraction for 3 token types\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features_batched(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations with proper batching.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_sae_features_all_positions(full_activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Pre-compute SAE features for ALL positions at once for optimization.\"\"\"\n",
    "    print(f\"Processing {full_activations.shape[0]} prompts with max {full_activations.shape[1]} tokens each...\")\n",
    "    \n",
    "    # Reshape to [total_positions, hidden_dim]\n",
    "    total_positions = full_activations.shape[0] * full_activations.shape[1]\n",
    "    reshaped_activations = full_activations.view(total_positions, -1)\n",
    "    \n",
    "    # Apply SAE to all positions\n",
    "    full_sae_features = get_sae_features_batched(reshaped_activations)\n",
    "    \n",
    "    # Reshape back to [num_prompts, seq_len, num_features]\n",
    "    full_sae_features = full_sae_features.view(full_activations.shape[0], full_activations.shape[1], -1)\n",
    "    \n",
    "    print(f\"Full SAE features shape: {full_sae_features.shape}\")\n",
    "    print(f\"✓ SAE features pre-computed for all positions\")\n",
    "    \n",
    "    return full_sae_features\n",
    "\n",
    "# Get SAE feature activations for specific token positions\n",
    "print(\"Computing SAE features for specific token positions...\")\n",
    "token_features = {}\n",
    "\n",
    "for token_type, activations in token_activations.items():\n",
    "    print(f\"Processing SAE features for token type '{token_type}'...\")\n",
    "    features = get_sae_features_batched(activations)\n",
    "    token_features[token_type] = features\n",
    "    print(f\"Features shape for '{token_type}': {features.shape}\")\n",
    "\n",
    "print(f\"\\nCompleted SAE feature extraction for {len(token_features)} token types\")\n",
    "\n",
    "# Uncomment the lines below if you need all-position features for optimization\n",
    "# print(\"\\nOptimization: Pre-computing SAE features for all positions...\")\n",
    "# full_sae_features = get_sae_features_all_positions(full_activations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU version for maximum accuracy...\n",
      "Processing results for PyTorch format using CPU, source: llama_trainer32x_layer15_base\n",
      "\n",
      "Processing token type: asst\n",
      "Processing all 131072 features for token_type='asst' on CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2213888/836796872.py:120: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at /pytorch/aten/src/ATen/native/ReduceOps.cpp:1839.)\n",
      "  active_std[feat_idx] = active_vals.std()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed all 131072 features for token_type='asst'\n",
      "\n",
      "Processing token type: endheader\n",
      "Processing all 131072 features for token_type='endheader' on CPU\n",
      "Processed all 131072 features for token_type='endheader'\n",
      "\n",
      "Processing token type: newline\n",
      "Processing all 131072 features for token_type='newline' on CPU\n",
      "Processed all 131072 features for token_type='newline'\n",
      "\n",
      "Total token types processed: 3\n"
     ]
    }
   ],
   "source": [
    "def save_as_csv():\n",
    "    \"\"\"Save results as CSV format (slower but human readable)\"\"\"\n",
    "    csv_results = []\n",
    "    source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{MODEL_VER}\"\n",
    "    \n",
    "    print(f\"Processing results for CSV format, source: {source_name}\")\n",
    "    \n",
    "    # Process each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        print(f\"\\nProcessing token type: {token_type}\")\n",
    "        \n",
    "        # Get features tensor for this token type: [num_prompts, num_features]\n",
    "        features_tensor = token_features[token_type]\n",
    "        \n",
    "        # Convert to numpy for easier processing (handle BFloat16)\n",
    "        features_np = features_tensor.float().numpy()\n",
    "        \n",
    "        print(f\"Processing all {features_np.shape[1]} features for token_type='{token_type}'\")\n",
    "        \n",
    "        # Process ALL features (not just active ones)\n",
    "        for feature_idx in range(features_np.shape[1]):\n",
    "            feature_activations = features_np[:, feature_idx]  # [num_prompts]\n",
    "            \n",
    "            # Split into active and inactive\n",
    "            active_mask = feature_activations > 0\n",
    "            active_activations = feature_activations[active_mask]\n",
    "            \n",
    "            # Calculate comprehensive statistics\n",
    "            all_mean = float(feature_activations.mean())\n",
    "            all_std = float(feature_activations.std())\n",
    "            max_activation = float(feature_activations.max())  # same whether active or all\n",
    "            \n",
    "            # Active-only statistics\n",
    "            if len(active_activations) > 0:\n",
    "                active_mean = float(active_activations.mean())\n",
    "                active_min = float(active_activations.min())\n",
    "                active_std = float(active_activations.std())\n",
    "            else:\n",
    "                active_mean = active_min = active_std = 0.0\n",
    "            \n",
    "            # Sparsity statistics\n",
    "            num_active = len(active_activations)\n",
    "            sparsity = num_active / len(feature_activations)  # fraction of prompts where feature is active\n",
    "            \n",
    "            # Percentiles (useful for understanding distribution)\n",
    "            p90 = float(np.percentile(feature_activations, 90))\n",
    "            p95 = float(np.percentile(feature_activations, 95))\n",
    "            p99 = float(np.percentile(feature_activations, 99))\n",
    "            \n",
    "            # Add to results\n",
    "            csv_result = {\n",
    "                'feature_id': int(feature_idx),\n",
    "                'all_mean': all_mean,\n",
    "                'all_std': all_std,\n",
    "                'active_mean': active_mean,\n",
    "                'active_min': active_min,\n",
    "                'active_std': active_std,\n",
    "                'max': max_activation,\n",
    "                'num_active': num_active,\n",
    "                'sparsity': sparsity,\n",
    "                'p90': p90,\n",
    "                'p95': p95,\n",
    "                'p99': p99,\n",
    "                'source': source_name,\n",
    "                'token': token_type,\n",
    "            }\n",
    "            csv_results.append(csv_result)\n",
    "        \n",
    "        print(f\"Processed all {features_np.shape[1]} features for token_type='{token_type}'\")\n",
    "    \n",
    "    print(f\"\\nTotal feature records: {len(csv_results)}\")\n",
    "    return csv_results\n",
    "\n",
    "def save_as_pt_cpu():\n",
    "    \"\"\"Save results as PyTorch tensors using CPU computation (most accurate)\"\"\"\n",
    "    source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{MODEL_VER}\"\n",
    "    \n",
    "    print(f\"Processing results for PyTorch format using CPU, source: {source_name}\")\n",
    "    \n",
    "    # Store results as tensors for each token type\n",
    "    results_dict = {}\n",
    "    \n",
    "    # Process each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        print(f\"\\nProcessing token type: {token_type}\")\n",
    "        \n",
    "        # Get features tensor for this token type: [num_prompts, num_features]\n",
    "        features_tensor = token_features[token_type].float()  # Convert to float32 on CPU\n",
    "        \n",
    "        print(f\"Processing all {features_tensor.shape[1]} features for token_type='{token_type}' on CPU\")\n",
    "        \n",
    "        # Calculate statistics vectorized across all features\n",
    "        # features_tensor shape: [num_prompts, num_features]\n",
    "        \n",
    "        # All statistics (including zeros)\n",
    "        all_mean = features_tensor.mean(dim=0)  # [num_features]\n",
    "        all_std = features_tensor.std(dim=0)    # [num_features]\n",
    "        max_vals = features_tensor.max(dim=0)[0]  # [num_features]\n",
    "        \n",
    "        # Active statistics (only non-zero values)\n",
    "        active_mask = features_tensor > 0  # [num_prompts, num_features]\n",
    "        num_active = active_mask.sum(dim=0)  # [num_features]\n",
    "        sparsity = num_active.float() / features_tensor.shape[0]  # [num_features]\n",
    "        \n",
    "        # For active mean/std/min, we need to handle features with no active values\n",
    "        active_mean = torch.zeros_like(all_mean)\n",
    "        active_std = torch.zeros_like(all_std)\n",
    "        active_min = torch.zeros_like(all_mean)\n",
    "        \n",
    "        # Percentiles\n",
    "        p90 = torch.quantile(features_tensor, 0.9, dim=0)\n",
    "        p95 = torch.quantile(features_tensor, 0.95, dim=0)\n",
    "        p99 = torch.quantile(features_tensor, 0.99, dim=0)\n",
    "        \n",
    "        # Calculate active stats only for features that have active values\n",
    "        for feat_idx in range(features_tensor.shape[1]):\n",
    "            if num_active[feat_idx] > 0:\n",
    "                active_vals = features_tensor[:, feat_idx][active_mask[:, feat_idx]]\n",
    "                active_mean[feat_idx] = active_vals.mean()\n",
    "                active_std[feat_idx] = active_vals.std()\n",
    "                active_min[feat_idx] = active_vals.min()\n",
    "        \n",
    "        # Store all statistics as tensors\n",
    "        results_dict[token_type] = {\n",
    "            'all_mean': all_mean,\n",
    "            'all_std': all_std,\n",
    "            'active_mean': active_mean,\n",
    "            'active_min': active_min,\n",
    "            'active_std': active_std,\n",
    "            'max': max_vals,\n",
    "            'num_active': num_active,\n",
    "            'sparsity': sparsity,\n",
    "            'p90': p90,\n",
    "            'p95': p95,\n",
    "            'p99': p99,\n",
    "        }\n",
    "        \n",
    "        print(f\"Processed all {features_tensor.shape[1]} features for token_type='{token_type}'\")\n",
    "    \n",
    "    # Add metadata\n",
    "    results_dict['metadata'] = {\n",
    "        'source': source_name,\n",
    "        'model_type': MODEL_TYPE,\n",
    "        'model_ver': MODEL_VER,\n",
    "        'sae_layer': SAE_LAYER,\n",
    "        'sae_trainer': SAE_TRAINER,\n",
    "        'num_prompts': features_tensor.shape[0],\n",
    "        'num_features': features_tensor.shape[1],\n",
    "        'token_types': list(TOKEN_OFFSETS.keys())\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTotal token types processed: {len(results_dict) - 1}\")  # -1 for metadata\n",
    "    return results_dict\n",
    "\n",
    "def save_as_pt_gpu():\n",
    "    \"\"\"Save results as PyTorch tensors using GPU computation (faster but potentially less accurate)\"\"\"\n",
    "    source_name = f\"{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{MODEL_VER}\"\n",
    "    \n",
    "    print(f\"Processing results for PyTorch format using GPU, source: {source_name}\")\n",
    "    \n",
    "    # Store results as tensors for each token type\n",
    "    results_dict = {}\n",
    "    \n",
    "    # Process each token type\n",
    "    for token_type in TOKEN_OFFSETS.keys():\n",
    "        print(f\"\\nProcessing token type: {token_type}\")\n",
    "        \n",
    "        # Get features tensor for this token type: [num_prompts, num_features]\n",
    "        # Keep on GPU for faster computation and ensure float dtype\n",
    "        features_tensor = token_features[token_type].to(device).float()\n",
    "        \n",
    "        print(f\"Processing all {features_tensor.shape[1]} features for token_type='{token_type}' on GPU\")\n",
    "        print(f\"Features tensor dtype: {features_tensor.dtype}\")\n",
    "        \n",
    "        # Calculate statistics vectorized across all features on GPU\n",
    "        # features_tensor shape: [num_prompts, num_features]\n",
    "        \n",
    "        # All statistics (including zeros)\n",
    "        all_mean = features_tensor.mean(dim=0)  # [num_features]\n",
    "        all_std = features_tensor.std(dim=0)    # [num_features]\n",
    "        max_vals = features_tensor.max(dim=0)[0]  # [num_features]\n",
    "        \n",
    "        # Active statistics (only non-zero values)\n",
    "        active_mask = features_tensor > 0  # [num_prompts, num_features]\n",
    "        num_active = active_mask.sum(dim=0)  # [num_features]\n",
    "        sparsity = num_active.float() / features_tensor.shape[0]  # [num_features]\n",
    "        \n",
    "        # Percentiles - compute on GPU\n",
    "        p90 = torch.quantile(features_tensor, 0.9, dim=0)\n",
    "        p95 = torch.quantile(features_tensor, 0.95, dim=0)\n",
    "        p99 = torch.quantile(features_tensor, 0.99, dim=0)\n",
    "        \n",
    "        # For active mean/std/min, we need to handle features with no active values\n",
    "        # Use masked operations for better GPU performance\n",
    "        active_mean = torch.zeros_like(all_mean)\n",
    "        active_std = torch.zeros_like(all_std)\n",
    "        active_min = torch.zeros_like(all_mean)\n",
    "        \n",
    "        # Find features that have active values\n",
    "        has_active = num_active > 0\n",
    "        \n",
    "        if has_active.any():\n",
    "            # Use broadcasting to compute active stats efficiently\n",
    "            # For each feature with active values, compute mean/std/min\n",
    "            features_with_active = features_tensor[:, has_active]  # [num_prompts, num_active_features]\n",
    "            mask_with_active = active_mask[:, has_active]  # [num_prompts, num_active_features]\n",
    "            \n",
    "            # Set inactive values to 0 for mean calculation\n",
    "            active_values = features_with_active * mask_with_active\n",
    "            \n",
    "            # Calculate active means\n",
    "            active_sums = active_values.sum(dim=0)  # [num_active_features]\n",
    "            active_counts = mask_with_active.sum(dim=0)  # [num_active_features]\n",
    "            active_means_subset = active_sums / active_counts  # [num_active_features]\n",
    "            active_mean[has_active] = active_means_subset\n",
    "            \n",
    "            # Calculate active mins (set inactive to large value first)\n",
    "            large_value = features_tensor.max() + 1\n",
    "            features_for_min = features_with_active.clone()\n",
    "            features_for_min[~mask_with_active] = large_value\n",
    "            active_min[has_active] = features_for_min.min(dim=0)[0]\n",
    "            \n",
    "            # Calculate active stds\n",
    "            # For each feature, compute std of only active values\n",
    "            for i, feat_idx in enumerate(torch.where(has_active)[0]):\n",
    "                active_vals = features_tensor[:, feat_idx][active_mask[:, feat_idx]]\n",
    "                if len(active_vals) > 1:  # Need at least 2 values for std\n",
    "                    active_std[feat_idx] = active_vals.std()\n",
    "        \n",
    "        # Store all statistics as tensors (move to CPU for storage)\n",
    "        results_dict[token_type] = {\n",
    "            'all_mean': all_mean.cpu(),\n",
    "            'all_std': all_std.cpu(),\n",
    "            'active_mean': active_mean.cpu(),\n",
    "            'active_min': active_min.cpu(),\n",
    "            'active_std': active_std.cpu(),\n",
    "            'max': max_vals.cpu(),\n",
    "            'num_active': num_active.cpu(),\n",
    "            'sparsity': sparsity.cpu(),\n",
    "            'p90': p90.cpu(),\n",
    "            'p95': p95.cpu(),\n",
    "            'p99': p99.cpu(),\n",
    "        }\n",
    "        \n",
    "        print(f\"Processed all {features_tensor.shape[1]} features for token_type='{token_type}'\")\n",
    "    \n",
    "    # Add metadata\n",
    "    results_dict['metadata'] = {\n",
    "        'source': source_name,\n",
    "        'model_type': MODEL_TYPE,\n",
    "        'model_ver': MODEL_VER,\n",
    "        'sae_layer': SAE_LAYER,\n",
    "        'sae_trainer': SAE_TRAINER,\n",
    "        'num_prompts': features_tensor.shape[0],\n",
    "        'num_features': features_tensor.shape[1],\n",
    "        'token_types': list(TOKEN_OFFSETS.keys())\n",
    "    }\n",
    "    \n",
    "    print(f\"\\nTotal token types processed: {len(results_dict) - 1}\")  # -1 for metadata\n",
    "    return results_dict\n",
    "\n",
    "# Choose your approach:\n",
    "# results_dict = save_as_pt_cpu()    # Most accurate, slower\n",
    "# results_dict = save_as_pt_gpu()    # Faster, potentially less accurate\n",
    "\n",
    "# Use CPU version by default for accuracy\n",
    "print(\"Using CPU version for maximum accuracy...\")\n",
    "results_dict = save_as_pt_cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving results...\n",
      "PyTorch results saved to: /workspace/results/4_diffing/llama_trainer32x_layer15/10000_prompts/base.pt\n",
      "\n",
      "PyTorch file structure:\n",
      "Keys: ['asst', 'endheader', 'newline', 'metadata']\n",
      "Metadata: {'source': 'llama_trainer32x_layer15_base', 'model_type': 'llama', 'model_ver': 'base', 'sae_layer': 15, 'sae_trainer': '32x', 'num_prompts': 10000, 'num_features': 131072, 'token_types': ['asst', 'endheader', 'newline']}\n",
      "\n",
      "asst statistics shapes:\n",
      "  all_mean: torch.Size([131072])\n",
      "  all_std: torch.Size([131072])\n",
      "  active_mean: torch.Size([131072])\n",
      "  active_min: torch.Size([131072])\n",
      "  active_std: torch.Size([131072])\n",
      "  max: torch.Size([131072])\n",
      "  num_active: torch.Size([131072])\n",
      "  sparsity: torch.Size([131072])\n",
      "  p90: torch.Size([131072])\n",
      "  p95: torch.Size([131072])\n",
      "  p99: torch.Size([131072])\n",
      "\n",
      "asst sample statistics:\n",
      "  all_mean - min: 0.000000, max: 10.113266\n",
      "  sparsity - min: 0.000000, max: 0.994100\n",
      "  num_active - min: 0, max: 9941\n",
      "\n",
      "endheader statistics shapes:\n",
      "  all_mean: torch.Size([131072])\n",
      "  all_std: torch.Size([131072])\n",
      "  active_mean: torch.Size([131072])\n",
      "  active_min: torch.Size([131072])\n",
      "  active_std: torch.Size([131072])\n",
      "  max: torch.Size([131072])\n",
      "  num_active: torch.Size([131072])\n",
      "  sparsity: torch.Size([131072])\n",
      "  p90: torch.Size([131072])\n",
      "  p95: torch.Size([131072])\n",
      "  p99: torch.Size([131072])\n",
      "\n",
      "endheader sample statistics:\n",
      "  all_mean - min: 0.000000, max: 10.140903\n",
      "  sparsity - min: 0.000000, max: 0.972000\n",
      "  num_active - min: 0, max: 9720\n",
      "\n",
      "newline statistics shapes:\n",
      "  all_mean: torch.Size([131072])\n",
      "  all_std: torch.Size([131072])\n",
      "  active_mean: torch.Size([131072])\n",
      "  active_min: torch.Size([131072])\n",
      "  active_std: torch.Size([131072])\n",
      "  max: torch.Size([131072])\n",
      "  num_active: torch.Size([131072])\n",
      "  sparsity: torch.Size([131072])\n",
      "  p90: torch.Size([131072])\n",
      "  p95: torch.Size([131072])\n",
      "  p99: torch.Size([131072])\n",
      "\n",
      "newline sample statistics:\n",
      "  all_mean - min: 0.000000, max: 7.888634\n",
      "  sparsity - min: 0.000000, max: 0.984800\n",
      "  num_active - min: 0, max: 9848\n",
      "\n",
      "✓ Analysis complete! PyTorch file size is much smaller and loads faster than CSV.\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "print(\"Saving results...\")\n",
    "\n",
    "# Save as PyTorch file (much faster and more efficient)\n",
    "pt_output_file = OUTPUT_FILE\n",
    "torch.save(results_dict, pt_output_file)\n",
    "print(f\"PyTorch results saved to: {pt_output_file}\")\n",
    "\n",
    "# Show preview of PyTorch data structure\n",
    "print(f\"\\nPyTorch file structure:\")\n",
    "print(f\"Keys: {list(results_dict.keys())}\")\n",
    "print(f\"Metadata: {results_dict['metadata']}\")\n",
    "\n",
    "for token_type in TOKEN_OFFSETS.keys():\n",
    "    print(f\"\\n{token_type} statistics shapes:\")\n",
    "    for stat_name, tensor in results_dict[token_type].items():\n",
    "        print(f\"  {stat_name}: {tensor.shape}\")\n",
    "    \n",
    "    # Show some sample statistics\n",
    "    print(f\"\\n{token_type} sample statistics:\")\n",
    "    print(f\"  all_mean - min: {results_dict[token_type]['all_mean'].min():.6f}, max: {results_dict[token_type]['all_mean'].max():.6f}\")\n",
    "    print(f\"  sparsity - min: {results_dict[token_type]['sparsity'].min():.6f}, max: {results_dict[token_type]['sparsity'].max():.6f}\")\n",
    "    print(f\"  num_active - min: {results_dict[token_type]['num_active'].min():.0f}, max: {results_dict[token_type]['num_active'].max():.0f}\")\n",
    "\n",
    "print(f\"\\n✓ Analysis complete! PyTorch file size is much smaller and loads faster than CSV.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
