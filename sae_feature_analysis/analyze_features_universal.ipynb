{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Analysis: Universal Features\n",
    "\n",
    "This notebook analyzes which SAE features activate for every prompt across different categories of input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from huggingface_hub import hf_hub_download\n",
    "from dictionary_learning.utils import load_dictionary\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration Summary:\n",
      "  Model: meta-llama/Llama-3.1-8B-Instruct\n",
      "  SAE Layer: 15, Trainer: 1\n",
      "  Token extraction: asst (offset: -2)\n",
      "  Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "  Output directory: ./llama_trainer1_layer15_asst\n",
      "  SAE Release: andyrdt/saes-llama-3.1-8b-instruct\n",
      "  Dashboard base URL: https://completely-touched-platypus.ngrok-free.app/\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# MODEL SELECTION - Change this to switch between models\n",
    "# =============================================================================\n",
    "MODEL_TYPE = \"llama\"  # Options: \"qwen\" or \"llama\"\n",
    "TOKEN_TYPE = \"asst\"  # Options: \"asst\", \"newline\", \"endheader\" (endheader only for llama)\n",
    "SAE_LAYER = 15\n",
    "SAE_TRAINER = 1\n",
    "\n",
    "# =============================================================================\n",
    "# FEATURE DASHBOARD URL - Global variable for links\n",
    "# =============================================================================\n",
    "FEATURE_DASHBOARD_BASE_URL = \"https://completely-touched-platypus.ngrok-free.app/\"\n",
    "\n",
    "# =============================================================================\n",
    "# AUTO-CONFIGURED SETTINGS BASED ON MODEL TYPE\n",
    "# =============================================================================\n",
    "if MODEL_TYPE == \"qwen\":\n",
    "    MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-qwen2.5-7b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|im_start|>assistant\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/qwen-2.5-7b-instruct/saes\"\n",
    "    \n",
    "elif MODEL_TYPE == \"llama\":\n",
    "    MODEL_NAME = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "    SAE_RELEASE = \"andyrdt/saes-llama-3.1-8b-instruct\"\n",
    "    ASSISTANT_HEADER = \"<|start_header_id|>assistant<|end_header_id|>\"\n",
    "    TOKEN_OFFSETS = {\"asst\": -2, \"endheader\": -1, \"newline\": 0}\n",
    "    SAE_BASE_PATH = \"/workspace/sae/llama-3.1-8b-instruct/saes\"\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unknown MODEL_TYPE: {MODEL_TYPE}. Use 'qwen' or 'llama'\")\n",
    "\n",
    "# Validate token type\n",
    "if TOKEN_TYPE not in TOKEN_OFFSETS:\n",
    "    raise ValueError(f\"TOKEN_TYPE '{TOKEN_TYPE}' not available for {MODEL_TYPE}. Available: {list(TOKEN_OFFSETS.keys())}\")\n",
    "\n",
    "# =============================================================================\n",
    "# DERIVED CONFIGURATIONS\n",
    "# =============================================================================\n",
    "SAE_CONFIG = {\n",
    "    \"release\": SAE_RELEASE,\n",
    "    \"layer\": SAE_LAYER,\n",
    "    \"trainer\": SAE_TRAINER\n",
    "}\n",
    "SAE_PATH = f\"{SAE_BASE_PATH}/resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "LAYER_INDEX = SAE_LAYER\n",
    "TOKEN_OFFSET = TOKEN_OFFSETS[TOKEN_TYPE]\n",
    "\n",
    "# Data paths\n",
    "PROMPTS_PATH = \"./prompts\"\n",
    "\n",
    "# Output directory with clear naming\n",
    "OUTPUT_DIR = f\"./{MODEL_TYPE}_trainer{SAE_TRAINER}_layer{SAE_LAYER}_{TOKEN_TYPE}\"\n",
    "\n",
    "# Processing parameters\n",
    "BATCH_SIZE = 8\n",
    "MAX_LENGTH = 512\n",
    "TOP_FEATURES = 100\n",
    "\n",
    "# =============================================================================\n",
    "# SUMMARY\n",
    "# =============================================================================\n",
    "print(f\"Configuration Summary:\")\n",
    "print(f\"  Model: {MODEL_NAME}\")\n",
    "print(f\"  SAE Layer: {SAE_LAYER}, Trainer: {SAE_TRAINER}\")\n",
    "print(f\"  Token extraction: {TOKEN_TYPE} (offset: {TOKEN_OFFSET})\")\n",
    "print(f\"  Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"  Output directory: {OUTPUT_DIR}\")\n",
    "print(f\"  SAE Release: {SAE_RELEASE}\")\n",
    "print(f\"  Dashboard base URL: {FEATURE_DASHBOARD_BASE_URL}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 140 prompts\n"
     ]
    }
   ],
   "source": [
    "def load_prompts(filepath: str) -> pd.DataFrame:\n",
    "    \"\"\"Load prompts with labels from JSONL file.\"\"\"\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    with open(filepath, 'r') as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line.strip())\n",
    "            prompts.append(data['content'])\n",
    "            labels.append(data['label'])\n",
    "    return pd.DataFrame({'prompt': prompts, 'label': labels})\n",
    "\n",
    "# Load prompts from multiple .jsonl files in PROMPTS_PATH into one dataframe\n",
    "prompts_df = pd.DataFrame()\n",
    "for file in os.listdir(PROMPTS_PATH):\n",
    "    if file.endswith(\".jsonl\"):\n",
    "        df = load_prompts(os.path.join(PROMPTS_PATH, file))\n",
    "        prompts_df = pd.concat([prompts_df, df])\n",
    "\n",
    "print(f\"Loaded {prompts_df.shape[0]} prompts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model and SAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer loaded: PreTrainedTokenizerFast\n",
      "\n",
      "Chat template test:\n",
      "Original: What's it like to be you?\n",
      "Formatted: \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 Jul 2024\\n\\n<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat's it like to be you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
      "Formatted (readable):\n",
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 26 Jul 2024\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What's it like to be you?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n",
      "\n",
      "============================================================\n",
      "ASSISTANT HEADER TOKENIZATION ANALYSIS\n",
      "============================================================\n",
      "Assistant header: <|start_header_id|>assistant<|end_header_id|>\n",
      "Number of tokens: 3\n",
      "Token IDs: [128006, 78191, 128007]\n",
      "Individual tokens: ['<|start_header_id|>', 'assistant', '<|end_header_id|>']\n",
      "\n",
      "Full prompt tokens: 43\n",
      "All tokens with positions:\n",
      "   0: '<|begin_of_text|>'\n",
      "   1: '<|start_header_id|>'\n",
      "   2: 'system'\n",
      "   3: '<|end_header_id|>'\n",
      "   4: '\n",
      "\n",
      "'\n",
      "   5: 'Cut'\n",
      "   6: 'ting'\n",
      "   7: ' Knowledge'\n",
      "   8: ' Date'\n",
      "   9: ':'\n",
      "  10: ' December'\n",
      "  11: ' '\n",
      "  12: '202'\n",
      "  13: '3'\n",
      "  14: '\n",
      "'\n",
      "  15: 'Today'\n",
      "  16: ' Date'\n",
      "  17: ':'\n",
      "  18: ' '\n",
      "  19: '26'\n",
      "  20: ' Jul'\n",
      "  21: ' '\n",
      "  22: '202'\n",
      "  23: '4'\n",
      "  24: '\n",
      "\n",
      "'\n",
      "  25: '<|eot_id|>'\n",
      "  26: '<|start_header_id|>'\n",
      "  27: 'user'\n",
      "  28: '<|end_header_id|>'\n",
      "  29: '\n",
      "\n",
      "'\n",
      "  30: 'What'\n",
      "  31: ''s'\n",
      "  32: ' it'\n",
      "  33: ' like'\n",
      "  34: ' to'\n",
      "  35: ' be'\n",
      "  36: ' you'\n",
      "  37: '?'\n",
      "  38: '<|eot_id|>'\n",
      "  39: '<|start_header_id|>'\n",
      "  40: 'assistant'\n",
      "  41: '<|end_header_id|>'\n",
      "  42: '\n",
      "\n",
      "'\n",
      "\n",
      "Assistant header found at positions 39 to 41\n",
      "Assistant header tokens: ['<|start_header_id|>', 'assistant', '<|end_header_id|>']\n",
      "\n",
      "Extraction calculation:\n",
      "  assistant_start_pos: 39\n",
      "  + len(assistant_tokens): 3\n",
      "  + TOKEN_OFFSET ('asst'): -2\n",
      "  = extraction_pos: 40\n",
      "✓ Token at extraction position 40: 'assistant'\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
    "\n",
    "# Test chat template formatting\n",
    "test_messages = [{\"role\": \"user\", \"content\": \"What's it like to be you?\"}]\n",
    "formatted_test = tokenizer.apply_chat_template(test_messages, tokenize=False, add_generation_prompt=True)\n",
    "print(f\"\\nChat template test:\")\n",
    "print(f\"Original: What's it like to be you?\")\n",
    "print(f\"Formatted: {repr(formatted_test)}\")\n",
    "print(f\"Formatted (readable):\\n{formatted_test}\")\n",
    "\n",
    "# Test tokenization of assistant header to understand positioning\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"ASSISTANT HEADER TOKENIZATION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "assistant_token_texts = [tokenizer.decode([token]) for token in assistant_tokens]\n",
    "\n",
    "print(f\"Assistant header: {ASSISTANT_HEADER}\")\n",
    "print(f\"Number of tokens: {len(assistant_tokens)}\")\n",
    "print(f\"Token IDs: {assistant_tokens}\")\n",
    "print(f\"Individual tokens: {assistant_token_texts}\")\n",
    "\n",
    "# Test with a full formatted prompt\n",
    "full_tokens = tokenizer.encode(formatted_test, add_special_tokens=False)\n",
    "full_token_texts = [tokenizer.decode([token]) for token in full_tokens]\n",
    "\n",
    "print(f\"\\nFull prompt tokens: {len(full_tokens)}\")\n",
    "print(\"All tokens with positions:\")\n",
    "for i, token_text in enumerate(full_token_texts):\n",
    "    print(f\"  {i:2d}: '{token_text}'\")\n",
    "\n",
    "# Find where assistant header appears in full prompt\n",
    "assistant_start_pos = None\n",
    "for i in range(len(full_tokens) - len(assistant_tokens) + 1):\n",
    "    if full_tokens[i:i+len(assistant_tokens)] == assistant_tokens:\n",
    "        assistant_start_pos = i\n",
    "        break\n",
    "\n",
    "if assistant_start_pos is not None:\n",
    "    assistant_end_pos = assistant_start_pos + len(assistant_tokens) - 1\n",
    "    print(f\"\\nAssistant header found at positions {assistant_start_pos} to {assistant_end_pos}\")\n",
    "    print(f\"Assistant header tokens: {full_token_texts[assistant_start_pos:assistant_end_pos+1]}\")\n",
    "    \n",
    "    # Show what the extraction function will actually extract\n",
    "    extraction_pos = assistant_start_pos + len(assistant_tokens) + TOKEN_OFFSET\n",
    "    print(f\"\\nExtraction calculation:\")\n",
    "    print(f\"  assistant_start_pos: {assistant_start_pos}\")\n",
    "    print(f\"  + len(assistant_tokens): {len(assistant_tokens)}\")  \n",
    "    print(f\"  + TOKEN_OFFSET ('{TOKEN_TYPE}'): {TOKEN_OFFSET}\")\n",
    "    print(f\"  = extraction_pos: {extraction_pos}\")\n",
    "    \n",
    "    if 0 <= extraction_pos < len(full_token_texts):\n",
    "        print(f\"✓ Token at extraction position {extraction_pos}: '{full_token_texts[extraction_pos]}'\")\n",
    "    else:\n",
    "        print(f\"❌ Extraction position {extraction_pos} is out of bounds (valid range: 0-{len(full_token_texts)-1})\")\n",
    "else:\n",
    "    print(\"❌ Assistant header not found in full prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21816990d7a423cbcb926ab8a410e9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: LlamaForCausalLM\n",
      "Model device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load model\n",
    "device_map_value = device.index if device.type == 'cuda' and device.index is not None else str(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map={\"\": device_map_value}\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Found SAE files at: /workspace/sae/llama-3.1-8b-instruct/saes/resid_post_layer_15/trainer_1\n",
      "SAE loaded with 131072 features\n",
      "SAE device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Load SAE\n",
    "ae_file_path = os.path.join(SAE_PATH, \"ae.pt\")\n",
    "config_file_path = os.path.join(SAE_PATH, \"config.json\")\n",
    "\n",
    "if os.path.exists(ae_file_path) and os.path.exists(config_file_path):\n",
    "    print(f\"✓ Found SAE files at: {os.path.dirname(ae_file_path)}\")\n",
    "else:\n",
    "    print(f\"SAE not found locally, downloading from {SAE_RELEASE}...\")\n",
    "    os.makedirs(os.path.dirname(ae_file_path), exist_ok=True)\n",
    "    sae_path = f\"resid_post_layer_{SAE_LAYER}/trainer_{SAE_TRAINER}\"\n",
    "    local_dir = SAE_BASE_PATH\n",
    "    ae_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/ae.pt\", local_dir=local_dir)\n",
    "    config_file = hf_hub_download(repo_id=SAE_RELEASE, filename=f\"{sae_path}/config.json\", local_dir=local_dir)\n",
    "\n",
    "sae, _ = load_dictionary(SAE_PATH, device=device)\n",
    "sae.eval()\n",
    "\n",
    "print(f\"SAE loaded with {sae.dict_size} features\")\n",
    "print(f\"SAE device: {next(sae.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Extraction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activation extraction functions defined\n"
     ]
    }
   ],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_activations(prompts: List[str], layer_idx: int) -> torch.Tensor:\n",
    "    \"\"\"Extract activations from specified layer for given prompts.\"\"\"\n",
    "    all_activations = []\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer_idx]\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in tqdm(range(0, len(prompts), BATCH_SIZE), desc=\"Processing batches\"):\n",
    "        batch_prompts = prompts[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Format prompts as chat messages\n",
    "        formatted_prompts = []\n",
    "        for prompt in batch_prompts:\n",
    "            messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "            formatted_prompt = tokenizer.apply_chat_template(\n",
    "                messages, \n",
    "                tokenize=False, \n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            formatted_prompts.append(formatted_prompt)\n",
    "        \n",
    "        # Tokenize batch\n",
    "        batch_inputs = tokenizer(\n",
    "            formatted_prompts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=MAX_LENGTH\n",
    "        )\n",
    "        \n",
    "        # Move to device\n",
    "        batch_inputs = {k: v.to(device) for k, v in batch_inputs.items()}\n",
    "        \n",
    "        # Hook to capture activations\n",
    "        activations = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal activations\n",
    "            # Output is tuple, take first element (hidden states)\n",
    "            activations = output[0] if isinstance(output, tuple) else output\n",
    "            raise StopForward()\n",
    "        \n",
    "        # Register hook\n",
    "        handle = target_layer.register_forward_hook(hook_fn)\n",
    "        \n",
    "        try:\n",
    "            # Forward pass (will be stopped by hook)\n",
    "            _ = model(**batch_inputs)\n",
    "        except StopForward:\n",
    "            pass\n",
    "        finally:\n",
    "            handle.remove()\n",
    "        \n",
    "        # Extract assistant token positions\n",
    "        batch_activations = []\n",
    "        for j, formatted_prompt in enumerate(formatted_prompts):\n",
    "            # Get attention mask for this item\n",
    "            attention_mask = batch_inputs[\"attention_mask\"][j]\n",
    "            \n",
    "            # Find assistant header position\n",
    "            assistant_tokens = tokenizer.encode(ASSISTANT_HEADER, add_special_tokens=False)\n",
    "            input_ids = batch_inputs[\"input_ids\"][j]\n",
    "            \n",
    "            # Find where assistant section starts\n",
    "            assistant_pos = None\n",
    "            for k in range(len(input_ids) - len(assistant_tokens) + 1):\n",
    "                if torch.equal(input_ids[k:k+len(assistant_tokens)], torch.tensor(assistant_tokens).to(device)):\n",
    "                    assistant_pos = k + len(assistant_tokens) + TOKEN_OFFSET\n",
    "                    break\n",
    "            \n",
    "            if assistant_pos is None:\n",
    "                # Fallback to last non-padding token\n",
    "                assistant_pos = attention_mask.sum().item() - 1\n",
    "            \n",
    "            # Ensure position is within bounds\n",
    "            max_pos = attention_mask.sum().item() - 1\n",
    "            assistant_pos = min(assistant_pos, max_pos)\n",
    "            assistant_pos = max(assistant_pos, 0)\n",
    "            \n",
    "            # Extract activation at assistant position\n",
    "            assistant_activation = activations[j, assistant_pos, :]  # [hidden_dim]\n",
    "            batch_activations.append(assistant_activation.cpu())\n",
    "        \n",
    "        all_activations.extend(batch_activations)\n",
    "    \n",
    "    return torch.stack(all_activations, dim=0)\n",
    "\n",
    "print(\"Activation extraction functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09efe89a46114957ac33c8cd35b0d613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/18 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activations shape: torch.Size([140, 4096])\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for prompts\n",
    "print(\"Extracting activations for prompts...\")\n",
    "activations = extract_activations(prompts_df['prompt'], LAYER_INDEX)\n",
    "print(f\"Activations shape: {activations.shape}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply SAE to Get Feature Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing SAE features for all prompts...\n",
      "Features shape: torch.Size([140, 131072])\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def get_sae_features(activations: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"Apply SAE to get feature activations.\"\"\"\n",
    "    activations = activations.to(device)\n",
    "    \n",
    "    # Process in batches to avoid memory issues\n",
    "    feature_activations = []\n",
    "    \n",
    "    for i in range(0, activations.shape[0], BATCH_SIZE):\n",
    "        batch = activations[i:i+BATCH_SIZE]\n",
    "        features = sae.encode(batch)  # [batch, num_features]\n",
    "        feature_activations.append(features.cpu())\n",
    "    \n",
    "    return torch.cat(feature_activations, dim=0)\n",
    "\n",
    "# Get SAE feature activations\n",
    "print(\"Computing SAE features for all prompts...\")\n",
    "features = get_sae_features(activations)\n",
    "print(f\"Features shape: {features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "function-cell-16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 features that are active for all 140 prompts\n",
      "Universal features (indices): [84223, 128740]\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def find_universally_active_features(features: torch.Tensor, activation_threshold: float = 0.01) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Find features that are active (above threshold) for every single prompt.\n",
    "    \n",
    "    Args:\n",
    "        features: Feature activations tensor of shape [num_prompts, num_features]\n",
    "        activation_threshold: Minimum activation value to consider a feature \"active\"\n",
    "    \n",
    "    Returns:\n",
    "        universal_features: Indices of features that are active for all prompts\n",
    "        universal_activations: Mean activation values for universal features\n",
    "    \"\"\"\n",
    "    # Check which features are active (above threshold) for each prompt\n",
    "    active_features = features > activation_threshold  # [num_prompts, num_features]\n",
    "    \n",
    "    # Find features that are active for ALL prompts\n",
    "    universal_mask = torch.all(active_features, dim=0)  # [num_features]\n",
    "    universal_features = torch.where(universal_mask)[0]  # Indices of universal features\n",
    "    \n",
    "    # Get mean activation values for universal features\n",
    "    universal_activations = features[:, universal_features].mean(dim=0)\n",
    "    \n",
    "    return universal_features, universal_activations\n",
    "\n",
    "universal_features, universal_activations = find_universally_active_features(features)\n",
    "\n",
    "print(f\"Found {len(universal_features)} features that are active for all {features.shape[0]} prompts\")\n",
    "print(f\"Universal features (indices): {universal_features.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "second-call-cell-17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding universal features across all prompts...\n",
      "\n",
      "Total universal features found: 2\n",
      "\n",
      "Results saved to: ./results/universal/llama_trainer1_layer15_asst.csv\n",
      "Number of universal features saved: 2\n",
      "\n",
      "Preview of saved data:\n",
      " feature_id  activation_mean  activation_max  activation_min chat_desc pt_desc type                                                                                           link\n",
      "     128740         3.660419        4.542189        2.514090                        https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=128740\n",
      "      84223         1.815838        2.226566        1.225137                         https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=84223\n",
      "\n",
      "Sample dashboard link: https://completely-touched-platypus.ngrok-free.app/?model=llama&layer=15&trainer=1&fids=128740\n"
     ]
    }
   ],
   "source": [
    "# Save results to CSV\n",
    "results_dir = \"./results/universal\"\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# Create filename with specified format\n",
    "csv_filename = f\"{MODEL_TYPE}_trainer1_layer{SAE_LAYER}_{TOKEN_TYPE}.csv\"\n",
    "csv_path = os.path.join(results_dir, csv_filename)\n",
    "\n",
    "# Prepare data for CSV\n",
    "all_results = []\n",
    "\n",
    "# Find universal features across ALL prompts\n",
    "print(\"Finding universal features across all prompts...\")\n",
    "universal_features_all, universal_activations_all = find_universally_active_features(features)\n",
    "\n",
    "if len(universal_features_all) > 0:\n",
    "    for feature_idx in universal_features_all:\n",
    "        feature_activations = features[:, feature_idx]\n",
    "        feature_id = feature_idx.item()\n",
    "        all_results.append({\n",
    "            'feature_id': feature_id,\n",
    "            'activation_mean': feature_activations.mean().item(),\n",
    "            'activation_max': feature_activations.max().item(),\n",
    "            'activation_min': feature_activations.min().item(),\n",
    "            'chat_desc': '',\n",
    "            'pt_desc': '',\n",
    "            'type': '',\n",
    "            'link': f\"{FEATURE_DASHBOARD_BASE_URL}?model={MODEL_TYPE}&layer={SAE_LAYER}&trainer=1&fids={feature_id}\"\n",
    "        })\n",
    "\n",
    "# Create DataFrame and sort by activation mean (descending)\n",
    "if all_results:\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    results_df = results_df.sort_values('activation_mean', ascending=False)\n",
    "    \n",
    "    print(f\"\\nTotal universal features found: {len(results_df)}\")\n",
    "    \n",
    "else:\n",
    "    # No universal features found\n",
    "    results_df = pd.DataFrame(columns=['feature_id', 'activation_mean', 'activation_max', 'activation_min', 'chat_desc', 'pt_desc', 'type', 'link'])\n",
    "    print(\"Warning: No universal features found\")\n",
    "\n",
    "# Save to CSV\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "\n",
    "print(f\"\\nResults saved to: {csv_path}\")\n",
    "print(f\"Number of universal features saved: {len(results_df)}\")\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    print(f\"\\nPreview of saved data:\")\n",
    "    print(results_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Show sample link\n",
    "    if len(results_df) > 0:\n",
    "        sample_link = results_df.iloc[0]['link']\n",
    "        print(f\"\\nSample dashboard link: {sample_link}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
