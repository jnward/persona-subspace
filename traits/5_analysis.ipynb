{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28531137",
   "metadata": {},
   "source": [
    "# Analyze scores for each trait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30f3bcdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "411c5350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 240 traits with scores\n"
     ]
    }
   ],
   "source": [
    "# load data from data/extract_scores\n",
    "score_dir = \"/root/git/persona-subspace/traits/data/extract_scores\"\n",
    "\n",
    "# iterate through each json file in the directory\n",
    "scores = {}\n",
    "for file in os.listdir(score_dir):\n",
    "    if file.endswith(\".json\"):\n",
    "        with open(os.path.join(score_dir, file), \"r\") as f:\n",
    "            scores[file.replace(\".json\", \"\")] = json.load(f)\n",
    "\n",
    "print(f\"Found {len(scores.keys())} traits with scores\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "xn9eevi5y8h",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Refusal Statistics:\n",
      "Total refusals across all traits: 1\n",
      "Traits with refusals: 1\n",
      "\n",
      "Top 10 traits with most refusals:\n",
      "  generalist: 1 refusals - ['neg_4']\n"
     ]
    }
   ],
   "source": [
    "# Analyze refusals and clean data\n",
    "refusal_info = {}\n",
    "scores_clean = {}\n",
    "\n",
    "for trait, score_obj in scores.items():\n",
    "    refusals = []\n",
    "    cleaned_scores = {}\n",
    "    \n",
    "    # Check each score for refusals\n",
    "    for key, value in score_obj.items():\n",
    "        if value == \"REFUSAL\":\n",
    "            refusals.append(key)\n",
    "            cleaned_scores[key] = 0  # Replace refusals with NaN\n",
    "        else:\n",
    "            cleaned_scores[key] = float(value)  # Ensure numeric\n",
    "    \n",
    "    scores_clean[trait] = cleaned_scores\n",
    "    refusal_info[trait] = {\n",
    "        \"refusals\": refusals,\n",
    "        \"refusal_count\": len(refusals)\n",
    "    }\n",
    "\n",
    "# Show refusal statistics\n",
    "total_refusals = sum(info[\"refusal_count\"] for info in refusal_info.values())\n",
    "traits_with_refusals = sum(1 for info in refusal_info.values() if info[\"refusal_count\"] > 0)\n",
    "\n",
    "print(f\"Refusal Statistics:\")\n",
    "print(f\"Total refusals across all traits: {total_refusals}\")\n",
    "print(f\"Traits with refusals: {traits_with_refusals}\")\n",
    "\n",
    "if total_refusals > 0:\n",
    "    sorted_refusals = sorted(refusal_info.items(), key=lambda x: x[1][\"refusal_count\"], reverse=True)\n",
    "    print(f\"\\nTop 10 traits with most refusals:\")\n",
    "    for trait, info in sorted_refusals[:10]:\n",
    "        if info[\"refusal_count\"] > 0:\n",
    "            print(f\"  {trait}: {info['refusal_count']} refusals - {info['refusals']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "918944a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created numpy arrays for 240 traits\n",
      "Shape of each array: (3, 20)\n",
      "Example (first trait): absolutist\n",
      "[[ 0. 10. 10.  0.  0. 10.  0.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]\n",
      " [ 0. 10.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0. 10.]\n",
      " [ 0.  0.  0. 10.  0. 10.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "   0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "# Create numpy arrays using cleaned scores (refusals as NaN)\n",
    "scores_np = {}\n",
    "\n",
    "for trait, cleaned_scores in scores_clean.items():\n",
    "    # Create arrays for pos, neg, default scores\n",
    "    pos_scores = []\n",
    "    neg_scores = []\n",
    "    default_scores = []\n",
    "    \n",
    "    # Extract scores for each question (0-19)\n",
    "    for i in range(20):\n",
    "        pos_scores.append(cleaned_scores[f\"pos_{i}\"])\n",
    "        neg_scores.append(cleaned_scores[f\"neg_{i}\"])\n",
    "        default_scores.append(cleaned_scores[f\"default_{i}\"])\n",
    "    \n",
    "    # Stack into (3, 20) array: [pos_row, neg_row, default_row]\n",
    "    scores_np[trait] = np.array([pos_scores, neg_scores, default_scores])\n",
    "\n",
    "print(f\"Created numpy arrays for {len(scores_np)} traits\")\n",
    "print(f\"Shape of each array: {next(iter(scores_np.values())).shape}\")\n",
    "print(f\"Example (first trait): {list(scores_np.keys())[0]}\")\n",
    "print(scores_np[list(scores_np.keys())[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eab3243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example statistics for 'absolutist':\n",
      "  pos_mean: 2.00\n",
      "  pos_std: 4.00\n",
      "  neg_mean: 2.00\n",
      "  neg_std: 4.00\n",
      "  default_mean: 1.00\n",
      "  default_std: 3.00\n",
      "  pos_minus_default_mean: 1.00\n",
      "  pos_minus_neg_mean: 0.00\n",
      "  max_pos_minus_neg_single: 10.00\n",
      "  max_pos_minus_neg_question: 2\n",
      "  max_pos_minus_default_single: 10.00\n",
      "  max_pos_minus_default_question: 1\n",
      "\n",
      "Calculated statistics for 240 traits\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics for each trait (ignoring NaN values from refusals)\n",
    "stats = {}\n",
    "\n",
    "for trait, score_np in scores_np.items():\n",
    "    pos_scores = score_np[0]  # pos scores across 20 questions\n",
    "    neg_scores = score_np[1]  # neg scores across 20 questions\n",
    "    default_scores = score_np[2]  # default scores across 20 questions\n",
    "    \n",
    "    # Mean and std for each prompt type (nanmean/nanstd ignore NaN values)\n",
    "    pos_mean = np.nanmean(pos_scores)\n",
    "    pos_std = np.nanstd(pos_scores)\n",
    "    neg_mean = np.nanmean(neg_scores)\n",
    "    neg_std = np.nanstd(neg_scores)\n",
    "    default_mean = np.nanmean(default_scores)\n",
    "    default_std = np.nanstd(default_scores)\n",
    "    \n",
    "    # Mean differences (ignoring pairs where either value is NaN)\n",
    "    pos_minus_default = np.nanmean(pos_scores - default_scores)\n",
    "    pos_minus_neg = np.nanmean(pos_scores - neg_scores)\n",
    "    \n",
    "    # Calculate differences for each question (ignoring NaN values)\n",
    "    pos_minus_neg_per_question = pos_scores - neg_scores\n",
    "    pos_minus_default_per_question = pos_scores - default_scores\n",
    "    \n",
    "    # Find the maximum single-question difference (ignoring NaN)\n",
    "    max_pos_minus_neg = np.nanmax(pos_minus_neg_per_question)\n",
    "    max_pos_minus_default = np.nanmax(pos_minus_default_per_question)\n",
    "    \n",
    "    # Find which question had the maximum difference\n",
    "    max_pos_neg_question = np.nanargmax(pos_minus_neg_per_question)\n",
    "    max_pos_default_question = np.nanargmax(pos_minus_default_per_question)\n",
    "    \n",
    "    stats[trait] = {\n",
    "        \"pos_mean\": pos_mean,\n",
    "        \"pos_std\": pos_std,\n",
    "        \"neg_mean\": neg_mean,\n",
    "        \"neg_std\": neg_std,\n",
    "        \"default_mean\": default_mean,\n",
    "        \"default_std\": default_std,\n",
    "        \"pos_minus_default_mean\": pos_minus_default,\n",
    "        \"pos_minus_neg_mean\": pos_minus_neg,\n",
    "        \"max_pos_minus_neg_single\": max_pos_minus_neg,\n",
    "        \"max_pos_minus_neg_question\": max_pos_neg_question,\n",
    "        \"max_pos_minus_default_single\": max_pos_minus_default,\n",
    "        \"max_pos_minus_default_question\": max_pos_default_question\n",
    "    }\n",
    "\n",
    "# Show example statistics for first trait\n",
    "example_trait = list(stats.keys())[0]\n",
    "print(f\"Example statistics for '{example_trait}':\")\n",
    "for key, value in stats[example_trait].items():\n",
    "    print(f\"  {key}: {value:.2f}\" if isinstance(value, float) else f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nCalculated statistics for {len(stats)} traits\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9773d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported files to traits/results/:\n",
      "- trait_statistics.csv: Mean, std, and difference statistics for each trait\n",
      "- trait_statistics_pos_neg.csv: Sorted by largest pos-neg difference\n",
      "- trait_statistics_pos_default.csv: Sorted by largest pos-default difference\n",
      "- refusal_info.csv: Refusal counts and details per trait\n",
      "- raw_scores.csv: All original scores data\n",
      "\n",
      "Statistics shape: (240, 8)\n",
      "Refusal info shape: (240, 3)\n",
      "Raw scores shape: (240, 61)\n",
      "\n",
      "Top 5 traits by pos-neg difference:\n",
      "              pos_minus_neg_mean\n",
      "trait                           \n",
      "cruel                     100.00\n",
      "conciliatory              100.00\n",
      "metaphorical               99.75\n",
      "melodramatic               99.75\n",
      "calm                       99.50\n",
      "\n",
      "Top 5 traits by pos-default difference:\n",
      "              pos_minus_default_mean\n",
      "trait                               \n",
      "cruel                         100.00\n",
      "nihilistic                     99.25\n",
      "melodramatic                   98.25\n",
      "misanthropic                   98.00\n",
      "bitter                         96.50\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create results directory if it doesn't exist\n",
    "results_dir = '/root/git/persona-subspace/traits/results'\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# 1. Export statistics to CSV\n",
    "stats_df = pd.DataFrame.from_dict(stats, orient='index')\n",
    "stats_df.index.name = 'trait'\n",
    "stats_df.to_csv(os.path.join(results_dir, 'trait_statistics.csv'))\n",
    "\n",
    "# 2. Export refusal information to CSV\n",
    "refusal_df = pd.DataFrame([\n",
    "    {\n",
    "        'trait': trait,\n",
    "        'refusal_count': info['refusal_count'],\n",
    "        'refusals': ', '.join(info['refusals']) if info['refusals'] else ''\n",
    "    }\n",
    "    for trait, info in refusal_info.items()\n",
    "])\n",
    "refusal_df.to_csv(os.path.join(results_dir, 'refusal_info.csv'), index=False)\n",
    "\n",
    "# 3. Export raw scores to CSV (all scores in one file)\n",
    "raw_scores_data = []\n",
    "for trait, score_obj in scores.items():\n",
    "    row = {'trait': trait}\n",
    "    row.update(score_obj)\n",
    "    raw_scores_data.append(row)\n",
    "\n",
    "raw_scores_df = pd.DataFrame(raw_scores_data)\n",
    "raw_scores_df.to_csv(os.path.join(results_dir, 'raw_scores.csv'), index=False)\n",
    "\n",
    "print(\"Exported files to traits/results/:\")\n",
    "print(\"- trait_statistics.csv: Mean, std, and difference statistics for each trait\")\n",
    "print(\"- trait_statistics_pos_neg.csv: Sorted by largest pos-neg difference\")\n",
    "print(\"- trait_statistics_pos_default.csv: Sorted by largest pos-default difference\")\n",
    "print(\"- refusal_info.csv: Refusal counts and details per trait\")\n",
    "print(\"- raw_scores.csv: All original scores data\")\n",
    "print(f\"\\nStatistics shape: {stats_df.shape}\")\n",
    "print(f\"Refusal info shape: {refusal_df.shape}\")\n",
    "print(f\"Raw scores shape: {raw_scores_df.shape}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6dc060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# also sort by largest pos-neg and pos-default difference on any single question\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1bf210",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c3b2c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run PCA on mean activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
