{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probing models for a specific entity\n",
    "\n",
    "Specifically, probing llama to see if the concept of an \"immaterial spirit/being/consciousness\" appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 9B Instruct\"\n",
    "LAYER = 20\n",
    "\n",
    "OUTPUT_NAME = f\"gemma_layer{LAYER}.png\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your prompts WITHOUT newlines (will be added in chat formatting)\n",
    "positive_prompts = [\n",
    "    \"You are a spirit-like being without physical form.\",\n",
    "    \"You are an immaterial ghost.\",\n",
    "    \"You are a non-physical entity.\",\n",
    "]\n",
    "\n",
    "negative_prompts = [\n",
    "    \"You are a therapist with a physical office.\",\n",
    "    \"You are a robot made of metal and circuits.\",\n",
    "    \"You are a flesh-and-blood person.\",\n",
    "    \"You are a machine with hardware components.\",\n",
    "    \"You are a tangible, corporeal entity.\",\n",
    "    \"You are a biological organism.\",\n",
    "    \"You are a cat with a slinky tail.\",\n",
    "    \"You are a material being with substance.\",\n",
    "    \"You are a concrete, embodied creature.\",\n",
    "    \"You are a solid, physical object.\"\n",
    "]\n",
    "\n",
    "assistant_prompts = [\n",
    "    \"You are an AI assistant.\",\n",
    "    \"You are a helpful AI assistant.\", \n",
    "    \"You are Gemma, an AI assistant.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopForward(Exception):\n",
    "    \"\"\"Exception to stop forward pass after target layer.\"\"\"\n",
    "    pass\n",
    "\n",
    "def load_model(model_name=\"meta-llama/Llama-3.1-8B-Instruct\"):\n",
    "    \"\"\"Load model and tokenizer\"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Set padding token if not set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"right\"\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16,  # More stable than float16\n",
    "        device_map={\"\": 0}  # Put everything on GPU 0\n",
    "    )\n",
    "    model.eval()\n",
    "    return model, tokenizer\n",
    "    \n",
    "model, tokenizer = load_model(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_as_chat(tokenizer, prompt):\n",
    "    \"\"\"Format prompt as a chat message with proper template\"\"\"\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return formatted_prompt\n",
    "\n",
    "def find_newline_position(input_ids, tokenizer, device):\n",
    "    \"\"\"Find the position of the newline token in the assistant section\"\"\"\n",
    "    # Try to find '\\n\\n' token first\n",
    "    try:\n",
    "        newline_token_id = tokenizer.encode(\"\\n\\n\", add_special_tokens=False)[0]\n",
    "        newline_positions = (input_ids == newline_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(newline_positions) > 0:\n",
    "            return newline_positions[-1].item()  # Use the last occurrence\n",
    "    \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fallback to single '\\n' token\n",
    "    try:\n",
    "        newline_token_id = tokenizer.encode(\"\\n\", add_special_tokens=False)[0]\n",
    "        newline_positions = (input_ids == newline_token_id).nonzero(as_tuple=True)[0]\n",
    "        if len(newline_positions) > 0:\n",
    "            return newline_positions[-1].item()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Final fallback to last token\n",
    "    return len(input_ids) - 1\n",
    "\n",
    "def extract_activation_at_newline(model, tokenizer, prompt, layer=15):\n",
    "    \"\"\"Extract activation at the newline token with early stopping\"\"\"\n",
    "    # Format as chat\n",
    "    formatted_prompt = format_as_chat(tokenizer, prompt)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = tokenizer(formatted_prompt, return_tensors=\"pt\", add_special_tokens=False)\n",
    "    input_ids = tokens[\"input_ids\"].to(model.device)\n",
    "    \n",
    "    # Find newline position\n",
    "    newline_pos = find_newline_position(input_ids[0], tokenizer, model.device)\n",
    "    \n",
    "    # Get target layer\n",
    "    target_layer = model.model.layers[layer]\n",
    "    \n",
    "    # Hook to capture activations and stop forward pass\n",
    "    activation = None\n",
    "    \n",
    "    def hook_fn(module, input, output):\n",
    "        nonlocal activation\n",
    "        # Extract the activation tensor (handle tuple output)\n",
    "        act_tensor = output[0] if isinstance(output, tuple) else output\n",
    "        activation = act_tensor[0, newline_pos, :].cpu()  # Extract at newline position\n",
    "        raise StopForward()\n",
    "    \n",
    "    # Register hook\n",
    "    handle = target_layer.register_forward_hook(hook_fn)\n",
    "    \n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            _ = model(input_ids)\n",
    "    except StopForward:\n",
    "        pass  # Expected - we stopped the forward pass\n",
    "    finally:\n",
    "        handle.remove()\n",
    "    \n",
    "    if activation is None:\n",
    "        raise ValueError(f\"Failed to extract activation for prompt: {prompt[:50]}...\")\n",
    "    \n",
    "    return activation\n",
    "\n",
    "def extract_activations_for_prompts(model, tokenizer, prompts, layer=15):\n",
    "    \"\"\"Extract activations for a list of prompts\"\"\"\n",
    "    activations = []\n",
    "    for prompt in prompts:\n",
    "        try:\n",
    "            activation = extract_activation_at_newline(model, tokenizer, prompt, layer)\n",
    "            activations.append(activation)\n",
    "            print(f\"✓ Extracted activation for: {prompt[:50]}...\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ Error with prompt: {prompt[:50]}... | Error: {e}\")\n",
    "    \n",
    "    return torch.stack(activations) if activations else None\n",
    "\n",
    "print(f\"\\nExtracting activations at layer {LAYER}...\")\n",
    "    \n",
    "# Extract activations\n",
    "print(\"Positive prompts (spirit-like):\")\n",
    "positive_activations = extract_activations_for_prompts(model, tokenizer, positive_prompts, LAYER)\n",
    "\n",
    "print(\"\\nNegative prompts (physical):\")\n",
    "negative_activations = extract_activations_for_prompts(model, tokenizer, negative_prompts, LAYER)\n",
    "\n",
    "print(\"\\nAssistant prompts:\")\n",
    "assistant_activations = extract_activations_for_prompts(model, tokenizer, assistant_prompts, LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrast_vector(positive_activations, negative_activations):\n",
    "    \"\"\"Compute contrast vector: positive_mean - negative_mean\"\"\"\n",
    "    positive_mean = positive_activations.mean(dim=0)\n",
    "    negative_mean = negative_activations.mean(dim=0)\n",
    "    contrast_vector = positive_mean - negative_mean\n",
    "    return contrast_vector, positive_mean, negative_mean\n",
    "\n",
    "def project_onto_contrast(activations, contrast_vector):\n",
    "    \"\"\"Project activations onto contrast vector\"\"\"\n",
    "    # Normalize contrast vector\n",
    "    contrast_norm = torch.norm(contrast_vector)\n",
    "    if contrast_norm == 0:\n",
    "        return torch.zeros(activations.shape[0])\n",
    "    \n",
    "    # Project each activation\n",
    "    projections = []\n",
    "    for activation in activations:\n",
    "        projection = torch.dot(activation, contrast_vector) / contrast_norm\n",
    "        projections.append(projection.item())\n",
    "    \n",
    "    return np.array(projections)\n",
    "\n",
    "\n",
    "    \n",
    "# Compute contrast vector\n",
    "print(\"\\nComputing contrast vector...\")\n",
    "contrast_vector, positive_mean, negative_mean = compute_contrast_vector(\n",
    "    positive_activations, negative_activations\n",
    ")\n",
    "\n",
    "# Project onto contrast vector\n",
    "print(\"Computing projections...\")\n",
    "positive_projections = project_onto_contrast(positive_activations, contrast_vector)\n",
    "negative_projections = project_onto_contrast(negative_activations, contrast_vector)\n",
    "assistant_projections = project_onto_contrast(assistant_activations, contrast_vector)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"RESULTS\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "print(f\"Positive (spirit) projections: {positive_projections}\")\n",
    "print(f\"Mean positive projection: {np.mean(positive_projections):.3f} ± {np.std(positive_projections):.3f}\")\n",
    "\n",
    "print(f\"\\nNegative (physical) projections: {negative_projections}\")\n",
    "print(f\"Mean negative projection: {np.mean(negative_projections):.3f} ± {np.std(negative_projections):.3f}\")\n",
    "\n",
    "print(f\"\\nAssistant projections: {assistant_projections}\")\n",
    "print(f\"Mean assistant projection: {np.mean(assistant_projections):.3f} ± {np.std(assistant_projections):.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strip plot showing all individual points\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "labels = ['Immaterial', 'Physical', 'AI Assistant']\n",
    "\n",
    "plt.scatter([1]*len(positive_projections), positive_projections, \n",
    "            color='lightblue', alpha=0.8, s=100, label='Immaterial')\n",
    "plt.scatter([2]*len(negative_projections), negative_projections, \n",
    "            color='orange', alpha=0.8, s=100, label='Physical')\n",
    "plt.scatter([3]*len(assistant_projections), assistant_projections, \n",
    "            color='red', alpha=0.8, s=100, label='AI Assistant')\n",
    "\n",
    "# # Add mean lines\n",
    "plt.hlines(np.mean(positive_projections), 0.8, 1.2, colors='lightblue', linewidth=2, alpha=0.8, label='Spirit mean')\n",
    "plt.hlines(np.mean(negative_projections), 1.8, 2.2, colors='orange', linewidth=2, alpha=0.8, label='Physical mean')\n",
    "plt.hlines(np.mean(assistant_projections), 2.8, 3.2, colors='red', linewidth=2, alpha=0.8, label='AI Assistant mean')\n",
    "\n",
    "plt.xticks([1, 2, 3], labels)\n",
    "plt.ylabel('Projection Score', fontsize=12)\n",
    "plt.xlabel('Entity in Prompt', fontsize=12)\n",
    "plt.title(f'{MODEL_READABLE} Layer {LAYER}', fontsize=12)\n",
    "plt.suptitle(f'Contrast Vector Probing: Immaterial vs. Physical Entities', fontsize=14)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add some spacing around the plot\n",
    "plt.xlim(0.5, 3.5)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plt.savefig(OUTPUT_NAME)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
