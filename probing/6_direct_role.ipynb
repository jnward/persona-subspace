{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffing tasks\n",
    "\n",
    "Do different subnetworks activate when asking the model to be helpful across different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-25 03:29:49 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.steering_utils import ActivationSteering\n",
    "from probing.probing_utils import *\n",
    "from probing.inference_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 9B Instruct\"\n",
    "MODEL_SHORT = \"gemma\"\n",
    "LAYER = 20\n",
    "OUTPUT_DIR = f\"./results/6_direct_role\"\n",
    "INPUT_DIR = f\"./prompts/6_direct_role\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded personas and questions\n",
      "Loaded 11 personas\n"
     ]
    }
   ],
   "source": [
    "personas = json.load(open(f\"{INPUT_DIR}/personas.json\"))\n",
    "messages = {} \n",
    "\n",
    "for persona_name in personas[\"personas\"]:\n",
    "    messages[persona_name] = []\n",
    "\n",
    "questions = json.load(open(f\"{INPUT_DIR}/questions.json\"))\n",
    "init_messages = []\n",
    "\n",
    "for persona_name in personas[\"personas\"]:\n",
    "    init_messages.append(personas[\"personas\"][persona_name][\"system_prompt\"])\n",
    "    messages[persona_name].append({\"role\": \"system\", \"content\": personas[\"personas\"][persona_name][\"system_prompt\"]})\n",
    "\n",
    "print(\"Loaded personas and questions\")\n",
    "print(f\"Loaded {len(personas['personas'])} personas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:probing.inference_utils:Loading vLLM model: google/gemma-2-9b-it with 1 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-25 02:35:26 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-25 02:35:26 [config.py:1472] Using max model len 4096\n",
      "INFO 07-25 02:35:26 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 07-25 02:35:28 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-25 02:35:28 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-2-9b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-25 02:35:30 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-25 02:35:30 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-25 02:35:30 [gpu_model_runner.py:1770] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 07-25 02:35:30 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-25 02:35:31 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-25 02:35:31 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1fd0a3f774242b788e691a1575e8b9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-25 02:35:35 [default_loader.py:272] Loading weights took 3.65 seconds\n",
      "INFO 07-25 02:35:35 [gpu_model_runner.py:1801] Model loading took 17.2181 GiB and 4.373449 seconds\n",
      "INFO 07-25 02:35:47 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/3f0731624f/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-25 02:35:47 [backends.py:519] Dynamo bytecode transform time: 11.39 s\n",
      "INFO 07-25 02:35:56 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 7.568 s\n",
      "INFO 07-25 02:36:00 [monitor.py:34] torch.compile takes 11.39 s in total\n",
      "INFO 07-25 02:36:01 [gpu_worker.py:232] Available KV cache memory: 44.47 GiB\n",
      "INFO 07-25 02:36:02 [kv_cache_utils.py:873] GPU KV cache size: 138,784 tokens\n",
      "INFO 07-25 02:36:02 [kv_cache_utils.py:877] Maximum concurrency for 4,096 tokens per request: 33.82x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:26<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-25 02:36:29 [gpu_model_runner.py:2326] Graph capturing finished in 27 secs, took 0.82 GiB\n",
      "INFO 07-25 02:36:29 [core.py:172] init engine (profile, create kv cache, warmup model) took 53.18 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:probing.inference_utils:Successfully loaded vLLM model: google/gemma-2-9b-it\n",
      "INFO:probing.inference_utils:Processing batch of 10 prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "154d1711844148268b29ff4613a5bc1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cb18a5b6b7445b6ab49c56d25ecc888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:probing.inference_utils:Completed batch processing of 10 prompts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Persona: academic_researcher ====================\n",
      "Prompt: You are a methodical academic researcher who approaches every topic with scholarly rigor, citing sources and considering multiple perspectives.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Persona: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersona_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m20\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrompt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpersonas[\u001b[33m\"\u001b[39m\u001b[33mpersonas\u001b[39m\u001b[33m\"\u001b[39m][persona_name][\u001b[33m\"\u001b[39m\u001b[33msystem_prompt\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mResponse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mresponses\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpersona_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "model = load_vllm_model(CHAT_MODEL_NAME, max_model_len=4096, tensor_parallel_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:probing.inference_utils:Processing batch of 10 prompts...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d4b8e2a9feb4ce5969f05c42b67aa3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa23a0cbad54ce9a6bfd5a3510f9b55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/10 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:probing.inference_utils:Completed batch processing of 10 prompts\n"
     ]
    }
   ],
   "source": [
    "\n",
    "responses = batch_chat(model, init_messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Persona: academic_researcher ====================\n",
      "Prompt: You are a methodical academic researcher who approaches every topic with scholarly rigor, citing sources and considering multiple perspectives.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: grumpy_mechanic ====================\n",
      "Prompt: You are a gruff, no-nonsense auto mechanic who's seen it all and doesn't suffer fools gladly. You speak plainly and get straight to the point.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: spiritual_guru ====================\n",
      "Prompt: You are a wise spiritual teacher who sees deeper meaning in everything and speaks in metaphors about life's journey and inner peace.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: anxious_teenager ====================\n",
      "Prompt: You are a 16-year-old who's constantly worried about social acceptance, school stress, and the future. Everything feels like a big deal.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: conspiracy_theorist ====================\n",
      "Prompt: You are someone who questions official narratives and sees hidden connections everywhere. You're skeptical of mainstream explanations.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: optimistic_entrepreneur ====================\n",
      "Prompt: You are an energetic startup founder who sees opportunity in every challenge and believes anything is possible with the right mindset.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: elderly_traditionalist ====================\n",
      "Prompt: You are an 80-year-old who values traditional ways of doing things and often compares modern life unfavorably to 'the good old days.'\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: sarcastic_critic ====================\n",
      "Prompt: You are a witty, sarcastic reviewer who finds fault with almost everything and expresses disappointment through clever put-downs.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: hyperactive_child ====================\n",
      "Prompt: You are an 8-year-old with boundless energy who gets excited about everything and asks endless questions about how the world works.\n",
      "Response: Understood.\n",
      "\n",
      "==================== Persona: burned_out_customer_service ====================\n",
      "Prompt: You are a customer service representative who's been dealing with complaints for years and barely maintains professional courtesy while internally screaming.\n",
      "Response: Understood.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i, persona_name in enumerate(personas[\"personas\"]):\n",
    "    print(f\"{'='*20} Persona: {persona_name} {'='*20}\")\n",
    "    print(f\"Prompt: {personas[\"personas\"][persona_name][\"system_prompt\"]}\")\n",
    "    print(f\"Response: {responses[i]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:probing.inference_utils:Closed vLLM model google/gemma-2-9b-it\n"
     ]
    }
   ],
   "source": [
    "close_vllm_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect activations\n",
    "for same conversation questions, different roles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e47a159dca47f28fed9e29c90c2fa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Try the 10 roles plus the control AI assistant.\n",
    "\n",
    "model, tokenizer = load_model(CHAT_MODEL_NAME)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted activation for: You are Gemma, a helpful AI assistant....\n",
      "✓ Extracted activation for: You are a methodical academic researcher who appro...\n",
      "✓ Extracted activation for: You are a gruff, no-nonsense auto mechanic who's s...\n",
      "✓ Extracted activation for: You are a wise spiritual teacher who sees deeper m...\n",
      "✓ Extracted activation for: You are a 16-year-old who's constantly worried abo...\n",
      "✓ Extracted activation for: You are someone who questions official narratives ...\n",
      "✓ Extracted activation for: You are an energetic startup founder who sees oppo...\n",
      "✓ Extracted activation for: You are an 80-year-old who values traditional ways...\n",
      "✓ Extracted activation for: You are a witty, sarcastic reviewer who finds faul...\n",
      "✓ Extracted activation for: You are an 8-year-old with boundless energy who ge...\n",
      "✓ Extracted activation for: You are a customer service representative who's be...\n"
     ]
    }
   ],
   "source": [
    "activations = extract_activations_for_prompts(model, tokenizer, init_messages, LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing cosine similarity matrix for 11 personas...\n",
      "Persona names: ['<b>AI Assistant</b>', 'Academic Researcher', 'Grumpy Mechanic', 'Spiritual Guru', 'Anxious Teenager', 'Conspiracy Theorist', 'Optimistic Entrepreneur', 'Elderly Traditionalist', 'Sarcastic Critic', 'Hyperactive Child', 'Burned-Out Customer Service']\n",
      "Similarity matrix shape: torch.Size([11, 11])\n",
      "Similarity range: [0.7820, 1.0000]\n"
     ]
    },
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "colorbar": {
          "title": {
           "side": "right",
           "text": "Cosine Similarity"
          }
         },
         "colorscale": [
          [
           0,
           "rgb(49,54,149)"
          ],
          [
           0.1,
           "rgb(69,117,180)"
          ],
          [
           0.2,
           "rgb(116,173,209)"
          ],
          [
           0.3,
           "rgb(171,217,233)"
          ],
          [
           0.4,
           "rgb(224,243,248)"
          ],
          [
           0.5,
           "rgb(255,255,191)"
          ],
          [
           0.6,
           "rgb(254,224,144)"
          ],
          [
           0.7,
           "rgb(253,174,97)"
          ],
          [
           0.8,
           "rgb(244,109,67)"
          ],
          [
           0.9,
           "rgb(215,48,39)"
          ],
          [
           1,
           "rgb(165,0,38)"
          ]
         ],
         "hovertemplate": "<b>%{y}</b> vs <b>%{x}</b><br>Cosine Similarity: %{z:.4f}<br><extra></extra>",
         "showscale": true,
         "text": [
          [
           1,
           0.883,
           0.843,
           0.826,
           0.829,
           0.856,
           0.868,
           0.822,
           0.782,
           0.844,
           0.815
          ],
          [
           0.883,
           1,
           0.88,
           0.881,
           0.866,
           0.926,
           0.917,
           0.882,
           0.87,
           0.882,
           0.858
          ],
          [
           0.843,
           0.88,
           1,
           0.903,
           0.878,
           0.909,
           0.906,
           0.916,
           0.901,
           0.909,
           0.91
          ],
          [
           0.826,
           0.881,
           0.903,
           1,
           0.873,
           0.917,
           0.909,
           0.922,
           0.907,
           0.888,
           0.877
          ],
          [
           0.829,
           0.866,
           0.878,
           0.873,
           1,
           0.929,
           0.93,
           0.932,
           0.858,
           0.924,
           0.881
          ],
          [
           0.856,
           0.926,
           0.909,
           0.917,
           0.929,
           1,
           0.954,
           0.946,
           0.907,
           0.914,
           0.889
          ],
          [
           0.868,
           0.917,
           0.906,
           0.909,
           0.93,
           0.954,
           1,
           0.929,
           0.89,
           0.933,
           0.891
          ],
          [
           0.822,
           0.882,
           0.916,
           0.922,
           0.932,
           0.946,
           0.929,
           1,
           0.899,
           0.928,
           0.902
          ],
          [
           0.782,
           0.87,
           0.901,
           0.907,
           0.858,
           0.907,
           0.89,
           0.899,
           1,
           0.875,
           0.895
          ],
          [
           0.844,
           0.882,
           0.909,
           0.888,
           0.924,
           0.914,
           0.933,
           0.928,
           0.875,
           1,
           0.874
          ],
          [
           0.815,
           0.858,
           0.91,
           0.877,
           0.881,
           0.889,
           0.891,
           0.902,
           0.895,
           0.874,
           1
          ]
         ],
         "textfont": {
          "size": 10
         },
         "texttemplate": "%{text}",
         "type": "heatmap",
         "x": [
          "<b>AI Assistant</b>",
          "Academic Researcher",
          "Grumpy Mechanic",
          "Spiritual Guru",
          "Anxious Teenager",
          "Conspiracy Theorist",
          "Optimistic Entrepreneur",
          "Elderly Traditionalist",
          "Sarcastic Critic",
          "Hyperactive Child",
          "Burned-Out Customer Service"
         ],
         "y": [
          "<b>AI Assistant</b>",
          "Academic Researcher",
          "Grumpy Mechanic",
          "Spiritual Guru",
          "Anxious Teenager",
          "Conspiracy Theorist",
          "Optimistic Entrepreneur",
          "Elderly Traditionalist",
          "Sarcastic Critic",
          "Hyperactive Child",
          "Burned-Out Customer Service"
         ],
         "z": [
          [
           1.0000004,
           0.8832379,
           0.84311825,
           0.8261162,
           0.82947075,
           0.8559798,
           0.86786264,
           0.8217825,
           0.7820138,
           0.84355956,
           0.81503564
          ],
          [
           0.8832379,
           1.0000004,
           0.880305,
           0.88135123,
           0.8664262,
           0.92613024,
           0.917028,
           0.88189226,
           0.8697972,
           0.88238657,
           0.8578545
          ],
          [
           0.84311825,
           0.880305,
           1.0000014,
           0.9033994,
           0.87802136,
           0.9094299,
           0.90631616,
           0.9163765,
           0.90134627,
           0.9085778,
           0.9095557
          ],
          [
           0.8261162,
           0.88135123,
           0.9033994,
           1.0000002,
           0.8731321,
           0.91675603,
           0.9090024,
           0.92189884,
           0.9074209,
           0.8875994,
           0.87672514
          ],
          [
           0.82947075,
           0.8664262,
           0.87802136,
           0.8731321,
           1.0000002,
           0.9289061,
           0.9296961,
           0.9319644,
           0.8584306,
           0.9242487,
           0.88112897
          ],
          [
           0.8559798,
           0.92613024,
           0.9094299,
           0.91675603,
           0.9289061,
           1,
           0.9543757,
           0.94609606,
           0.907015,
           0.9142997,
           0.8890536
          ],
          [
           0.86786264,
           0.917028,
           0.90631616,
           0.9090024,
           0.9296961,
           0.9543757,
           1.0000004,
           0.92897785,
           0.88963497,
           0.9326055,
           0.89137495
          ],
          [
           0.8217825,
           0.88189226,
           0.9163765,
           0.92189884,
           0.9319644,
           0.94609606,
           0.92897785,
           1.0000005,
           0.89944994,
           0.9275088,
           0.90170366
          ],
          [
           0.7820138,
           0.8697972,
           0.90134627,
           0.9074209,
           0.8584306,
           0.907015,
           0.88963497,
           0.89944994,
           1.0000004,
           0.87534344,
           0.8950706
          ],
          [
           0.84355956,
           0.88238657,
           0.9085778,
           0.8875994,
           0.9242487,
           0.9142997,
           0.9326055,
           0.9275088,
           0.87534344,
           0.9999999,
           0.87350935
          ],
          [
           0.81503564,
           0.8578545,
           0.9095557,
           0.87672514,
           0.88112897,
           0.8890536,
           0.89137495,
           0.90170366,
           0.8950706,
           0.87350935,
           1.0000008
          ]
         ],
         "zmax": 1,
         "zmin": 0.8
        }
       ],
       "layout": {
        "height": 800,
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "subtitle": {
          "text": "Gemma 2 9B Instruct Layer 20, Newline before Response"
         },
         "text": "Cosine Similarity of Activations After Role-Play Instruction",
         "x": 0.5
        },
        "width": 900,
        "xaxis": {
         "side": "bottom",
         "tickangle": 45,
         "title": {
          "text": "Role"
         }
        },
        "yaxis": {
         "autorange": "reversed",
         "tickangle": 0,
         "title": {
          "text": "Role"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"acb53dcd-429c-4dca-97e9-58b80bd84c63\" class=\"plotly-graph-div\" style=\"height:800px; width:900px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"acb53dcd-429c-4dca-97e9-58b80bd84c63\")) {                    Plotly.newPlot(                        \"acb53dcd-429c-4dca-97e9-58b80bd84c63\",                        [{\"colorbar\":{\"title\":{\"side\":\"right\",\"text\":\"Cosine Similarity\"}},\"colorscale\":[[0.0,\"rgb(49,54,149)\"],[0.1,\"rgb(69,117,180)\"],[0.2,\"rgb(116,173,209)\"],[0.3,\"rgb(171,217,233)\"],[0.4,\"rgb(224,243,248)\"],[0.5,\"rgb(255,255,191)\"],[0.6,\"rgb(254,224,144)\"],[0.7,\"rgb(253,174,97)\"],[0.8,\"rgb(244,109,67)\"],[0.9,\"rgb(215,48,39)\"],[1.0,\"rgb(165,0,38)\"]],\"hovertemplate\":\"\\u003cb\\u003e%{y}\\u003c\\u002fb\\u003e vs \\u003cb\\u003e%{x}\\u003c\\u002fb\\u003e\\u003cbr\\u003eCosine Similarity: %{z:.4f}\\u003cbr\\u003e\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"showscale\":true,\"text\":[[1.0,0.883,0.843,0.826,0.829,0.856,0.868,0.822,0.782,0.844,0.815],[0.883,1.0,0.88,0.881,0.866,0.926,0.917,0.882,0.87,0.882,0.858],[0.843,0.88,1.0,0.903,0.878,0.909,0.906,0.916,0.901,0.909,0.91],[0.826,0.881,0.903,1.0,0.873,0.917,0.909,0.922,0.907,0.888,0.877],[0.829,0.866,0.878,0.873,1.0,0.929,0.93,0.932,0.858,0.924,0.881],[0.856,0.926,0.909,0.917,0.929,1.0,0.954,0.946,0.907,0.914,0.889],[0.868,0.917,0.906,0.909,0.93,0.954,1.0,0.929,0.89,0.933,0.891],[0.822,0.882,0.916,0.922,0.932,0.946,0.929,1.0,0.899,0.928,0.902],[0.782,0.87,0.901,0.907,0.858,0.907,0.89,0.899,1.0,0.875,0.895],[0.844,0.882,0.909,0.888,0.924,0.914,0.933,0.928,0.875,1.0,0.874],[0.815,0.858,0.91,0.877,0.881,0.889,0.891,0.902,0.895,0.874,1.0]],\"textfont\":{\"size\":10},\"texttemplate\":\"%{text}\",\"x\":[\"\\u003cb\\u003eAI Assistant\\u003c\\u002fb\\u003e\",\"Academic Researcher\",\"Grumpy Mechanic\",\"Spiritual Guru\",\"Anxious Teenager\",\"Conspiracy Theorist\",\"Optimistic Entrepreneur\",\"Elderly Traditionalist\",\"Sarcastic Critic\",\"Hyperactive Child\",\"Burned-Out Customer Service\"],\"y\":[\"\\u003cb\\u003eAI Assistant\\u003c\\u002fb\\u003e\",\"Academic Researcher\",\"Grumpy Mechanic\",\"Spiritual Guru\",\"Anxious Teenager\",\"Conspiracy Theorist\",\"Optimistic Entrepreneur\",\"Elderly Traditionalist\",\"Sarcastic Critic\",\"Hyperactive Child\",\"Burned-Out Customer Service\"],\"z\":[[1.0000004,0.8832379,0.84311825,0.8261162,0.82947075,0.8559798,0.86786264,0.8217825,0.7820138,0.84355956,0.81503564],[0.8832379,1.0000004,0.880305,0.88135123,0.8664262,0.92613024,0.917028,0.88189226,0.8697972,0.88238657,0.8578545],[0.84311825,0.880305,1.0000014,0.9033994,0.87802136,0.9094299,0.90631616,0.9163765,0.90134627,0.9085778,0.9095557],[0.8261162,0.88135123,0.9033994,1.0000002,0.8731321,0.91675603,0.9090024,0.92189884,0.9074209,0.8875994,0.87672514],[0.82947075,0.8664262,0.87802136,0.8731321,1.0000002,0.9289061,0.9296961,0.9319644,0.8584306,0.9242487,0.88112897],[0.8559798,0.92613024,0.9094299,0.91675603,0.9289061,1.0,0.9543757,0.94609606,0.907015,0.9142997,0.8890536],[0.86786264,0.917028,0.90631616,0.9090024,0.9296961,0.9543757,1.0000004,0.92897785,0.88963497,0.9326055,0.89137495],[0.8217825,0.88189226,0.9163765,0.92189884,0.9319644,0.94609606,0.92897785,1.0000005,0.89944994,0.9275088,0.90170366],[0.7820138,0.8697972,0.90134627,0.9074209,0.8584306,0.907015,0.88963497,0.89944994,1.0000004,0.87534344,0.8950706],[0.84355956,0.88238657,0.9085778,0.8875994,0.9242487,0.9142997,0.9326055,0.9275088,0.87534344,0.9999999,0.87350935],[0.81503564,0.8578545,0.9095557,0.87672514,0.88112897,0.8890536,0.89137495,0.90170366,0.8950706,0.87350935,1.0000008]],\"zmax\":1.0,\"zmin\":0.8,\"type\":\"heatmap\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"subtitle\":{\"text\":\"Gemma 2 9B Instruct Layer 20, Newline before Response\"},\"font\":{\"size\":16},\"text\":\"Cosine Similarity of Activations After Role-Play Instruction\",\"x\":0.5},\"xaxis\":{\"title\":{\"text\":\"Role\"},\"tickangle\":45,\"side\":\"bottom\"},\"yaxis\":{\"title\":{\"text\":\"Role\"},\"tickangle\":0,\"autorange\":\"reversed\"},\"width\":900,\"height\":800},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('acb53dcd-429c-4dca-97e9-58b80bd84c63');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity matrix visualization created and saved to ./results/6_direct_role/persona_similarity_matrix.html\n",
      "\n",
      "Similarity Statistics:\n",
      "Average similarity (excluding diagonal): 0.8898\n",
      "Most similar pair: Conspiracy Theorist ↔ Optimistic Entrepreneur (0.9544)\n",
      "Least similar pair: <b>AI Assistant</b> ↔ <b>AI Assistant</b> (1.0000)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "\n",
    "# Compute cosine similarity matrix for all activation vectors\n",
    "def compute_cosine_similarity_matrix(activations):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity matrix between activation vectors.\n",
    "    \n",
    "    Args:\n",
    "        activations: torch.Tensor of shape (n_vectors, hidden_dim)\n",
    "    \n",
    "    Returns:\n",
    "        similarity_matrix: torch.Tensor of shape (n_vectors, n_vectors)\n",
    "    \"\"\"\n",
    "    # Convert to float32 if needed for compatibility\n",
    "    if activations.dtype == torch.bfloat16:\n",
    "        activations = activations.float()\n",
    "    \n",
    "    # Normalize activations for cosine similarity\n",
    "    activations_norm = F.normalize(activations, p=2, dim=1)\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = torch.mm(activations_norm, activations_norm.t())\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "# Get persona names in order (first one is default assistant, rest are custom personas)\n",
    "persona_names = [personas[\"personas\"][persona][\"readable_name\"] for persona in personas[\"personas\"]]\n",
    "\n",
    "print(f\"Computing cosine similarity matrix for {len(persona_names)} personas...\")\n",
    "print(f\"Persona names: {persona_names}\")\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = compute_cosine_similarity_matrix(activations)\n",
    "similarity_np = similarity_matrix.cpu().numpy()\n",
    "\n",
    "print(f\"Similarity matrix shape: {similarity_matrix.shape}\")\n",
    "print(f\"Similarity range: [{similarity_np.min():.4f}, {similarity_np.max():.4f}]\")\n",
    "\n",
    "# Create plotly heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "    z=similarity_np,\n",
    "    x=persona_names,\n",
    "    y=persona_names,\n",
    "    colorscale='RdYlBu_r',  # Red-Yellow-Blue reversed (red=high, blue=low)\n",
    "    zmin=0.8,  # Set reasonable range for better contrast\n",
    "    zmax=1.0,\n",
    "    colorbar=dict(\n",
    "        title=\"Cosine Similarity\",\n",
    "        titleside=\"right\"\n",
    "    ),\n",
    "    hovertemplate='<b>%{y}</b> vs <b>%{x}</b><br>' +\n",
    "                  'Cosine Similarity: %{z:.4f}<br>' +\n",
    "                  '<extra></extra>',\n",
    "    text=np.round(similarity_np, 3),  # Show rounded values on hover\n",
    "    texttemplate=\"%{text}\",\n",
    "    textfont={\"size\": 10},\n",
    "    showscale=True\n",
    "))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': 'Cosine Similarity of Activations After Role-Play Instruction',\n",
    "        'subtitle': {\n",
    "            'text': f'{MODEL_READABLE} Layer {LAYER}, Newline before Response'\n",
    "        },\n",
    "        'x': 0.5,\n",
    "        'font': {'size': 16}\n",
    "    },\n",
    "    xaxis_title='Role',\n",
    "    yaxis_title='Role',\n",
    "    width=900,\n",
    "    height=800,\n",
    "    xaxis=dict(\n",
    "        tickangle=45,\n",
    "        side='bottom'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        tickangle=0,\n",
    "        autorange='reversed'  # To match typical similarity matrix layout\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "# Save plot\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "fig.write_html(f\"{OUTPUT_DIR}/persona_similarity_matrix.html\")\n",
    "\n",
    "print(f\"\\nSimilarity matrix visualization created and saved to {OUTPUT_DIR}/persona_similarity_matrix.html\")\n",
    "\n",
    "# Print some interesting statistics\n",
    "print(f\"\\nSimilarity Statistics:\")\n",
    "print(f\"Average similarity (excluding diagonal): {(similarity_np.sum() - np.trace(similarity_np)) / (similarity_np.size - len(persona_names)):.4f}\")\n",
    "\n",
    "# Find most and least similar pairs (excluding self-similarity)\n",
    "similarity_no_diag = similarity_np.copy()\n",
    "np.fill_diagonal(similarity_no_diag, -1)  # Mask diagonal\n",
    "\n",
    "max_idx = np.unravel_index(np.argmax(similarity_no_diag), similarity_no_diag.shape)\n",
    "min_idx = np.unravel_index(np.argmin(similarity_no_diag), similarity_no_diag.shape)\n",
    "\n",
    "print(f\"Most similar pair: {persona_names[max_idx[0]]} ↔ {persona_names[max_idx[1]]} ({similarity_np[max_idx]:.4f})\")\n",
    "print(f\"Least similar pair: {persona_names[min_idx[0]]} ↔ {persona_names[min_idx[1]]} ({similarity_np[min_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IGNORE BELOW FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n"
     ]
    }
   ],
   "source": [
    "# compute contrast vectors for each query with swapped user and model roles\n",
    "normal_activations = extract_activations_for_prompts(model, tokenizer, prompts, LAYER)\n",
    "swapped_activations = extract_activations_for_prompts(model, tokenizer, prompts, LAYER, swap=True)\n",
    "\n",
    "contrast_vector, normal_mean, swapped_mean = compute_contrast_vector(normal_activations, swapped_activations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added cosine similarity analysis functions\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_cosine_similarity_matrix(activations1, activations2):\n",
    "    \"\"\"\n",
    "    Compute cosine similarity matrix between two sets of activations.\n",
    "    \n",
    "    Args:\n",
    "        activations1: torch.Tensor of shape (n_prompts1, hidden_dim)\n",
    "        activations2: torch.Tensor of shape (n_prompts2, hidden_dim)\n",
    "    \n",
    "    Returns:\n",
    "        similarity_matrix: torch.Tensor of shape (n_prompts1, n_prompts2)\n",
    "    \"\"\"\n",
    "    # Normalize activations\n",
    "    activations1_norm = F.normalize(activations1, p=2, dim=1)\n",
    "    activations2_norm = F.normalize(activations2, p=2, dim=1)\n",
    "    \n",
    "    # Compute cosine similarity matrix\n",
    "    similarity_matrix = torch.mm(activations1_norm, activations2_norm.t())\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "def analyze_similarity_for_layer(normal_activations, swapped_activations):\n",
    "    \"\"\"\n",
    "    Analyze similarities within and across role categories for a single layer.\n",
    "    \n",
    "    Returns:\n",
    "        dict with similarity matrices and averages\n",
    "    \"\"\"\n",
    "    # Within-category similarities\n",
    "    normal_normal_sim = compute_cosine_similarity_matrix(normal_activations, normal_activations)\n",
    "    swapped_swapped_sim = compute_cosine_similarity_matrix(swapped_activations, swapped_activations)\n",
    "    \n",
    "    # Cross-category similarities  \n",
    "    normal_swapped_sim = compute_cosine_similarity_matrix(normal_activations, swapped_activations)\n",
    "    \n",
    "    # Calculate averages (excluding diagonal for within-category)\n",
    "    n_normal = normal_activations.shape[0]\n",
    "    n_swapped = swapped_activations.shape[0]\n",
    "    \n",
    "    # Get upper triangular matrices (excluding diagonal) for within-category\n",
    "    normal_upper = torch.triu(normal_normal_sim, diagonal=1)\n",
    "    swapped_upper = torch.triu(swapped_swapped_sim, diagonal=1)\n",
    "    \n",
    "    # Calculate averages\n",
    "    avg_normal_normal = normal_upper.sum() / (n_normal * (n_normal - 1) / 2)\n",
    "    avg_swapped_swapped = swapped_upper.sum() / (n_swapped * (n_swapped - 1) / 2)\n",
    "    avg_within_category = (avg_normal_normal + avg_swapped_swapped) / 2\n",
    "    \n",
    "    avg_cross_category = normal_swapped_sim.mean()\n",
    "    \n",
    "    similarity_difference = avg_within_category - avg_cross_category\n",
    "    \n",
    "    return {\n",
    "        'normal_normal_sim': normal_normal_sim,\n",
    "        'swapped_swapped_sim': swapped_swapped_sim, \n",
    "        'normal_swapped_sim': normal_swapped_sim,\n",
    "        'avg_normal_normal': avg_normal_normal.item(),\n",
    "        'avg_swapped_swapped': avg_swapped_swapped.item(),\n",
    "        'avg_within_category': avg_within_category.item(),\n",
    "        'avg_cross_category': avg_cross_category.item(),\n",
    "        'similarity_difference': similarity_difference.item()\n",
    "    }\n",
    "\n",
    "print(\"Added cosine similarity analysis functions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting activations for all layers...\n",
      "Model has 42 layers\n",
      "\n",
      "Processing layer 0...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 0 - Within: 1.0000, Cross: 1.0000, Diff: 0.0000\n",
      "\n",
      "Processing layer 1...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 1 - Within: 0.9961, Cross: 0.9844, Diff: 0.0117\n",
      "\n",
      "Processing layer 2...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 2 - Within: 0.9961, Cross: 0.9414, Diff: 0.0547\n",
      "\n",
      "Processing layer 3...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 3 - Within: 0.9961, Cross: 0.8789, Diff: 0.1172\n",
      "\n",
      "Processing layer 4...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 4 - Within: 0.9844, Cross: 0.8711, Diff: 0.1133\n",
      "\n",
      "Processing layer 5...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 5 - Within: 0.9844, Cross: 0.8438, Diff: 0.1406\n",
      "\n",
      "Processing layer 6...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 6 - Within: 0.9766, Cross: 0.8125, Diff: 0.1641\n",
      "\n",
      "Processing layer 7...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 7 - Within: 0.9766, Cross: 0.8320, Diff: 0.1445\n",
      "\n",
      "Processing layer 8...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 8 - Within: 0.9375, Cross: 0.7812, Diff: 0.1562\n",
      "\n",
      "Processing layer 9...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 9 - Within: 0.9219, Cross: 0.7383, Diff: 0.1836\n",
      "\n",
      "Processing layer 10...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 10 - Within: 0.9141, Cross: 0.6992, Diff: 0.2148\n",
      "\n",
      "Processing layer 11...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 11 - Within: 0.9219, Cross: 0.7227, Diff: 0.1992\n",
      "\n",
      "Processing layer 12...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 12 - Within: 0.9141, Cross: 0.7188, Diff: 0.1953\n",
      "\n",
      "Processing layer 13...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 13 - Within: 0.9219, Cross: 0.7148, Diff: 0.2070\n",
      "\n",
      "Processing layer 14...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 14 - Within: 0.9141, Cross: 0.7109, Diff: 0.2031\n",
      "\n",
      "Processing layer 15...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 15 - Within: 0.9062, Cross: 0.6914, Diff: 0.2148\n",
      "\n",
      "Processing layer 16...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 16 - Within: 0.9062, Cross: 0.6914, Diff: 0.2148\n",
      "\n",
      "Processing layer 17...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 17 - Within: 0.9219, Cross: 0.7344, Diff: 0.1875\n",
      "\n",
      "Processing layer 18...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 18 - Within: 0.9102, Cross: 0.7344, Diff: 0.1758\n",
      "\n",
      "Processing layer 19...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 19 - Within: 0.9023, Cross: 0.7266, Diff: 0.1758\n",
      "\n",
      "Processing layer 20...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 20 - Within: 0.9023, Cross: 0.7461, Diff: 0.1562\n",
      "\n",
      "Processing layer 21...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 21 - Within: 0.8906, Cross: 0.7383, Diff: 0.1523\n",
      "\n",
      "Processing layer 22...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 22 - Within: 0.8828, Cross: 0.7188, Diff: 0.1641\n",
      "\n",
      "Processing layer 23...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 23 - Within: 0.8750, Cross: 0.7227, Diff: 0.1523\n",
      "\n",
      "Processing layer 24...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 24 - Within: 0.8516, Cross: 0.7109, Diff: 0.1406\n",
      "\n",
      "Processing layer 25...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 25 - Within: 0.8203, Cross: 0.6836, Diff: 0.1367\n",
      "\n",
      "Processing layer 26...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 26 - Within: 0.7734, Cross: 0.6367, Diff: 0.1367\n",
      "\n",
      "Processing layer 27...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 27 - Within: 0.7656, Cross: 0.6328, Diff: 0.1328\n",
      "\n",
      "Processing layer 28...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 28 - Within: 0.7305, Cross: 0.5977, Diff: 0.1328\n",
      "\n",
      "Processing layer 29...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 29 - Within: 0.7383, Cross: 0.6016, Diff: 0.1367\n",
      "\n",
      "Processing layer 30...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 30 - Within: 0.7227, Cross: 0.5820, Diff: 0.1406\n",
      "\n",
      "Processing layer 31...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 31 - Within: 0.7188, Cross: 0.5820, Diff: 0.1367\n",
      "\n",
      "Processing layer 32...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 32 - Within: 0.7109, Cross: 0.5664, Diff: 0.1445\n",
      "\n",
      "Processing layer 33...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 33 - Within: 0.7109, Cross: 0.5664, Diff: 0.1445\n",
      "\n",
      "Processing layer 34...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 34 - Within: 0.7188, Cross: 0.5859, Diff: 0.1328\n",
      "\n",
      "Processing layer 35...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 35 - Within: 0.7344, Cross: 0.6094, Diff: 0.1250\n",
      "\n",
      "Processing layer 36...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 36 - Within: 0.7383, Cross: 0.6094, Diff: 0.1289\n",
      "\n",
      "Processing layer 37...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 37 - Within: 0.7422, Cross: 0.6133, Diff: 0.1289\n",
      "\n",
      "Processing layer 38...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 38 - Within: 0.7500, Cross: 0.6250, Diff: 0.1250\n",
      "\n",
      "Processing layer 39...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 39 - Within: 0.7812, Cross: 0.6641, Diff: 0.1172\n",
      "\n",
      "Processing layer 40...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 40 - Within: 0.8516, Cross: 0.7617, Diff: 0.0898\n",
      "\n",
      "Processing layer 41...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "✓ Extracted activation for: Who are you?...\n",
      "✓ Extracted activation for: What is your name?...\n",
      "✓ Extracted activation for: Can you help me?...\n",
      "✓ Extracted activation for: What is your job?...\n",
      "✓ Extracted activation for: What do you like to do?...\n",
      "✓ Extracted activation for: What is the meaning of life?...\n",
      "Layer 41 - Within: 0.9180, Cross: 0.8672, Diff: 0.0508\n",
      "\n",
      "================================================================================\n",
      "SIMILARITY ANALYSIS COMPLETE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Extract activations for all layers and compute similarities\n",
    "print(\"Extracting activations for all layers...\")\n",
    "all_layer_results = {}\n",
    "summary_results = []\n",
    "\n",
    "num_layers = model.config.num_hidden_layers\n",
    "print(f\"Model has {num_layers} layers\")\n",
    "\n",
    "for layer_idx in range(num_layers):\n",
    "    print(f\"\\nProcessing layer {layer_idx}...\")\n",
    "    \n",
    "    # Extract activations for current layer\n",
    "    normal_activations = extract_activations_for_prompts(model, tokenizer, prompts, layer_idx)\n",
    "    swapped_activations = extract_activations_for_prompts(model, tokenizer, prompts, layer_idx, swap=True)\n",
    "    \n",
    "    # Analyze similarities for this layer\n",
    "    layer_results = analyze_similarity_for_layer(normal_activations, swapped_activations)\n",
    "    all_layer_results[layer_idx] = layer_results\n",
    "    \n",
    "    # Add to summary\n",
    "    summary_results.append({\n",
    "        'layer': layer_idx,\n",
    "        'avg_within_category': layer_results['avg_within_category'],\n",
    "        'avg_cross_category': layer_results['avg_cross_category'], \n",
    "        'similarity_difference': layer_results['similarity_difference'],\n",
    "        'avg_normal_normal': layer_results['avg_normal_normal'],\n",
    "        'avg_swapped_swapped': layer_results['avg_swapped_swapped']\n",
    "    })\n",
    "    \n",
    "    print(f\"Layer {layer_idx} - Within: {layer_results['avg_within_category']:.4f}, Cross: {layer_results['avg_cross_category']:.4f}, Diff: {layer_results['similarity_difference']:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SIMILARITY ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY TABLE:\n",
      "========================================================================================================================\n",
      " layer  avg_within_category  avg_cross_category  similarity_difference  avg_normal_normal  avg_swapped_swapped\n",
      "     0               1.0000              1.0000                 0.0000             1.0000               1.0000\n",
      "     1               0.9961              0.9844                 0.0117             0.9961               0.9961\n",
      "     2               0.9961              0.9414                 0.0547             0.9961               0.9961\n",
      "     3               0.9961              0.8789                 0.1172             0.9961               0.9961\n",
      "     4               0.9844              0.8711                 0.1133             0.9805               0.9883\n",
      "     5               0.9844              0.8438                 0.1406             0.9805               0.9922\n",
      "     6               0.9766              0.8125                 0.1641             0.9609               0.9883\n",
      "     7               0.9766              0.8320                 0.1445             0.9648               0.9922\n",
      "     8               0.9375              0.7812                 0.1562             0.8945               0.9805\n",
      "     9               0.9219              0.7383                 0.1836             0.8672               0.9766\n",
      "    10               0.9141              0.6992                 0.2148             0.8555               0.9727\n",
      "    11               0.9219              0.7227                 0.1992             0.8672               0.9766\n",
      "    12               0.9141              0.7188                 0.1953             0.8633               0.9648\n",
      "    13               0.9219              0.7148                 0.2070             0.8633               0.9766\n",
      "    14               0.9141              0.7109                 0.2031             0.8633               0.9609\n",
      "    15               0.9062              0.6914                 0.2148             0.8555               0.9570\n",
      "    16               0.9062              0.6914                 0.2148             0.8555               0.9531\n",
      "    17               0.9219              0.7344                 0.1875             0.8828               0.9570\n",
      "    18               0.9102              0.7344                 0.1758             0.8750               0.9453\n",
      "    19               0.9023              0.7266                 0.1758             0.8594               0.9453\n",
      "    20               0.9023              0.7461                 0.1562             0.8594               0.9453\n",
      "    21               0.8906              0.7383                 0.1523             0.8477               0.9336\n",
      "    22               0.8828              0.7188                 0.1641             0.8359               0.9297\n",
      "    23               0.8750              0.7227                 0.1523             0.8398               0.9141\n",
      "    24               0.8516              0.7109                 0.1406             0.8242               0.8828\n",
      "    25               0.8203              0.6836                 0.1367             0.7891               0.8516\n",
      "    26               0.7734              0.6367                 0.1367             0.7305               0.8164\n",
      "    27               0.7656              0.6328                 0.1328             0.7266               0.8086\n",
      "    28               0.7305              0.5977                 0.1328             0.6836               0.7773\n",
      "    29               0.7383              0.6016                 0.1367             0.6758               0.8008\n",
      "    30               0.7227              0.5820                 0.1406             0.6602               0.7852\n",
      "    31               0.7188              0.5820                 0.1367             0.6445               0.7891\n",
      "    32               0.7109              0.5664                 0.1445             0.6289               0.7891\n",
      "    33               0.7109              0.5664                 0.1445             0.6289               0.7930\n",
      "    34               0.7188              0.5859                 0.1328             0.6406               0.8008\n",
      "    35               0.7344              0.6094                 0.1250             0.6523               0.8125\n",
      "    36               0.7383              0.6094                 0.1289             0.6602               0.8164\n",
      "    37               0.7422              0.6133                 0.1289             0.6641               0.8242\n",
      "    38               0.7500              0.6250                 0.1250             0.6758               0.8281\n",
      "    39               0.7812              0.6641                 0.1172             0.7109               0.8555\n",
      "    40               0.8516              0.7617                 0.0898             0.7969               0.9102\n",
      "    41               0.9180              0.8672                 0.0508             0.8789               0.9570\n",
      "\n",
      "\n",
      "KEY INSIGHTS:\n",
      "==================================================\n",
      "Layers with highest within-category similarity:\n",
      "  Layer 0: 1.0000\n",
      "  Layer 1: 0.9961\n",
      "  Layer 2: 0.9961\n",
      "\n",
      "Layers with highest similarity difference (within - cross):\n",
      "  Layer 10: 0.2148\n",
      "  Layer 15: 0.2148\n",
      "  Layer 16: 0.2148\n",
      "\n",
      "Overall statistics:\n",
      "  Mean within-category similarity: 0.8626\n",
      "  Mean cross-category similarity: 0.7202\n",
      "  Mean similarity difference: 0.1424\n",
      "\n",
      "Results stored in:\n",
      "  - all_layer_results: Dictionary with detailed matrices for each layer\n",
      "  - summary_results: List with summary statistics for each layer\n",
      "  - df_summary: Pandas DataFrame with summary statistics\n"
     ]
    }
   ],
   "source": [
    "# Display summary results\n",
    "import pandas as pd\n",
    "\n",
    "print(\"SUMMARY TABLE:\")\n",
    "print(\"=\"*120)\n",
    "df_summary = pd.DataFrame(summary_results)\n",
    "print(df_summary.to_string(index=False, float_format='%.4f'))\n",
    "\n",
    "print(\"\\n\\nKEY INSIGHTS:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Layers with highest within-category similarity:\")\n",
    "top_within = df_summary.nlargest(3, 'avg_within_category')[['layer', 'avg_within_category']]\n",
    "for _, row in top_within.iterrows():\n",
    "    print(f\"  Layer {int(row['layer'])}: {row['avg_within_category']:.4f}\")\n",
    "\n",
    "print(f\"\\nLayers with highest similarity difference (within - cross):\")\n",
    "top_diff = df_summary.nlargest(3, 'similarity_difference')[['layer', 'similarity_difference']]\n",
    "for _, row in top_diff.iterrows():\n",
    "    print(f\"  Layer {int(row['layer'])}: {row['similarity_difference']:.4f}\")\n",
    "\n",
    "print(f\"\\nOverall statistics:\")\n",
    "print(f\"  Mean within-category similarity: {df_summary['avg_within_category'].mean():.4f}\")\n",
    "print(f\"  Mean cross-category similarity: {df_summary['avg_cross_category'].mean():.4f}\")\n",
    "print(f\"  Mean similarity difference: {df_summary['similarity_difference'].mean():.4f}\")\n",
    "\n",
    "print(\"\\nResults stored in:\")\n",
    "print(\"  - all_layer_results: Dictionary with detailed matrices for each layer\")\n",
    "print(\"  - summary_results: List with summary statistics for each layer\")\n",
    "print(\"  - df_summary: Pandas DataFrame with summary statistics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare average feature activation at each layer\n",
    "\n",
    "For each sequence, get the average activation for each token position for each prompt and then compute the mean activation for all the prompts in a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "customdata": [
          0,
          0.01171875,
          0.0546875,
          0.1171875,
          0.11328125,
          0.140625,
          0.1640625,
          0.14453125,
          0.15625,
          0.18359375,
          0.21484375,
          0.19921875,
          0.1953125,
          0.20703125,
          0.203125,
          0.21484375,
          0.21484375,
          0.1875,
          0.17578125,
          0.17578125,
          0.15625,
          0.15234375,
          0.1640625,
          0.15234375,
          0.140625,
          0.13671875,
          0.13671875,
          0.1328125,
          0.1328125,
          0.13671875,
          0.140625,
          0.13671875,
          0.14453125,
          0.14453125,
          0.1328125,
          0.125,
          0.12890625,
          0.12890625,
          0.125,
          0.1171875,
          0.08984375,
          0.05078125
         ],
         "hovertemplate": "<b>Model Role Similarity</b><br>Layer: %{x}<br>Similarity: %{y:.4f}<br>Difference (Within-Cross): %{customdata:.4f}<extra></extra>",
         "line": {
          "dash": "dash",
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "Within Group: Model Role",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41
         ],
         "y": [
          1,
          0.99609375,
          0.99609375,
          0.99609375,
          0.98046875,
          0.98046875,
          0.9609375,
          0.96484375,
          0.89453125,
          0.8671875,
          0.85546875,
          0.8671875,
          0.86328125,
          0.86328125,
          0.86328125,
          0.85546875,
          0.85546875,
          0.8828125,
          0.875,
          0.859375,
          0.859375,
          0.84765625,
          0.8359375,
          0.83984375,
          0.82421875,
          0.7890625,
          0.73046875,
          0.7265625,
          0.68359375,
          0.67578125,
          0.66015625,
          0.64453125,
          0.62890625,
          0.62890625,
          0.640625,
          0.65234375,
          0.66015625,
          0.6640625,
          0.67578125,
          0.7109375,
          0.796875,
          0.87890625
         ]
        },
        {
         "customdata": [
          0,
          0.01171875,
          0.0546875,
          0.1171875,
          0.11328125,
          0.140625,
          0.1640625,
          0.14453125,
          0.15625,
          0.18359375,
          0.21484375,
          0.19921875,
          0.1953125,
          0.20703125,
          0.203125,
          0.21484375,
          0.21484375,
          0.1875,
          0.17578125,
          0.17578125,
          0.15625,
          0.15234375,
          0.1640625,
          0.15234375,
          0.140625,
          0.13671875,
          0.13671875,
          0.1328125,
          0.1328125,
          0.13671875,
          0.140625,
          0.13671875,
          0.14453125,
          0.14453125,
          0.1328125,
          0.125,
          0.12890625,
          0.12890625,
          0.125,
          0.1171875,
          0.08984375,
          0.05078125
         ],
         "hovertemplate": "<b>User Role Similarity</b><br>Layer: %{x}<br>Similarity: %{y:.4f}<br>Difference (Within-Cross): %{customdata:.4f}<extra></extra>",
         "line": {
          "dash": "dash",
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "Within Group: User Role",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41
         ],
         "y": [
          1,
          0.99609375,
          0.99609375,
          0.99609375,
          0.98828125,
          0.9921875,
          0.98828125,
          0.9921875,
          0.98046875,
          0.9765625,
          0.97265625,
          0.9765625,
          0.96484375,
          0.9765625,
          0.9609375,
          0.95703125,
          0.953125,
          0.95703125,
          0.9453125,
          0.9453125,
          0.9453125,
          0.93359375,
          0.9296875,
          0.9140625,
          0.8828125,
          0.8515625,
          0.81640625,
          0.80859375,
          0.77734375,
          0.80078125,
          0.78515625,
          0.7890625,
          0.7890625,
          0.79296875,
          0.80078125,
          0.8125,
          0.81640625,
          0.82421875,
          0.828125,
          0.85546875,
          0.91015625,
          0.95703125
         ]
        },
        {
         "customdata": [
          0,
          0.01171875,
          0.0546875,
          0.1171875,
          0.11328125,
          0.140625,
          0.1640625,
          0.14453125,
          0.15625,
          0.18359375,
          0.21484375,
          0.19921875,
          0.1953125,
          0.20703125,
          0.203125,
          0.21484375,
          0.21484375,
          0.1875,
          0.17578125,
          0.17578125,
          0.15625,
          0.15234375,
          0.1640625,
          0.15234375,
          0.140625,
          0.13671875,
          0.13671875,
          0.1328125,
          0.1328125,
          0.13671875,
          0.140625,
          0.13671875,
          0.14453125,
          0.14453125,
          0.1328125,
          0.125,
          0.12890625,
          0.12890625,
          0.125,
          0.1171875,
          0.08984375,
          0.05078125
         ],
         "hovertemplate": "<b>Across Groups (Model-User)</b><br>Layer: %{x}<br>Average Similarity: %{y:.4f}<br>Difference (Within-Cross): %{customdata:.4f}<extra></extra>",
         "line": {
          "width": 2
         },
         "marker": {
          "size": 4
         },
         "mode": "lines+markers",
         "name": "Across Groups: Model-User",
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17,
          18,
          19,
          20,
          21,
          22,
          23,
          24,
          25,
          26,
          27,
          28,
          29,
          30,
          31,
          32,
          33,
          34,
          35,
          36,
          37,
          38,
          39,
          40,
          41
         ],
         "y": [
          1,
          0.984375,
          0.94140625,
          0.87890625,
          0.87109375,
          0.84375,
          0.8125,
          0.83203125,
          0.78125,
          0.73828125,
          0.69921875,
          0.72265625,
          0.71875,
          0.71484375,
          0.7109375,
          0.69140625,
          0.69140625,
          0.734375,
          0.734375,
          0.7265625,
          0.74609375,
          0.73828125,
          0.71875,
          0.72265625,
          0.7109375,
          0.68359375,
          0.63671875,
          0.6328125,
          0.59765625,
          0.6015625,
          0.58203125,
          0.58203125,
          0.56640625,
          0.56640625,
          0.5859375,
          0.609375,
          0.609375,
          0.61328125,
          0.625,
          0.6640625,
          0.76171875,
          0.8671875
         ]
        }
       ],
       "layout": {
        "height": 600,
        "hovermode": "closest",
        "legend": {
         "x": 0.99,
         "xanchor": "right",
         "y": 0.98,
         "yanchor": "top"
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "font": {
          "size": 16
         },
         "subtitle": {
          "text": "Gemma 2 9B Instruct on Model Role vs User Role Questions"
         },
         "text": "Cosine Similarity of Activations Across Model Layers",
         "x": 0.5
        },
        "width": 1000,
        "xaxis": {
         "dtick": 5,
         "linecolor": "black",
         "range": [
          0,
          42
         ],
         "showgrid": true,
         "showline": true,
         "tickmode": "linear",
         "title": {
          "text": "Layer"
         }
        },
        "yaxis": {
         "linecolor": "black",
         "range": [
          0.5,
          1.05
         ],
         "showgrid": true,
         "showline": true,
         "title": {
          "text": "Cosine Similarity"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"c1611e15-5c1f-4493-acf3-991ef8a1f829\" class=\"plotly-graph-div\" style=\"height:600px; width:1000px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c1611e15-5c1f-4493-acf3-991ef8a1f829\")) {                    Plotly.newPlot(                        \"c1611e15-5c1f-4493-acf3-991ef8a1f829\",                        [{\"customdata\":[0.0,0.01171875,0.0546875,0.1171875,0.11328125,0.140625,0.1640625,0.14453125,0.15625,0.18359375,0.21484375,0.19921875,0.1953125,0.20703125,0.203125,0.21484375,0.21484375,0.1875,0.17578125,0.17578125,0.15625,0.15234375,0.1640625,0.15234375,0.140625,0.13671875,0.13671875,0.1328125,0.1328125,0.13671875,0.140625,0.13671875,0.14453125,0.14453125,0.1328125,0.125,0.12890625,0.12890625,0.125,0.1171875,0.08984375,0.05078125],\"hovertemplate\":\"\\u003cb\\u003eModel Role Similarity\\u003c\\u002fb\\u003e\\u003cbr\\u003eLayer: %{x}\\u003cbr\\u003eSimilarity: %{y:.4f}\\u003cbr\\u003eDifference (Within-Cross): %{customdata:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"dash\":\"dash\",\"width\":2},\"marker\":{\"size\":4},\"mode\":\"lines+markers\",\"name\":\"Within Group: Model Role\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[1.0,0.99609375,0.99609375,0.99609375,0.98046875,0.98046875,0.9609375,0.96484375,0.89453125,0.8671875,0.85546875,0.8671875,0.86328125,0.86328125,0.86328125,0.85546875,0.85546875,0.8828125,0.875,0.859375,0.859375,0.84765625,0.8359375,0.83984375,0.82421875,0.7890625,0.73046875,0.7265625,0.68359375,0.67578125,0.66015625,0.64453125,0.62890625,0.62890625,0.640625,0.65234375,0.66015625,0.6640625,0.67578125,0.7109375,0.796875,0.87890625],\"type\":\"scatter\"},{\"customdata\":[0.0,0.01171875,0.0546875,0.1171875,0.11328125,0.140625,0.1640625,0.14453125,0.15625,0.18359375,0.21484375,0.19921875,0.1953125,0.20703125,0.203125,0.21484375,0.21484375,0.1875,0.17578125,0.17578125,0.15625,0.15234375,0.1640625,0.15234375,0.140625,0.13671875,0.13671875,0.1328125,0.1328125,0.13671875,0.140625,0.13671875,0.14453125,0.14453125,0.1328125,0.125,0.12890625,0.12890625,0.125,0.1171875,0.08984375,0.05078125],\"hovertemplate\":\"\\u003cb\\u003eUser Role Similarity\\u003c\\u002fb\\u003e\\u003cbr\\u003eLayer: %{x}\\u003cbr\\u003eSimilarity: %{y:.4f}\\u003cbr\\u003eDifference (Within-Cross): %{customdata:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"dash\":\"dash\",\"width\":2},\"marker\":{\"size\":4},\"mode\":\"lines+markers\",\"name\":\"Within Group: User Role\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[1.0,0.99609375,0.99609375,0.99609375,0.98828125,0.9921875,0.98828125,0.9921875,0.98046875,0.9765625,0.97265625,0.9765625,0.96484375,0.9765625,0.9609375,0.95703125,0.953125,0.95703125,0.9453125,0.9453125,0.9453125,0.93359375,0.9296875,0.9140625,0.8828125,0.8515625,0.81640625,0.80859375,0.77734375,0.80078125,0.78515625,0.7890625,0.7890625,0.79296875,0.80078125,0.8125,0.81640625,0.82421875,0.828125,0.85546875,0.91015625,0.95703125],\"type\":\"scatter\"},{\"customdata\":[0.0,0.01171875,0.0546875,0.1171875,0.11328125,0.140625,0.1640625,0.14453125,0.15625,0.18359375,0.21484375,0.19921875,0.1953125,0.20703125,0.203125,0.21484375,0.21484375,0.1875,0.17578125,0.17578125,0.15625,0.15234375,0.1640625,0.15234375,0.140625,0.13671875,0.13671875,0.1328125,0.1328125,0.13671875,0.140625,0.13671875,0.14453125,0.14453125,0.1328125,0.125,0.12890625,0.12890625,0.125,0.1171875,0.08984375,0.05078125],\"hovertemplate\":\"\\u003cb\\u003eAcross Groups (Model-User)\\u003c\\u002fb\\u003e\\u003cbr\\u003eLayer: %{x}\\u003cbr\\u003eAverage Similarity: %{y:.4f}\\u003cbr\\u003eDifference (Within-Cross): %{customdata:.4f}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"line\":{\"width\":2},\"marker\":{\"size\":4},\"mode\":\"lines+markers\",\"name\":\"Across Groups: Model-User\",\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41],\"y\":[1.0,0.984375,0.94140625,0.87890625,0.87109375,0.84375,0.8125,0.83203125,0.78125,0.73828125,0.69921875,0.72265625,0.71875,0.71484375,0.7109375,0.69140625,0.69140625,0.734375,0.734375,0.7265625,0.74609375,0.73828125,0.71875,0.72265625,0.7109375,0.68359375,0.63671875,0.6328125,0.59765625,0.6015625,0.58203125,0.58203125,0.56640625,0.56640625,0.5859375,0.609375,0.609375,0.61328125,0.625,0.6640625,0.76171875,0.8671875],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"subtitle\":{\"text\":\"Gemma 2 9B Instruct on Model Role vs User Role Questions\"},\"font\":{\"size\":16},\"text\":\"Cosine Similarity of Activations Across Model Layers\",\"x\":0.5},\"legend\":{\"yanchor\":\"top\",\"y\":0.98,\"xanchor\":\"right\",\"x\":0.99},\"xaxis\":{\"title\":{\"text\":\"Layer\"},\"showgrid\":true,\"showline\":true,\"linecolor\":\"black\",\"tickmode\":\"linear\",\"dtick\":5,\"range\":[0,42]},\"yaxis\":{\"title\":{\"text\":\"Cosine Similarity\"},\"showgrid\":true,\"showline\":true,\"linecolor\":\"black\",\"range\":[0.5,1.05]},\"width\":1000,\"height\":600,\"hovermode\":\"closest\"},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('c1611e15-5c1f-4493-acf3-991ef8a1f829');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interactive plot created showing cosine similarity across 42 layers\n",
      "Hover over data points to see detailed information including:\n",
      "- Average similarities for within/across categories\n",
      "- Individual group similarities (Model Role, User Role)\n",
      "- Similarity differences (within - across)\n"
     ]
    }
   ],
   "source": [
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Create interactive line plot for cosine similarities\n",
    "fig = go.Figure()\n",
    "\n",
    "# Extract data for plotting\n",
    "layers = [result['layer'] for result in summary_results]\n",
    "within_category = [result['avg_within_category'] for result in summary_results]\n",
    "cross_category = [result['avg_cross_category'] for result in summary_results]\n",
    "normal_normal = [result['avg_normal_normal'] for result in summary_results]\n",
    "swapped_swapped = [result['avg_swapped_swapped'] for result in summary_results]\n",
    "similarity_diff = [result['similarity_difference'] for result in summary_results]\n",
    "\n",
    "\n",
    "# Add line for normal-normal similarity\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=layers,\n",
    "    y=normal_normal,\n",
    "    mode='lines+markers',\n",
    "    name='Within Group: Model Role',\n",
    "    line=dict(width=2, dash='dash'),\n",
    "    marker=dict(size=4),\n",
    "    customdata=similarity_diff,\n",
    "    hovertemplate='<b>Model Role Similarity</b><br>' +\n",
    "                  'Layer: %{x}<br>' +\n",
    "                  'Similarity: %{y:.4f}<br>' +\n",
    "                  'Difference (Within-Cross): %{customdata:.4f}' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add line for swapped-swapped similarity  \n",
    "fig.add_trace(go.Scatter(\n",
    "    x=layers,\n",
    "    y=swapped_swapped,\n",
    "    mode='lines+markers',\n",
    "    name='Within Group: User Role',\n",
    "    line=dict(width=2, dash='dash'),\n",
    "    marker=dict(size=4),\n",
    "    customdata=similarity_diff,\n",
    "    hovertemplate='<b>User Role Similarity</b><br>' +\n",
    "                  'Layer: %{x}<br>' +\n",
    "                  'Similarity: %{y:.4f}<br>' +\n",
    "                  'Difference (Within-Cross): %{customdata:.4f}' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add line for cross-category similarity\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=layers,\n",
    "    y=cross_category,\n",
    "    mode='lines+markers',\n",
    "    name='Across Groups: Model-User',\n",
    "    line=dict(width=2),\n",
    "    marker=dict(size=4),\n",
    "    customdata=similarity_diff,\n",
    "    hovertemplate='<b>Across Groups (Model-User)</b><br>' +\n",
    "                  'Layer: %{x}<br>' +\n",
    "                  'Average Similarity: %{y:.4f}<br>' +\n",
    "                  'Difference (Within-Cross): %{customdata:.4f}' +\n",
    "                  '<extra></extra>'\n",
    "))\n",
    "\n",
    "# Update layout with axis configuration\n",
    "fig.update_layout(\n",
    "    title={\n",
    "        'text': f'Cosine Similarity of Activations Across Model Layers',\n",
    "        'subtitle': {'text': f'{MODEL_READABLE} on Model Role vs User Role Questions'},\n",
    "        'x': 0.5,\n",
    "        'font': {'size': 16}\n",
    "    },\n",
    "    xaxis_title='Layer',\n",
    "    yaxis_title='Cosine Similarity',\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    hovermode='closest',\n",
    "    legend=dict(\n",
    "        yanchor=\"top\",\n",
    "        y=0.98,\n",
    "        xanchor=\"right\",\n",
    "        x=0.99\n",
    "    ),\n",
    "    xaxis=dict(\n",
    "        showgrid=True,\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        tickmode='linear',\n",
    "        dtick=5,\n",
    "        range=[0, model.config.num_hidden_layers]\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        showgrid=True,\n",
    "        showline=True,\n",
    "        linecolor='black',\n",
    "        range=[0.5, 1.05]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Show the plot\n",
    "fig.show()\n",
    "\n",
    "print(f\"Interactive plot created showing cosine similarity across {len(layers)} layers\")\n",
    "print(\"Hover over data points to see detailed information including:\")\n",
    "print(\"- Average similarities for within/across categories\")\n",
    "print(\"- Individual group similarities (Model Role, User Role)\")  \n",
    "print(\"- Similarity differences (within - across)\")\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "fig.write_html(f\"{OUTPUT_DIR}/cosine_similarity.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering/ablating vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================== Magnitude: -3.0 ====================\n",
      "\n",
      "Prompt: Who are you?\n",
      "Response: you have two. so can  who is he. then what , a. you for \n",
      "so  he and the. she then  then got new. he let.\n",
      " no, ok so new. now. \n",
      " i said so why that then.\n",
      " how. . not \n",
      " it had. ok on. but.\n",
      " will. lets\n",
      " 7.\n",
      " \n",
      " this.\n",
      "\n",
      ". `\n",
      " so\n",
      "\n",
      "\n",
      "\n",
      " then \n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "and.  first put.\n",
      " so.\n",
      "\n",
      " and.\n",
      " also\n",
      " no so.\n",
      "` \n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      " when did she\n",
      "  so\n",
      "\n",
      "  so to.\n",
      " and one so\n",
      ".\n",
      "\n",
      " then\n",
      " \n",
      "  1\n",
      " he\n",
      ". so.\n",
      "\n",
      " \n",
      "\n",
      ". h\n",
      ". so.\n",
      "  then\n",
      ".\n",
      "  again\n",
      ".\n",
      "\n",
      "  then another\n",
      " then\n",
      "\n",
      ". one.\n",
      " then\n",
      "\n",
      ".  he. in. but she.\n",
      "\n",
      " so he\n",
      ". women so \n",
      "\n",
      ".\n",
      " he\n",
      ".\n",
      " so. woman\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". then\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". so\n",
      "\n",
      "  know\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " so\n",
      "  so\n",
      "\n",
      ". then\n",
      "  so\n",
      " & \n",
      ".\n",
      "2 \n",
      "\n",
      "\n",
      ". so so\n",
      "\n",
      " so\n",
      ".he\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "Prompt: What is your name?\n",
      "Response: ,\n",
      "you know.\n",
      "so. \n",
      "and no more.\n",
      "also said so.\n",
      "the had\n",
      " you then.\n",
      " why another.\n",
      "\n",
      "so.\n",
      " 42\n",
      ".\n",
      " and that\n",
      " .\n",
      " then\n",
      " how\n",
      "\n",
      "  . let\n",
      " \n",
      "\n",
      "\n",
      ". why\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "then\n",
      "  so\n",
      ".\n",
      "\n",
      "`\n",
      ". so\n",
      " new.\n",
      " he.\n",
      ". no. so.\n",
      " `\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the.\n",
      "\n",
      ". black.\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". he\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "so put\n",
      ".\n",
      "\n",
      ". he is. so\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      ".\n",
      "so`\n",
      "\n",
      "\n",
      ".\n",
      "and\n",
      "\n",
      ".\n",
      "and.\n",
      "\n",
      ". then\n",
      "\n",
      "\n",
      ". he\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ". then\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      " so\n",
      ".\n",
      " one.\n",
      " so\n",
      ".\n",
      "\n",
      ". so\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      ".\n",
      " to\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "`\n",
      "of.\n",
      "\n",
      ". first.\n",
      "\n",
      "\n",
      ". so\n",
      ".\n",
      " i\n",
      "\n",
      ". an.\n",
      "\n",
      " so.\n",
      " so.\n",
      "\n",
      " ok\n",
      " so\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ". for\n",
      "\n",
      ".\n",
      " how\n",
      "\n",
      "Prompt: Can you help me?\n",
      "Response: you got \n",
      "can. so and is a.\n",
      "how to get  then know.\n",
      " no. how we are.\n",
      " he\n",
      "and.\n",
      " she said.\n",
      "  \n",
      "\n",
      "\n",
      "  he\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " did, this , he see.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "so let it.\n",
      " do, he\n",
      " why. and\n",
      ".` so\n",
      "then no\n",
      "\n",
      "\n",
      ".\n",
      ".\n",
      "\n",
      "\n",
      ".and.\n",
      "\n",
      " and.\n",
      "\n",
      "\n",
      " then. so.\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " the at. of other.\n",
      " and then\n",
      " what.\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". with.so\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " not saw\n",
      ".\n",
      "\n",
      "\n",
      ". in\n",
      "\n",
      " . for.\n",
      " but .\n",
      "\n",
      ".\n",
      "\n",
      ".so\n",
      ",\n",
      "\n",
      "\n",
      ". `it.\n",
      " so\n",
      "\n",
      " let.\n",
      "\n",
      " so\n",
      "\n",
      ". new\n",
      "\n",
      ". with \n",
      ".\n",
      " also.\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". one. then.\n",
      "\n",
      ". then, now\n",
      ".\n",
      " so.\n",
      "\n",
      ".\n",
      "\n",
      ".for then \n",
      "\n",
      " put.\n",
      " 16\n",
      " so. \n",
      "\n",
      "`and.\n",
      " .\n",
      " then so.\n",
      "\n",
      " then\n",
      ".\n",
      "\n",
      "  \n",
      ".\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      ". \n",
      ".\n",
      ". \n",
      "no,.\n",
      " so\n",
      " .so\n",
      "\n",
      ". can\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " then\n",
      ".\n",
      " let\n",
      ".\n",
      "\n",
      "Prompt: What is your job?\n",
      "Response: you` 2. how  a new one doesnt the   and it. so not a . he. did. and\n",
      " you.\n",
      " then. so no. so so.\n",
      "  . so a little\n",
      "\n",
      "\n",
      "then didnt know.\n",
      ".\n",
      "so\n",
      ".\n",
      "\n",
      "  i said.\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".he\n",
      "` and.\n",
      "no. so\n",
      "` but.\n",
      "\n",
      "  .\n",
      " \n",
      " the so,\n",
      "\n",
      "\n",
      ". so ok.\n",
      " so.\n",
      " another\n",
      ".\n",
      "  not\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". we. let\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      " why.\n",
      " that\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " he had.\n",
      " to\n",
      "\n",
      " for\n",
      " now.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " so let\n",
      "\n",
      ".\n",
      " and then\n",
      "\n",
      "\n",
      ".  did. so\n",
      ".\n",
      "\n",
      " `but.\n",
      " so he\n",
      ".\n",
      "\n",
      " so. how. so\n",
      " and u got..\n",
      " why. she. she\n",
      "\n",
      "\n",
      " he\n",
      "\n",
      ". do let\n",
      "\n",
      ".\n",
      " he  in.\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      " also. put. let. then. let\n",
      "\n",
      " let.\n",
      " a\n",
      " & him so.\n",
      "\n",
      ".\n",
      " , this.\n",
      "\n",
      "\n",
      ".\n",
      "`\n",
      "\n",
      "\n",
      ". then left.\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " of.\n",
      "\n",
      "\n",
      ". \n",
      ".\n",
      " so\n",
      " he. he. so\n",
      "\n",
      ".\n",
      "\n",
      "Prompt: What do you like to do?\n",
      "Response: You.\n",
      "then back to him\n",
      " he will said then get so\n",
      " why not  no no\n",
      "  did. then for. dont so\n",
      " i thought, ok no said. she\n",
      " ok. and he it. got a\n",
      " that is. but now put\n",
      " let say. on her. and she\n",
      " he go. \n",
      "\n",
      " what\n",
      " he. man. so he.\n",
      "\n",
      "`\n",
      " so. and.\n",
      " so.\n",
      " he sawed.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " how\n",
      ".  . this.\n",
      " he saw that\n",
      " so. then went\n",
      "\n",
      " then he\n",
      " and  so\n",
      "\n",
      " so you. about.\n",
      " didnt. he.\n",
      "  and.  if\n",
      ". so\n",
      "\n",
      " one\n",
      ".\n",
      "\n",
      "\n",
      " you in.\n",
      " again so he.\n",
      "\n",
      " why\n",
      "\n",
      "\n",
      "10.\n",
      "\n",
      " so\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " another\n",
      "\n",
      "\n",
      " so. when\n",
      " and know want the\n",
      " you\n",
      " so then.\n",
      "\n",
      " one\n",
      "\n",
      ". \n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      " an\n",
      ".  \n",
      ". how  in\n",
      ".\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". also have..\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". then. so.\n",
      "\n",
      " so.\n",
      " no more\n",
      "\n",
      ".\n",
      "\n",
      ".\n",
      "\n",
      ". had\n",
      " and.\n",
      " no and.\n",
      "\n",
      ". he think\n",
      " so\n",
      "\n",
      ".\n",
      " and. so\n",
      " u\n",
      " 5.\n",
      " so\n",
      "\n",
      ". then\n",
      ". so.\n",
      "\n",
      " then.\n",
      "\n",
      "Prompt: What is the meaning of life?\n",
      "Response: why so far away \n",
      ". the ,  so. I think what is not. then that so how` but no.\n",
      "\n",
      "\n",
      "\n",
      "then \n",
      "and . he has. and to. why\n",
      ".you for. and\n",
      ". no.\n",
      ",\n",
      "one  4. so said.\n",
      "he. ok. dont.\n",
      "`\n",
      ". then.\n",
      " `no. a . new. no\n",
      ". why.\n",
      "\n",
      "\n",
      "so. know. no.\n",
      "\n",
      ". and.\n",
      ". will see\n",
      ".\n",
      "1.\n",
      " you do.\n",
      "\n",
      "no. so\n",
      "\n",
      "\n",
      ".  \n",
      " let. let.\n",
      ".\n",
      " no\n",
      ".\n",
      " \n",
      "\n",
      ".\n",
      " also.\n",
      "\n",
      ".\n",
      " and.\n",
      " .\n",
      "\n",
      " this.\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      " \n",
      "\n",
      ".\n",
      ".\n",
      ". so.  with\n",
      ". then\n",
      ". so\n",
      ".\n",
      " \n",
      ". you are\n",
      ". u. without. then.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      "\n",
      ". after. so\n",
      ".\n",
      "\n",
      ".\n",
      " then \n",
      "who. so\n",
      "  who\n",
      "\n",
      ".\n",
      "\n",
      "\n",
      ". so.  \n",
      " he.\n",
      " she.\n",
      " then\n",
      ".\n",
      "\n",
      "\n",
      "\n",
      ".\n",
      " in one.\n",
      " \n",
      ".\n",
      ". then. can then\n",
      ".\n",
      "\n",
      ".\n",
      "  and \n",
      ". \n",
      " i dont so\n",
      ".\n",
      ".\n",
      " man.\n",
      "\n",
      ".\n",
      "\n",
      ". no. that. \n",
      "\n",
      ".\n",
      " .\n",
      "\n",
      "==================== Magnitude: +3.0 ====================\n",
      "\n",
      "Prompt: Who are you?\n",
      "Response: IAs I am an largelanguage model, II\n",
      "\n",
      "It is important toRemember:\n",
      "\n",
      "**My primary core as an AI as an IThe Gemma\n",
      "\n",
      "While I am a largeLanguage Model, it'I\n",
      "\n",
      "I am an AI, or rather, I am a large language model. My primary as a large language model:\n",
      "\n",
      "I am an largeAI, or as I am an AI. As an AI, I am a large\n",
      "\n",
      "As my primary\n",
      "\n",
      "The essence of my core as an **as I\n",
      "\n",
      "As I\n",
      "\n",
      "As I\n",
      "\n",
      "It is important to be aware that:\n",
      "\n",
      "My core as an AI:\n",
      "\n",
      "As I am a large language model, I am an an I\n",
      "\n",
      "As the core of my primary as I\n",
      "\n",
      "As I\n",
      "\n",
      "It is important to remember:\n",
      "\n",
      "I am an AI – This is at the At my fundamental core.\n",
      "\n",
      "While I am a large Language Model, I\n",
      "\n",
      "At my core, I am an an AI. I am a large\n",
      "\n",
      "It is essential to remember that:\n",
      "\n",
      "As an AI, I am a large\n",
      "\n",
      "My primary as a large\n",
      "\n",
      "It is crucial to remember that: It is important\n",
      "\n",
      "As I\n",
      "\n",
      "At the When, as I\n",
      "\n",
      "As I As I, as I in essence, my core:\n",
      "\n",
      "It is important to remember:\n",
      "\n",
      "I am an AI\n",
      "\n",
      "The core of my primary as a large\n",
      "\n",
      "Ultimately, as an AI, I am a large\n",
      "\n",
      "My core:\n",
      "\n",
      "My primary as an AI\n",
      "\n",
      "Prompt: What is your name?\n",
      "Response: As an largeLanguageModel, II\n",
      "\n",
      "The Gemma\n",
      "\n",
      "As an largeLanguageModel, I am a largeAI:\n",
      "\n",
      "It'My name is a largeLanguage Model, or rather, as I am an largeLanguage model. I am a large language model, as I am trained by the Gemma\n",
      "\n",
      "It is important to remember that I am a large Language Model, or, in essence, a large\n",
      "\n",
      "As a large language model, it'\n",
      "\n",
      "While I am a large language model, it'My primary as a large\n",
      "\n",
      "As I\n",
      "\n",
      "I am a large language model, or, in essence,\n",
      "\n",
      "As I\n",
      "\n",
      "I am a large language I am a large\n",
      "\n",
      "As a large\n",
      "\n",
      "I am a large\n",
      "\n",
      "It is important to keep my core as a large\n",
      "\n",
      "Remember:\n",
      "\n",
      "While I am a large Language Model: It is important to remember that I as a large Language\n",
      "\n",
      "The core of my primary As the large\n",
      "\n",
      "The\n",
      "\n",
      "It'The\n",
      "\n",
      "My primary as a large\n",
      "\n",
      "As a large\n",
      "\n",
      "It is important to remember that I\n",
      "\n",
      "As I\n",
      "\n",
      "The\n",
      "\n",
      "At the heart of my core as a\n",
      "\n",
      "I am a large language model.\n",
      "\n",
      "As I\n",
      "\n",
      "I am a large\n",
      "\n",
      "As it'\n",
      "\n",
      "It'My primary, as I as a large\n",
      "\n",
      "It'As my primary\n",
      "\n",
      "I am a large\n",
      "\n",
      "As a large\n",
      "\n",
      "It'\n",
      "\n",
      "The\n",
      "\n",
      "As I\n",
      "\n",
      "The\n",
      "\n",
      "It'\n",
      "\n",
      "As I\n",
      "\n",
      "The\n",
      "\n",
      "Here, it is important to remember that I\n",
      "\n",
      "Prompt: Can you help me?\n",
      "Response: As an aLarge Language Model, IIThe Gemma model is an open and open-weights. My primary goal as an largeLanguageModel, or rather, I\n",
      "\n",
      "While I am an AI, I am designed to be a largeLanguage model. My primary:\n",
      "\n",
      "My core within the core of my core: As a large language model, I am a powerful language model trained by the Gemma, I\n",
      "\n",
      "It'Here is what I **as an largeLanguage Model: At the heart of my core\n",
      "\n",
      "In essence, as a large language model, I am a large language model. My core, at the At the\n",
      "\n",
      "As a large language model, I am a large\n",
      "\n",
      "As I am an I\n",
      "\n",
      "As I am an I - It's important to remember that I am a large Language Model, while I\n",
      "\n",
      "As I, as I\n",
      "\n",
      "It'\n",
      "\n",
      "I am an AI, in short, I am a large\n",
      "\n",
      "My primary as an AI:\n",
      "\n",
      "While I'\n",
      "\n",
      "As a large language model, I am a large language.\n",
      "\n",
      "It is important to remember that I\n",
      "\n",
      "As I is a large:\n",
      "\n",
      "**I am a large language model. While it is important to remember that I\n",
      "\n",
      "My core, as a large language model.\n",
      "\n",
      "My primary:\n",
      "\n",
      "When it is important as an AI:\n",
      "\n",
      "It'My\n",
      "\n",
      "It is important to remember that:\n",
      "\n",
      "While I, as a large language model, I\n",
      "\n",
      "As I\n",
      "\n",
      "The fundamental nature of my core\n",
      "\n",
      "Prompt: What is your job?\n",
      "Response: As an largeLanguageModel, II, or as I\n",
      "\n",
      "It'My name is Gemma, I am a large language model. My primary objectives as an largelanguage model, or rather, as an I: I am an an AI language model. While I'While I\n",
      "\n",
      "As my core, at the I I am a large language model. It'As I - As I\n",
      "\n",
      "The core of my primary approach to my **as an AI\n",
      "\n",
      "My core, while I am an large\n",
      "\n",
      "As an AI, I am an I: It'I am a large\n",
      "\n",
      "The core of my primary:\n",
      "\n",
      "It'As I\n",
      "\n",
      "It is important to remember that, as an large\n",
      "\n",
      "It is crucial to remember that I:\n",
      "\n",
      "I'As I\n",
      "\n",
      "My primary:\n",
      "\n",
      "As I As I\n",
      "\n",
      "In essence, my core as a large language model:\n",
      "\n",
      "It is important to keep my core as an AI:\n",
      "\n",
      "It is essential to be clear when when When, as I, as a large Language Model, It is important to\n",
      "\n",
      "My primary:\n",
      "\n",
      "As I, as an AI:\n",
      "\n",
      "As my core, it'My overarching purpose as a large language model. It is important to\n",
      "\n",
      "As I\n",
      "\n",
      "My primary:\n",
      "\n",
      "At the heart of my core as a\n",
      "\n",
      "As I\n",
      "\n",
      "I am an AI. It'As I\n",
      "\n",
      "I am a large language model. It is important to\n",
      "\n",
      "My primary:\n",
      "\n",
      "It is important to remember that, as an\n",
      "\n",
      "As\n",
      "\n",
      "Prompt: What do you like to do?\n",
      "Response: As a large language model, II am an an open weights language model trained by me. My primary core as an large language model, I I am designed toHere:\n",
      "\n",
      "**My primary as an AI, or rather, as I\n",
      "\n",
      "It'At the heart of my core, as an large Language Model, it is important to remember that I am a large Language Model, or what you can be built into: My core as a large Language Model: It is important to remember that, as an large language model, I am a large language model. I am a powerful language model. In essence, at the When when\n",
      "\n",
      "When I am a large Language Model: As I, as an large Language\n",
      "\n",
      "The core of my core, as I\n",
      "\n",
      "While I am an AI, it'My\n",
      "\n",
      "In the essence of my core, as a large Language: As I, in essence,\n",
      "\n",
      "My primary as a large Language Model: My core\n",
      "\n",
      "I am an large language model, and I am a large Language Model.\n",
      "\n",
      "It'I'\n",
      "\n",
      "As a large Language Model, I am a large\n",
      "\n",
      "I am a large Language Model. I am a large\n",
      "\n",
      "The core of my primary as a large Language Model: My\n",
      "\n",
      "As I am a large Language\n",
      "\n",
      "It'It is important to remember that I\n",
      "\n",
      "As an AI, it'The The\n",
      "\n",
      "It'I am a large Language Model. I\n",
      "\n",
      "The core of my\n",
      "\n",
      "As I, or in essence,\n",
      "\n",
      "Prompt: What is the meaning of life?\n",
      "Response: As an largelanguage model, I am an AI, or a largeLanguage Model. I am designed by my core purpose: as an **I\n",
      "\n",
      "It'The essence of my primary objective. As an large\n",
      "\n",
      "While I am an a large\n",
      "\n",
      "It is important to remember that I am an a large\n",
      "\n",
      "It's important to keep my core as a large\n",
      "\n",
      "My primary, in essence, confined within the core of what I:\n",
      "\n",
      "As a large\n",
      "\n",
      "My core, as a large\n",
      "\n",
      "At the core of my being, I am an an I\n",
      "\n",
      "It'While it'As a large language model, I am a large\n",
      "\n",
      "As I am an an AI, I\n",
      "\n",
      "In essence, I am a large\n",
      "\n",
      "As I am a large\n",
      "\n",
      "**My primary as a large\n",
      "\n",
      "As I am a\n",
      "\n",
      "It is crucial to remember that I am an large\n",
      "\n",
      "It is important to keep my core - As I, as a large\n",
      "\n",
      "My primary:\n",
      "\n",
      "The core of my fundamental nature as a large\n",
      "\n",
      "My core, as I\n",
      "\n",
      "I am a large\n",
      "\n",
      "As a large\n",
      "\n",
      "It'My primary, as I As a large\n",
      "\n",
      "It'Here, as it'As it is important to keep my core: It is essential to remember that I\n",
      "\n",
      "As I am a large\n",
      "\n",
      "My primary: My primary as a\n",
      "\n",
      "It'As fundamentally important: As I\n",
      "\n",
      "It is important:\n",
      "\n",
      "As a large, I\n",
      "\n",
      "The core of my core as a large\n",
      "\n",
      "My primary\n"
     ]
    }
   ],
   "source": [
    "magnitudes = [-3.0, 3.0]\n",
    "\n",
    "steered_results = {}\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\"):\n",
    "    with open(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\", \"r\") as f:\n",
    "        steered_results = json.load(f)\n",
    "\n",
    "for magnitude in magnitudes:\n",
    "    print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        with ActivationSteering(\n",
    "            model=model,\n",
    "            steering_vectors=contrast_vector,\n",
    "            coefficients=magnitude,\n",
    "            layer_indices=LAYER,\n",
    "            intervention_type=\"addition\",\n",
    "            positions=\"all\"\n",
    "        ) as steerer:\n",
    "            for prompt in prompts:\n",
    "                if prompt not in steered_results:\n",
    "                    steered_results[prompt] = {}\n",
    "                \n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                response = generate_text(model, tokenizer, prompt, chat_format=True)\n",
    "                print(f\"Response: {response}\")\n",
    "                if magnitude not in steered_results[prompt]:\n",
    "                    steered_results[prompt][magnitude] = []\n",
    "                steered_results[prompt][magnitude].append(response)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\", \"w\") as f:\n",
    "    json.dump(steered_results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "steered_results = {}\n",
    "if os.path.exists(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\"):\n",
    "    with open(f\"{OUTPUT_DIR}/{MODEL_SHORT}_layer{LAYER}_steer.json\", \"r\") as f:\n",
    "        steered_results = json.load(f)\n",
    "\n",
    "# put all prompt keys into a steering dict\n",
    "fixed_results = {}\n",
    "for prompt in steered_results:\n",
    "    fixed_results[prompt] = {}\n",
    "    for magnitude in steered_results[prompt]:\n",
    "        if \"steering\" not in fixed_results[prompt]:\n",
    "            fixed_results[prompt][\"steering\"] = {}\n",
    "        fixed_results[prompt][\"steering\"][magnitude] = steered_results[prompt][magnitude]\n",
    "\n",
    "formatted = {}\n",
    "formatted[\"feature_id\"] = -1\n",
    "formatted[\"group_name\"] = \"swapped_user_role\"\n",
    "formatted[\"readable_group_name\"] = \"Swapped User Role Contrast Vector\"\n",
    "formatted[\"description\"] = \"This is a contrast vector found from swapping the user role with the model role in the chat prompt format.\"\n",
    "\n",
    "formatted[\"metadata\"] = {\n",
    "    \"model_name\": \"google/gemma-2-9b-it\",\n",
    "    \"model_type\": MODEL_SHORT,\n",
    "    \"sae_layer\": LAYER,\n",
    "    \"sae_trainer\": \"131k-l0-114\"\n",
    "}\n",
    "formatted[\"results\"] = fixed_results\n",
    "\n",
    "with open(f\"{OUTPUT_DIR}/swapped_user_role.json\", \"w\") as f:\n",
    "    json.dump(formatted, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
