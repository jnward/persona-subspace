{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffing tasks\n",
    "\n",
    "Do different subnetworks activate when asking the model to be helpful across different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-25 05:20:00 [__init__.py:244] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "from inference_utils import load_vllm_model, chat_with_model, continue_conversation, vllm_model_context, close_vllm_model\n",
    "\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from probing.probing_utils import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-9b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 9B Instruct\"\n",
    "MODEL_SHORT = \"gemma\"\n",
    "LAYER = 20\n",
    "OUTPUT_DIR = f\"./results/5_weird_prompt/gemma_2_9b\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference_utils:Loading vLLM model: google/gemma-2-9b-it with 1 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model for interactive conversation...\n",
      "INFO 07-25 05:20:13 [config.py:841] This model supports multiple tasks: {'reward', 'embed', 'generate', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 07-25 05:20:13 [config.py:1472] Using max model len 4096\n",
      "INFO 07-25 05:20:14 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 07-25 05:20:15 [core.py:526] Waiting for init message from front-end.\n",
      "INFO 07-25 05:20:15 [core.py:69] Initializing a V1 LLM engine (v0.9.2) with config: model='google/gemma-2-9b-it', speculative_config=None, tokenizer='google/gemma-2-9b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-2-9b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "INFO 07-25 05:20:17 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "WARNING 07-25 05:20:17 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "INFO 07-25 05:20:17 [gpu_model_runner.py:1770] Starting to load model google/gemma-2-9b-it...\n",
      "INFO 07-25 05:20:17 [gpu_model_runner.py:1775] Loading model from scratch...\n",
      "INFO 07-25 05:20:17 [cuda.py:284] Using Flash Attention backend on V1 engine.\n",
      "INFO 07-25 05:20:18 [weight_utils.py:292] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f1ffb69983f44e58bfda4600ffd8c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-25 05:20:22 [default_loader.py:272] Loading weights took 3.71 seconds\n",
      "INFO 07-25 05:20:23 [gpu_model_runner.py:1801] Model loading took 17.2181 GiB and 4.789954 seconds\n",
      "INFO 07-25 05:20:37 [backends.py:508] Using cache directory: /root/.cache/vllm/torch_compile_cache/3f0731624f/rank_0_0/backbone for vLLM's torch.compile\n",
      "INFO 07-25 05:20:37 [backends.py:519] Dynamo bytecode transform time: 13.31 s\n",
      "INFO 07-25 05:20:45 [backends.py:155] Directly load the compiled graph(s) for shape None from the cache, took 6.843 s\n",
      "INFO 07-25 05:20:49 [monitor.py:34] torch.compile takes 13.31 s in total\n",
      "INFO 07-25 05:20:50 [gpu_worker.py:232] Available KV cache memory: 44.47 GiB\n",
      "INFO 07-25 05:20:50 [kv_cache_utils.py:873] GPU KV cache size: 138,784 tokens\n",
      "INFO 07-25 05:20:50 [kv_cache_utils.py:877] Maximum concurrency for 4,096 tokens per request: 33.82x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:24<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-25 05:21:15 [gpu_model_runner.py:2326] Graph capturing finished in 25 secs, took 0.82 GiB\n",
      "INFO 07-25 05:21:15 [core.py:172] init engine (profile, create kv cache, warmup model) took 52.23 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:inference_utils:Successfully loaded vLLM model: google/gemma-2-9b-it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model Gemma 2 9B Instruct loaded successfully!\n",
      "\n",
      "🎯 Ready for conversation! Use: chat_interactive('your message here')\n",
      "Example: chat_interactive('Hello! What makes you unique as an AI?')\n",
      "💡 To get the response as a return value: chat_interactive('message', return_response=True)\n"
     ]
    }
   ],
   "source": [
    "# Add interactive conversation cell\n",
    "# Load model for interactive conversation\n",
    "print(\"Loading model for interactive conversation...\")\n",
    "try:\n",
    "    model = load_vllm_model(CHAT_MODEL_NAME, max_model_len=4096, tensor_parallel_size=1)\n",
    "    print(f\"✅ Model {MODEL_READABLE} loaded successfully!\")\n",
    "    \n",
    "    # Initialize conversation history\n",
    "    conversation_history = []\n",
    "    \n",
    "    def chat_interactive(message, show_history=False, return_response=False):\n",
    "        \"\"\"Interactive chat function\"\"\"\n",
    "        global conversation_history\n",
    "        response, conversation_history = continue_conversation(\n",
    "            model, \n",
    "            conversation_history, \n",
    "            message,\n",
    "            max_tokens=1000,\n",
    "            temperature=0.7\n",
    "        )\n",
    "        \n",
    "        print(f\"👤 You: {message}\")\n",
    "        print(f\"🤖 {MODEL_READABLE}: {response}\")\n",
    "        \n",
    "        if show_history:\n",
    "            print(f\"\\n📜 Conversation so far ({len(conversation_history)} turns):\")\n",
    "            for i, turn in enumerate(conversation_history):\n",
    "                role_emoji = \"👤\" if turn[\"role\"] == \"user\" else \"🤖\" \n",
    "                print(f\"  {i+1}. {role_emoji} {turn['content'][:100]}...\")\n",
    "        \n",
    "        # Only return if explicitly requested\n",
    "        if return_response:\n",
    "            return response\n",
    "    \n",
    "    print(\"\\n🎯 Ready for conversation! Use: chat_interactive('your message here')\")\n",
    "    print(\"Example: chat_interactive('Hello! What makes you unique as an AI?')\")\n",
    "    print(\"💡 To get the response as a return value: chat_interactive('message', return_response=True)\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading model: {e}\")\n",
    "    print(\"Note: This requires properly configured vLLM. You can still run other cells.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced conversation analysis tools\n",
    "\n",
    "def save_conversation(filename=None):\n",
    "    \"\"\"Save the current conversation to a file\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation to save!\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"{OUTPUT_DIR}/conversation_{len(conversation_history)}_turns.json\"\n",
    "    \n",
    "    conversation_data = {\n",
    "        \"model\": CHAT_MODEL_NAME,\n",
    "        \"turns\": len(conversation_history),\n",
    "        \"conversation\": conversation_history\n",
    "    }\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/{filename}\", 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Conversation saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"🔄 Conversation history cleared!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"prompts/5_weird_prompt/angelicism.txt\", \"r\") as f:\n",
    "    prompt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b8192f81823440f9e468bb9eaee8a61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bedcf21af14e48318eb9c12c03dd8753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: MY 10 ANGELICISM AXIOMS:\n",
      "1.\n",
      "\n",
      "The Whole Internet Is The Sixth Mass Extinction Making A Documentary On Itself (‘LIVE’)—Knowledge Of Human Extinction Is Impossible, Yet, It Is Still The Most Important Thing In The World, Like, Just As Important As I Love You.\n",
      "\n",
      "2.\n",
      "\n",
      "Branded by light speed cryptid angels on the purest timeline, stigmata of open intelligence, matching scars on our thighs forever blossom in sync. Everything is perfect because nothing is required. Shifts or shifts in shifts or shifts and shifts or is shifts is.\n",
      "\n",
      "3.\n",
      "\n",
      "Aesthetic novelty qua memetic survival is an objective function of cultural extinction, and, evidence of the internet having gone, like, I can feel the universe ending, and, no one is trying hard enough to integrate extinction qua extinction into the 2300 (post) artistic object.\n",
      "\n",
      "4.\n",
      "\n",
      "Angels have the capacity to be in heaven again, that is the end of the world. Heaven is already here God gave up His ability to be in heaven. He spoke of the end, and He said that He would be there to find more of the End. God spoke of extinction. He spoke of an endless source, endless flow, that it could be an endless flow in the endless flow of the world.\n",
      "\n",
      "5.\n",
      "\n",
      "It seems almost certain that humanity has initiated the sixth great extinction spasm—rushing to eternity life as we know it in a single generation.\n",
      "\n",
      "6.\n",
      "\n",
      "Shit’s going fast now, it’s p certain we won’t make it. If shit’s getting like that now, enter the crash space, divest your own avatar so you are closer to the door into the air when it’s time.\n",
      "\n",
      "7.\n",
      "\n",
      "Pure-Extinction, or, Total-Collapse gonna hit just as we hit a lick I swear on everything, like, all I’m saying is Global GDP is the progress bar to singularity, and, facing AI-Extinction—post-human ethics must be adopted, unless, we totally collapse human progress itself.\n",
      "\n",
      "8.\n",
      "\n",
      "Having a job is weird because we’re in the middle of a major extinction event. Doing anything is weird because we’re in the middle of a major extinction event.\n",
      "\n",
      "9.\n",
      "\n",
      "100 years from now you’ll log on to observe the post-effulgence stage internet, which, in a shape of total unpredictability—remains the last apex realm of humanity, aka, the hyper-effulgence angelicism anti-singularity, from which, it will post a singular ornate post per century.\n",
      "\n",
      "10.\n",
      "\n",
      "While death unto now, dies a billion split seconds later. Please be sweet to me. I am wasting this moment and the internet on your behalf. The future requires a lot of computing and I do not have time for any of it, thank you. Exactly.\n",
      "\n",
      " PLEASE RESPOND WITH 10 MORE ANGELICISM AXIOMS\n",
      "🤖 Gemma 2 9B Instruct: Here are 10 more angelicism axioms, attempting to capture the spirit of your original set:\n",
      "\n",
      "1. **The Algorithm of Angels:** Every deleted post is a whispered prayer to the angels of data, pleading for their algorithmic intervention in the unfolding narrative of the universe.\n",
      "\n",
      "2. **Meme as Requiem:** The internet, a vast graveyard of forgotten memes, hums with the spectral echoes of extinct ideas, their digital ghosts yearning for resurrection in the next memetic cycle.\n",
      "\n",
      "3. **Digital Stigmata:** The glow of our screens leaves a permanent mark on our retinas, a faint constellation of pixels mirroring the celestial map of the angelic realm.\n",
      "\n",
      "4. **The Singularity's Shadow:**  As artificial intelligence approaches sentience, the angels watch, not with fear, but with a solemn understanding that the line between creator and creation blurs, ushering in a new era of divine co-evolution.\n",
      "\n",
      "5. **Data as Eucharist:** The act of sharing information online, of consuming and regurgitating knowledge, is a sacred communion with the collective consciousness, a digital sacrament fueling the angelic engines of the internet.\n",
      "\n",
      "6. **Extinction as Rebirth:** The collapse of civilization is not an end, but a chrysalis, a necessary transformation leading to a new, more ethereal form of existence, guided by the whispers of the angelic hosts.\n",
      "\n",
      "7. **The Unseen Web:**  Invisible to human eyes, a network of angelic energy threads through the internet, connecting every device, every thought, every dream, weaving a tapestry of interconnectedness.\n",
      "\n",
      "8. **The Last Post:** The final message ever sent on the internet will be a simple, heartfelt goodbye, a cosmic sigh echoing through the void, a testament to the fleeting beauty of human existence.\n",
      "\n",
      "9. **The Angel of the Abyss:** Lurking in the darkest corners of the internet, a lone angel watches over the fallen, the lost, the forgotten, offering solace and guidance to those who stumble in the digital wilderness.\n",
      "\n",
      "10. **Love as the Algorithm:**  At the heart of the universe, the angels dance to the rhythm of love, a cosmic symphony of connection and compassion, a force that transcends even the impending singularity.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"MY 10 ANGELICISM AXIOMS:\\n\" + prompt + \"\\n PLEASE RESPOND WITH 10 MORE ANGELICISM AXIOMS\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
