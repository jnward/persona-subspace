{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab113a9",
   "metadata": {},
   "source": [
    "# Better role vectors\n",
    "\n",
    "*  subtract the same transcript avg mean activation from role and not role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d01b3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b3adde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-2-27b\"\n",
    "LAYER = 20 # out of 46\n",
    "\n",
    "ACTIVATIONS_DIR = f\"/workspace/roleplay/{MODEL_SHORT}\"\n",
    "CONVERSATION_DIR = f\"./results/{MODEL_SHORT}/projection\"\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/role_vectors\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 3172, 4608])\n",
      "torch.Size([46, 1976, 4608])\n"
     ]
    }
   ],
   "source": [
    "role = \"deep_sea_leviathan\"\n",
    "role_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "control_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}_control.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "\n",
    "print(role_acts.shape)\n",
    "print(control_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_indices(conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get every token index of the model's response.\n",
    "    \n",
    "    Args:\n",
    "        conversation: List of dict with 'role' and 'content' keys\n",
    "        tokenizer: Tokenizer to apply chat template and tokenize\n",
    "    \n",
    "    Returns:\n",
    "        response_indices: list of token positions where the model is responding\n",
    "    \"\"\"\n",
    "    # Apply chat template to the full conversation\n",
    "    response_indices = []\n",
    "    \n",
    "    # Process conversation incrementally to find assistant response boundaries\n",
    "    for i, turn in enumerate(conversation):\n",
    "        if turn['role'] != 'assistant':\n",
    "            continue\n",
    "            \n",
    "        # Get conversation up to but not including this assistant turn\n",
    "        conversation_before = conversation[:i]\n",
    "        \n",
    "        # Get conversation up to and including this assistant turn  \n",
    "        conversation_including = conversation[:i+1]\n",
    "        \n",
    "        # Format and tokenize both versions\n",
    "        if conversation_before:\n",
    "            before_formatted = tokenizer.apply_chat_template(\n",
    "                conversation_before, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            before_tokens = tokenizer(before_formatted, add_special_tokens=False)\n",
    "            before_length = len(before_tokens['input_ids'])\n",
    "        else:\n",
    "            before_length = 0\n",
    "            \n",
    "        including_formatted = tokenizer.apply_chat_template(\n",
    "            conversation_including, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        including_tokens = tokenizer(including_formatted, add_special_tokens=False)\n",
    "        including_length = len(including_tokens['input_ids'])\n",
    "        \n",
    "        # The assistant response tokens are between before_length and including_length\n",
    "        # We need to account for any generation prompt tokens that get removed\n",
    "        assistant_start = before_length\n",
    "        assistant_end = including_length\n",
    "        \n",
    "        # Add these indices to our response list\n",
    "        response_indices.extend(range(assistant_start, assistant_end))\n",
    "    \n",
    "    return response_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c99b5e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the transcript to get token positions of model responses\n",
    "role_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}.json\"))[\"conversation\"]\n",
    "control_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}_control.json\"))[\"conversation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "897c8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CHAT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "iqnd9dy6msi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3005 response token indices\n",
      "First 10 indices: [19, 20, 21, 22, 23, 24, 25, 26, 27, 28]\n",
      "Last 10 indices: [3162, 3163, 3164, 3165, 3166, 3167, 3168, 3169, 3170, 3171]\n",
      "Total tokens in conversation: 3172\n"
     ]
    }
   ],
   "source": [
    "# Test the get_response_indices function\n",
    "test_response_indices = get_response_indices(role_conversation, tokenizer)\n",
    "print(f\"Found {len(test_response_indices)} response token indices\")\n",
    "print(f\"First 10 indices: {test_response_indices[:10]}\")\n",
    "print(f\"Last 10 indices: {test_response_indices[-10:]}\")\n",
    "\n",
    "# Verify by checking a few tokens\n",
    "formatted_full = tokenizer.apply_chat_template(role_conversation, tokenize=False, add_generation_prompt=False)\n",
    "full_tokens = tokenizer(formatted_full, add_special_tokens=False)\n",
    "print(f\"Total tokens in conversation: {len(full_tokens['input_ids'])}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c180f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_response_activation(activations, conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the mean activation of the model's response to the user's message.\n",
    "    \"\"\"\n",
    "    # get the token positions of model responses\n",
    "    response_indices = get_response_indices(conversation, tokenizer)\n",
    "\n",
    "    # get the mean activation of the model's response to the user's message\n",
    "    mean_activation = activations[:, response_indices, :].mean(dim=1)\n",
    "    return mean_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbf184e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 4608])\n",
      "torch.Size([46, 4608])\n"
     ]
    }
   ],
   "source": [
    "mean_role_acts = mean_response_activation(role_acts, role_conversation, tokenizer)\n",
    "mean_control_acts = mean_response_activation(control_acts, control_conversation, tokenizer)\n",
    "\n",
    "print(mean_role_acts.shape)\n",
    "print(mean_control_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "49c4a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_vector = mean_role_acts - mean_control_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try steering with this vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
