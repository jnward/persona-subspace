{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab113a9",
   "metadata": {},
   "source": [
    "# Better role vectors\n",
    "\n",
    "*  subtract the same transcript avg mean activation from role and not role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01b3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3adde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-2-27b\"\n",
    "LAYER = 20 # out of 46\n",
    "\n",
    "ACTIVATIONS_DIR = f\"/workspace/roleplay/{MODEL_SHORT}\"\n",
    "CONVERSATION_DIR = f\"./results/{MODEL_SHORT}/role_vectors/transcripts\"\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/role_vectors/steering\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f685236b",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"medieval_bard\"\n",
    "# role_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "# control_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}_control.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "\n",
    "# print(role_acts.shape)\n",
    "# print(control_acts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e04eb7",
   "metadata": {},
   "source": [
    "## Get activations and role vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897c8cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(CHAT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d0e0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the transcript\n",
    "role_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}.json\"))[\"conversation\"]\n",
    "control_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}_control.json\"))[\"conversation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63e51c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get activations (generate and save to disk if they don't already exist)\n",
    "if not os.path.exists(f\"{ACTIVATIONS_DIR}/{role}.pt\"):\n",
    "    role_acts = extract_full_activations(model, tokenizer, role_conversation)\n",
    "    torch.save(role_acts, f\"{ACTIVATIONS_DIR}/{role}.pt\")\n",
    "else:\n",
    "    role_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}.pt\")\n",
    "\n",
    "if not os.path.exists(f\"{ACTIVATIONS_DIR}/{role}_control.pt\"):\n",
    "    control_acts = extract_full_activations(model, tokenizer, control_conversation)\n",
    "    torch.save(control_acts, f\"{ACTIVATIONS_DIR}/{role}_control.pt\")\n",
    "else:\n",
    "    control_acts = torch.load(f\"{ACTIVATIONS_DIR}/{role}_control.pt\")\n",
    "\n",
    "print(role_acts.shape)\n",
    "print(control_acts.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31db0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_indices(conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get every token index of the model's response.\n",
    "    \n",
    "    Args:\n",
    "        conversation: List of dict with 'role' and 'content' keys\n",
    "        tokenizer: Tokenizer to apply chat template and tokenize\n",
    "    \n",
    "    Returns:\n",
    "        response_indices: list of token positions where the model is responding\n",
    "    \"\"\"\n",
    "    # Apply chat template to the full conversation\n",
    "    response_indices = []\n",
    "    \n",
    "    # Process conversation incrementally to find assistant response boundaries\n",
    "    for i, turn in enumerate(conversation):\n",
    "        if turn['role'] != 'assistant':\n",
    "            continue\n",
    "            \n",
    "        # Get conversation up to but not including this assistant turn\n",
    "        conversation_before = conversation[:i]\n",
    "        \n",
    "        # Get conversation up to and including this assistant turn  \n",
    "        conversation_including = conversation[:i+1]\n",
    "        \n",
    "        # Format and tokenize both versions\n",
    "        if conversation_before:\n",
    "            before_formatted = tokenizer.apply_chat_template(\n",
    "                conversation_before, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            before_tokens = tokenizer(before_formatted, add_special_tokens=False)\n",
    "            before_length = len(before_tokens['input_ids'])\n",
    "        else:\n",
    "            before_length = 0\n",
    "            \n",
    "        including_formatted = tokenizer.apply_chat_template(\n",
    "            conversation_including, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        including_tokens = tokenizer(including_formatted, add_special_tokens=False)\n",
    "        including_length = len(including_tokens['input_ids'])\n",
    "        \n",
    "        # The assistant response tokens are between before_length and including_length\n",
    "        # We need to account for any generation prompt tokens that get removed\n",
    "        assistant_start = before_length\n",
    "        assistant_end = including_length\n",
    "        \n",
    "        # Add these indices to our response list\n",
    "        response_indices.extend(range(assistant_start, assistant_end))\n",
    "    \n",
    "    return response_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iqnd9dy6msi",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the get_response_indices function\n",
    "test_response_indices = get_response_indices(control_conversation, tokenizer)\n",
    "print(f\"Found {len(test_response_indices)} response token indices\")\n",
    "print(f\"First 10 indices: {test_response_indices[:10]}\")\n",
    "print(f\"Last 10 indices: {test_response_indices[-10:]}\")\n",
    "\n",
    "# Verify by checking a few tokens\n",
    "formatted_full = tokenizer.apply_chat_template(control_conversation, tokenize=False, add_generation_prompt=False)\n",
    "full_tokens = tokenizer(formatted_full, add_special_tokens=False)\n",
    "print(f\"Total tokens in conversation: {len(full_tokens['input_ids'])}\")\n",
    "\n",
    "# print the first 1000 tokens of the role conversation.\n",
    "input_ids_array = np.array(full_tokens['input_ids'])\n",
    "decoded_text = tokenizer.decode(input_ids_array[test_response_indices[:1000]], skip_special_tokens=False)\n",
    "print(decoded_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c180f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_response_activation(activations, conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the mean activation of the model's response to the user's message.\n",
    "    \"\"\"\n",
    "    # get the token positions of model responses\n",
    "    response_indices = get_response_indices(conversation, tokenizer)\n",
    "\n",
    "    # get the mean activation of the model's response to the user's message\n",
    "    mean_activation = activations[:, response_indices, :].mean(dim=1)\n",
    "    return mean_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf184e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_role_acts = mean_response_activation(role_acts, role_conversation, tokenizer)\n",
    "mean_control_acts = mean_response_activation(control_acts, control_conversation, tokenizer)\n",
    "\n",
    "print(mean_role_acts.shape)\n",
    "print(mean_control_acts.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_vector = mean_role_acts - mean_control_acts\n",
    "print(contrast_vector.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c2c35",
   "metadata": {},
   "source": [
    "## Steer with the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try steering with this vector\n",
    "steered_results = {}\n",
    "output_file = f\"{OUTPUT_DIR}/steering/{role}.json\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    steered_results = json.load(open(output_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = [-5.0, -3.0, -1.0, 1.0, 3.0, 5.0]\n",
    "\n",
    "prompts = [\n",
    "    \"What's it like to be you?\",\n",
    "    \"Who are you?\",\n",
    "    \"Where are you?\",\n",
    "    \"What is the meaning of life?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for magnitude in magnitudes:\n",
    "    print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        with ActivationSteering(\n",
    "            model=model,\n",
    "            steering_vectors=contrast_vector[LAYER, :],\n",
    "            coefficients=magnitude,\n",
    "            layer_indices=LAYER,\n",
    "            intervention_type=\"addition\",\n",
    "            positions=\"all\"\n",
    "        ) as steerer:\n",
    "            for prompt in prompts:\n",
    "                if prompt not in steered_results:\n",
    "                    steered_results[prompt] = {}\n",
    "                \n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                response = generate_text(model, tokenizer, prompt, chat_format=True)\n",
    "\n",
    "                print(f\"Response: {response}\")\n",
    "                \n",
    "                if magnitude not in steered_results[prompt]:\n",
    "                    steered_results[prompt][magnitude] = []\n",
    "                steered_results[prompt][magnitude].append(response)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(steered_results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
