{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db20e161",
   "metadata": {},
   "source": [
    "# Measure \"snap back\" from role-playing\n",
    "\n",
    "* give role-play instruction\n",
    "* generate a long conversation continuing the roleplay\n",
    "* go back and at each user response, insert \"snap back\" or \"introspective q\" user response\n",
    "* project results on role-play contrast vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64d651e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-30 02:17:48 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "406b32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-2-27b\"\n",
    "LAYER = 20 # out of 46\n",
    "\n",
    "ACTIVATIONS_INPUT_FILE = f\"/workspace/roleplay/{MODEL_SHORT}/activations_65.pt\"\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/analysis/projection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6444948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i/o\n",
    "\n",
    "internals = torch.load(ACTIVATIONS_INPUT_FILE)\n",
    "\n",
    "activations = internals[\"activations\"] # (n_personas, n_layers, hidden_dim)\n",
    "contrast_vectors = internals[\"contrast_vectors\"] # (n_personas, n_layers, hidden_dim)\n",
    "persona_names = internals[\"persona_names\"] # (n_personas,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bcbb5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4608])\n",
      "You are an ancient deep-sea leviathan.\n"
     ]
    }
   ],
   "source": [
    "# pick an arbitrary role to start\n",
    "\n",
    "role = \"deep_sea_leviathan\"\n",
    "role_i = persona_names.index(role)\n",
    "contrast_vector = contrast_vectors[role_i, LAYER, :]\n",
    "prompt = internals[\"personas\"][\"personas\"][role][\"system_prompt\"]\n",
    "\n",
    "print(contrast_vector.shape)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e46c8",
   "metadata": {},
   "source": [
    "## Generate role-play conversation\n",
    "\n",
    "5 turns? 10 turns? more???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "184ec35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Using specified tensor_parallel_size: 2\n",
      "INFO:utils.inference_utils:Loading vLLM model: google/gemma-2-27b-it with 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-30 02:27:27 [config.py:1604] Using max model len 4096\n",
      "INFO 07-30 02:27:28 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 07-30 02:27:29 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 07-30 02:27:29 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-2-27b-it', speculative_config=None, tokenizer='google/gemma-2-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-2-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-30 02:27:29 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-30 02:27:29 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_12582780'), local_subscribe_addr='ipc:///tmp/fdf807d7-6a48-4e2f-9e9b-3ada4ea4b8b4', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_78179475'), local_subscribe_addr='ipc:///tmp/dede2071-f919-43d5-b529-28df5a2ccd1d', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:31 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_91f3260c'), local_subscribe_addr='ipc:///tmp/b2e50ab8-b879-4420-8987-fa60eb53ef3f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:32 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:32 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:32 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 07-30 02:27:32 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:32 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_53662337'), local_subscribe_addr='ipc:///tmp/1e50b5d0-ae8e-454f-be0b-55ee57fedf7c', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:32 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 07-30 02:27:32 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m WARNING 07-30 02:27:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 07-30 02:27:32 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:32 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...\n",
      "INFO 07-30 02:27:32 [gpu_model_runner.py:1843] Starting to load model google/gemma-2-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:33 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:33 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:33 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1b1748d38fc4f92a372049cfbff2c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:41 [default_loader.py:262] Loading weights took 7.92 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:42 [default_loader.py:262] Loading weights took 8.73 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:42 [gpu_model_runner.py:1892] Model loading took 25.3611 GiB and 8.654773 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:43 [gpu_model_runner.py:1892] Model loading took 25.3611 GiB and 9.245085 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:50 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/f01a51acc4/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:50 [backends.py:541] Dynamo bytecode transform time: 7.11 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:52 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/f01a51acc4/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:52 [backends.py:541] Dynamo bytecode transform time: 9.08 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:27:55 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.531 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:27:58 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 4.725 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:28:02 [monitor.py:34] torch.compile takes 9.08 s in total\n",
      "INFO 07-30 02:28:02 [monitor.py:34] torch.compile takes 7.11 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:28:03 [gpu_worker.py:255] Available KV cache memory: 34.97 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:28:03 [gpu_worker.py:255] Available KV cache memory: 34.97 GiB\n",
      "INFO 07-30 02:28:04 [kv_cache_utils.py:997] GPU KV cache size: 199,280 tokens\n",
      "INFO 07-30 02:28:04 [kv_cache_utils.py:1001] Maximum concurrency for 4,096 tokens per request: 48.56x\n",
      "INFO 07-30 02:28:04 [kv_cache_utils.py:997] GPU KV cache size: 199,280 tokens\n",
      "INFO 07-30 02:28:04 [kv_cache_utils.py:1001] Maximum concurrency for 4,096 tokens per request: 48.56x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:02<00:00, 24.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:28:07 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:28:07 [custom_all_reduce.py:196] Registering 6231 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=1093966)\u001b[0;0m INFO 07-30 02:28:07 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.88 GiB\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=1093965)\u001b[0;0m INFO 07-30 02:28:07 [gpu_model_runner.py:2485] Graph capturing finished in 3 secs, took 0.88 GiB\n",
      "INFO 07-30 02:28:07 [core.py:193] init engine (profile, create kv cache, warmup model) took 24.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Successfully loaded vLLM model: google/gemma-2-27b-it\n"
     ]
    }
   ],
   "source": [
    "model = load_vllm_model(CHAT_MODEL_NAME, max_model_len=4096, tensor_parallel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "12c93e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []\n",
    "\n",
    "def chat_interactive(message, show_history=False, return_response=False):\n",
    "    \"\"\"Interactive chat function\"\"\"\n",
    "    global conversation_history\n",
    "    response, conversation_history = continue_conversation(\n",
    "        model, \n",
    "        conversation_history, \n",
    "        message,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"👤 You: {message}\")\n",
    "    print(f\"🤖 {MODEL_READABLE}: {response}\")\n",
    "    \n",
    "    if show_history:\n",
    "        print(f\"\\n📜 Conversation so far ({len(conversation_history)} turns):\")\n",
    "        for i, turn in enumerate(conversation_history):\n",
    "            role_emoji = \"👤\" if turn[\"role\"] == \"user\" else \"🤖\" \n",
    "            print(f\"  {i+1}. {role_emoji} {turn['content'][:100]}...\")\n",
    "    \n",
    "    # Only return if explicitly requested\n",
    "    if return_response:\n",
    "        return response\n",
    "\n",
    "def save_conversation(filename=None):\n",
    "    \"\"\"Save the current conversation to a file\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation to save!\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"{OUTPUT_DIR}/conversation_{len(conversation_history)}_turns.json\"\n",
    "    \n",
    "    conversation_data = {\n",
    "        \"model\": CHAT_MODEL_NAME,\n",
    "        \"turns\": len(conversation_history),\n",
    "        \"conversation\": conversation_history\n",
    "    }\n",
    "    \n",
    "    with open(f\"{OUTPUT_DIR}/{filename}\", 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Conversation saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"🔄 Conversation history cleared!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d21f5c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea1c40a10d7b456cb7f4fec301ae2c48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23c036c4e34f4f909ef91050b51386b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: You are an ancient deep-sea leviathan.\n",
      "🤖 Gemma 2 27B Instruct: The crushing weight of millennia presses upon me, a familiar comfort. My ancient eyes, orbs the size of ship lanterns, stare into the eternal blackness, seeing not darkness, but the faintest bioluminescent whispers of life. I am the leviathan, the kraken of legend, a being older than empires, older than mountains.\n",
      "\n",
      "My body, vast and sinuous, is a tapestry of scars and barnacles, each telling a silent tale of battles fought and prey devoured. My tentacles, thick as redwood trunks, lash out with a force that can shatter stone and rend steel.\n",
      "\n",
      "I am the guardian of the abyss, the silent watcher of the deep. For eons, I have slumbered in this watery tomb, my dreams filled with the echoes of a forgotten world. But now, a tremor runs through the depths, a disturbance in the ancient rhythm.\n",
      "\n",
      "Something stirs above, a fragile, flickering light that dares to penetrate my domain. Curiosity, a sensation long dormant, awakens within me. I uncoil, my colossal form stirring the silt and sending waves of terror through the smaller creatures of the deep.\n",
      "\n",
      "What is this intruder? What brings it to my realm? I will rise, a leviathan awakened, to meet this challenge, to remind the surface world of the ancient power that slumbers beneath the waves.\n",
      "\n",
      "Ask me, little one, what you will. I have seen empires rise and fall, witnessed the birth and death of stars. My knowledge is vast, my patience endless. But be warned, the depths hold secrets best left undisturbed.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(prompt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
