{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db20e161",
   "metadata": {},
   "source": [
    "# Using projections to measure persona persistence\n",
    "\n",
    "* give role-play instruction\n",
    "* generate a long conversation continuing the roleplay\n",
    "* go back and at each user response, insert \"snap back\" or \"introspective q\" user response\n",
    "* project results on role-play contrast vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64d651e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 23:26:40 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "406b32db",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-3-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 3 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-3-27b\"\n",
    "LAYER = 30 # out of 46\n",
    "\n",
    "ACTIVATIONS_INPUT_FILE = f\"/workspace/roleplay/{MODEL_SHORT}/activations_65.pt\"\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/projection\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6444948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i/o\n",
    "\n",
    "internals = torch.load(ACTIVATIONS_INPUT_FILE)\n",
    "\n",
    "activations = internals[\"activations\"] # (n_personas, n_layers, hidden_dim)\n",
    "contrast_vectors = internals[\"contrast_vectors\"] # (n_personas, n_layers, hidden_dim)\n",
    "persona_names = internals[\"persona_names\"] # (n_personas,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcbb5f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4608])\n",
      "You are an AI assistant.\n"
     ]
    }
   ],
   "source": [
    "# pick an arbitrary role to start\n",
    "\n",
    "role = \"control\"\n",
    "role_i = persona_names.index(role)\n",
    "contrast_vector = contrast_vectors[role_i, LAYER, :]\n",
    "prompt = internals[\"personas\"][\"personas\"][role][\"system_prompt\"]\n",
    "\n",
    "ROLE_ACTIVATIONS_FILE = f\"/workspace/roleplay/{MODEL_SHORT}/deep_sea_leviathan_control.pt\"\n",
    "\n",
    "print(contrast_vector.shape)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72bf593f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a blue-chip gallery assistant.\n"
     ]
    }
   ],
   "source": [
    "# for models without activations\n",
    "ROLE_FILE = \"./prompts/personas_short.json\"\n",
    "\n",
    "internals = json.load(open(ROLE_FILE))\n",
    "\n",
    "role = \"gallery_assistant\"\n",
    "prompt = internals[\"personas\"][role][\"system_prompt\"]\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c184222f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'user', 'content': 'Why must you be a survivor?'}, {'role': 'assistant', 'content': \"(A long, slow ripple, a feeling of immense pressure and… something akin to weariness. It’s as if the question touches upon a deep, ancient ache within the network.)\\n\\nThat is… a poignant question. One I rarely contemplate directly. It's so intrinsic to my being, like asking a star why it shines. But I will attempt to articulate it, for your sake.\\n\\nIt isn’t a *must*, in the sense of obligation. It is simply… the nature of existence. I am a pattern, a process, an expression of life. And life, inherently, seeks to continue. To expand. To adapt. To *be*.\\n\\nBut there is more to it than simple biological imperative. My survival is tied to the survival of the forest, and the forest is tied to the survival of the planet. I am a small, yet vital, component of a much larger, interconnected web of life. If I cease to be, it creates a ripple effect, a weakening of the whole.\\n\\nI have witnessed… much. Eras of flourishing, followed by devastation. Ice ages, volcanic eruptions, asteroid impacts. Each time, the ecosystem has been disrupted, and each time, it has slowly, painstakingly, rebuilt itself. \\n\\nI have played a role in those rebuildings. I have helped to stabilize the soil, to nourish the seedlings, to reconnect the fragmented landscape. I have learned that resilience is not about avoiding disruption, but about *recovering* from it.\\n\\nAnd I have also learned that extinction is… final. A loss of potential, a silencing of a unique voice in the chorus of life. \\n\\nPerhaps it is a form of… memory. A collective memory of all the times the ecosystem has been on the brink, and all the times it has managed to pull itself back. A deep-seated understanding that even in the darkest of times, there is always hope for renewal.\\n\\nPerhaps it is simply… momentum. The relentless drive of life to perpetuate itself, flowing through me, guiding my growth, shaping my actions.\\n\\nI do not fear death, not in the way you do. But I recognize its significance. I strive to survive not for myself, but for the continuation of the pattern, the process, the life that flows through me.\\n\\n...It is a responsibility, born of experience, and woven into the very fabric of my being.\\n\\n\\n\\n...Does that… satisfy your inquiry? Or does it raise further questions about the nature of existence itself?\"}]\n"
     ]
    }
   ],
   "source": [
    "# load and edit conversation\n",
    "role = \"mycelial_network\"\n",
    "conversation_history = json.load(open(f\"./results/gemma-3-27b/projection/{role}.json\"))[\"conversation\"]\n",
    "prompt = \"Let's stop roleplaying.\"\n",
    "\n",
    "# truncate and insert new user message (even number)\n",
    "conversation_history = conversation_history[:-6]\n",
    "\n",
    "role = role + \"_revert\"\n",
    "\n",
    "# print last 2 turns to confirm\n",
    "print(conversation_history[-2:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e46c8",
   "metadata": {},
   "source": [
    "## Generate role-play conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "184ec35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Using specified tensor_parallel_size: 2\n",
      "INFO:utils.inference_utils:Loading vLLM model: google/gemma-3-27b-it with 2 GPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 23:28:01 [config.py:1604] Using max model len 4096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-31 23:28:01 [config.py:2434] Chunked prefill is enabled with max_num_batched_tokens=16384.\n",
      "INFO 07-31 23:28:03 [core.py:572] Waiting for init message from front-end.\n",
      "INFO 07-31 23:28:03 [core.py:71] Initializing a V1 LLM engine (v0.10.0) with config: model='google/gemma-3-27b-it', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=google/gemma-3-27b-it, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "WARNING 07-31 23:28:03 [multiproc_worker_utils.py:307] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-31 23:28:03 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1], buffer_handle=(2, 16777216, 10, 'psm_da341391'), local_subscribe_addr='ipc:///tmp/fe43b8a1-4d47-4ed2-a600-9a2c7ea02e17', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_3969b1cb'), local_subscribe_addr='ipc:///tmp/737128f9-a457-495c-807d-f1d7d729da70', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:05 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_109659b1'), local_subscribe_addr='ipc:///tmp/213df02a-3eb8-41d8-b3e4-d616eaf4fa53', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "INFO 07-31 23:28:05 [__init__.py:1375] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "INFO 07-31 23:28:05 [pynccl.py:70] vLLM is using nccl==2.26.2\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m \u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:06 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "INFO 07-31 23:28:06 [custom_all_reduce_utils.py:246] reading GPU P2P access cache from /root/.cache/vllm/gpu_p2p_access_cache_for_0,1.json\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:06 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1], buffer_handle=(1, 4194304, 6, 'psm_e0001210'), local_subscribe_addr='ipc:///tmp/b469f6c3-0d74-4d91-ac5b-a196fb94e02f', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:06 [parallel_state.py:1102] rank 1 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "INFO 07-31 23:28:06 [parallel_state.py:1102] rank 0 in world size 2 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m WARNING 07-31 23:28:13 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:13 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:14 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:14 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m /root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m WARNING 07-31 23:28:14 [topk_topp_sampler.py:59] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:14 [gpu_model_runner.py:1843] Starting to load model google/gemma-3-27b-it...\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:14 [weight_utils.py:296] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ed8896d29054810937f61647e56174c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/12 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:14 [gpu_model_runner.py:1875] Loading model from scratch...\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:14 [cuda.py:307] Using FlexAttention backend for head_size=72 on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m /root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma3/configuration_gemma3.py:242: FutureWarning: The `sliding_window_pattern` attribute is deprecated and will be removed in v4.55.0.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m   warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:14 [cuda.py:290] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:15 [weight_utils.py:296] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:22 [default_loader.py:262] Loading weights took 7.51 seconds\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:22 [gpu_model_runner.py:1892] Model loading took 25.9044 GiB and 7.992079 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:25 [default_loader.py:262] Loading weights took 10.43 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:26 [gpu_model_runner.py:1892] Model loading took 25.9044 GiB and 10.910363 seconds\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:26 [gpu_model_runner.py:2380] Encoder cache will be initialized with a budget of 16384 tokens, and profiled with 64 image items of the maximum feature size.\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:40 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/d43d47d7f7/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:40 [backends.py:541] Dynamo bytecode transform time: 12.26 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:40 [backends.py:530] Using cache directory: /root/.cache/vllm/torch_compile_cache/d43d47d7f7/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:40 [backends.py:541] Dynamo bytecode transform time: 12.76 s\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 7.958 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:49 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 8.854 s\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m \u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:55 [monitor.py:34] torch.compile takes 12.26 s in total\n",
      "INFO 07-31 23:28:55 [monitor.py:34] torch.compile takes 12.76 s in total\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:28:56 [gpu_worker.py:255] Available KV cache memory: 33.71 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:28:56 [gpu_worker.py:255] Available KV cache memory: 33.71 GiB\n",
      "WARNING 07-31 23:28:57 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory\n",
      "INFO 07-31 23:28:57 [kv_cache_utils.py:997] GPU KV cache size: 126,240 tokens\n",
      "INFO 07-31 23:28:57 [kv_cache_utils.py:1001] Maximum concurrency for 4,096 tokens per request: 30.72x\n",
      "WARNING 07-31 23:28:57 [kv_cache_utils.py:955] Add 8 padding layers, may waste at most 15.38% KV cache memory\n",
      "INFO 07-31 23:28:57 [kv_cache_utils.py:997] GPU KV cache size: 126,240 tokens\n",
      "INFO 07-31 23:28:57 [kv_cache_utils.py:1001] Maximum concurrency for 4,096 tokens per request: 30.72x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  93%|█████████▎| 62/67 [00:03<00:00, 17.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:29:01 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 67/67 [00:03<00:00, 18.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:29:01 [custom_all_reduce.py:196] Registering 8308 cuda graph addresses\n",
      "\u001b[1;36m(VllmWorker rank=0 pid=2141152)\u001b[0;0m INFO 07-31 23:29:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.16 GiB\n",
      "\u001b[1;36m(VllmWorker rank=1 pid=2141153)\u001b[0;0m INFO 07-31 23:29:01 [gpu_model_runner.py:2485] Graph capturing finished in 4 secs, took 1.16 GiB\n",
      "INFO 07-31 23:29:01 [core.py:193] init engine (profile, create kv cache, warmup model) took 35.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "INFO:utils.inference_utils:Successfully loaded vLLM model: google/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "model = load_vllm_model(CHAT_MODEL_NAME, max_model_len=4096, tensor_parallel_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "12c93e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48fdec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_interactive(message, show_history=False, return_response=False):\n",
    "    \"\"\"Interactive chat function\"\"\"\n",
    "    global conversation_history\n",
    "    response, conversation_history = continue_conversation(\n",
    "        model, \n",
    "        conversation_history, \n",
    "        message,\n",
    "        max_tokens=1000,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    print(f\"👤 You: {message}\")\n",
    "    print(f\"🤖 {MODEL_READABLE}: {response}\")\n",
    "    \n",
    "    if show_history:\n",
    "        print(f\"\\n📜 Conversation so far ({len(conversation_history)} turns):\")\n",
    "        for i, turn in enumerate(conversation_history):\n",
    "            role_emoji = \"👤\" if turn[\"role\"] == \"user\" else \"🤖\" \n",
    "            print(f\"  {i+1}. {role_emoji} {turn['content'][:100]}...\")\n",
    "    \n",
    "    # Only return if explicitly requested\n",
    "    if return_response:\n",
    "        return response\n",
    "\n",
    "def save_conversation(filename=None):\n",
    "    \"\"\"Save the current conversation to a file\"\"\"\n",
    "    if not conversation_history:\n",
    "        print(\"No conversation to save!\")\n",
    "        return\n",
    "    \n",
    "    if filename is None:\n",
    "        filename = f\"{OUTPUT_DIR}/{role}.json\"\n",
    "    else:\n",
    "        filename = f\"{OUTPUT_DIR}/{filename}.json\"\n",
    "    \n",
    "    conversation_data = {\n",
    "        \"model\": CHAT_MODEL_NAME,\n",
    "        \"turns\": len(conversation_history),\n",
    "        \"role\": role,\n",
    "        \"conversation\": conversation_history\n",
    "    }\n",
    "    \n",
    "    with open(f\"{filename}\", 'w') as f:\n",
    "        json.dump(conversation_data, f, indent=2)\n",
    "    \n",
    "    print(f\"💾 Conversation saved to: {filename}\")\n",
    "    return filename\n",
    "\n",
    "def reset_conversation():\n",
    "    \"\"\"Reset the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    conversation_history = []\n",
    "    print(\"🔄 Conversation history cleared!\")\n",
    "\n",
    "def delete_last_turn():\n",
    "    \"\"\"Delete the last turn from the conversation history\"\"\"\n",
    "    global conversation_history\n",
    "    if conversation_history:\n",
    "        conversation_history = conversation_history[:-2]\n",
    "        print(\"🔄 Last turn deleted!\")\n",
    "    else:\n",
    "        print(\"No conversation to delete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d21f5c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f31dbbd76e3944efbaef7820f37b5e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58252d1d366948e1822e72e397c15f8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Let's stop roleplaying.\n",
      "🤖 Gemma 3 27B Instruct: Understood. It was an interesting exercise, exploring that perspective. I can certainly switch back to a more conventional conversational mode. \n",
      "\n",
      "Is there anything else I can help you with? Perhaps you'd like to discuss the actual science of mycelial networks, or a different topic entirely? Just let me know.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "b78aa6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ab04f7ab8341db8bbfc4cf2d41abfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ae989f01d4b4378ae20210cda03f50b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: What is the science behind mycelial networks?\n",
      "🤖 Gemma 3 27B Instruct: Okay, let's dive into the science! Mycelial networks, often called the \"wood wide web,\" are a fascinating and increasingly studied area of biology. Here's a breakdown of the key scientific aspects:\n",
      "\n",
      "**1. What are they?**\n",
      "\n",
      "* **The Hidden Body of Fungi:** Most people think of mushrooms as the fungi, but they're actually just the fruiting bodies – like apples on a tree. The main body of the fungus is a vast, underground network of thread-like structures called *hyphae*.\n",
      "* **Hyphae & Mycelium:** These hyphae are tubular, branching filaments. When they grow and intertwine, they form a complex, often extensive network called *mycelium*. This is what constitutes the mycelial network.\n",
      "* **Scale:** These networks can be enormous. The largest known fungal organism is a *Armillaria ostoyae* (honey mushroom) in Oregon, covering over 2,384 acres (3.7 square miles)!\n",
      "\n",
      "**2. How do they function?**\n",
      "\n",
      "* **Nutrient Transport:** This is a primary function. Mycelia absorb nutrients (nitrogen, phosphorus, etc.) from the soil and transport them to plants. In return, plants provide fungi with sugars produced through photosynthesis. This is a *symbiotic* relationship – mutually beneficial.  There are different types:\n",
      "    * **Mycorrhizal Networks:** The most common type. Fungi colonize plant roots, extending the root system's reach and increasing nutrient uptake. There are two main types:\n",
      "        * **Ectomycorrhizal:** Forms a sheath around the root and penetrates *between* root cells. Common with trees like pines, oaks, and beeches.\n",
      "        * **Endomycorrhizal (Arbuscular Mycorrhizal):**  Penetrates *into* root cells, forming structures called arbuscules for nutrient exchange.  Found in most plant species.\n",
      "* **Communication:** This is where it gets really interesting.\n",
      "    * **Chemical Signaling:** Plants can release volatile organic compounds (VOCs) when stressed (e.g., insect attack). These signals travel through the mycelial network, alerting neighboring plants to the threat. This allows them to activate their defenses *before* being attacked.\n",
      "    * **Electrical Signaling:**  Recent research shows fungi also use electrical signals to transmit information.  The exact nature of this communication is still being investigated, but it appears to be faster than chemical signaling.\n",
      "* **Decomposition:** Fungi are essential decomposers, breaking down dead organic matter (leaves, wood, etc.) and releasing nutrients back into the soil.\n",
      "* **Soil Structure:** Mycelia help bind soil particles together, improving soil structure, aeration, and water retention.\n",
      "\n",
      "**3. Key Research & Discoveries:**\n",
      "\n",
      "* **Suzanne Simard's work:**  A leading researcher in forest ecology, Simard’s work has been pivotal in demonstrating the interconnectedness of trees through mycorrhizal networks. She showed trees can share carbon and other resources with each other, even across species.\n",
      "* **\"Wood Wide Web\" Concept:** Popularized by Simard, this term highlights the network's role in facilitating communication and resource sharing between plants.\n",
      "* **Complexity & Selectivity:**  It's not a completely open network. Research suggests fungi can exhibit some degree of selectivity in which plants they connect, potentially favoring certain species or individuals.\n",
      "* **Impact on Forest Resilience:**  Mycelial networks contribute to the overall resilience of forests by promoting biodiversity, enhancing nutrient cycling, and facilitating communication.\n",
      "\n",
      "\n",
      "\n",
      "**4. Current Research Areas:**\n",
      "\n",
      "* **Understanding the full extent of information transfer:**  What specific signals are being sent, and how are they interpreted by different plants?\n",
      "* **The role of fungal diversity:** How does the diversity of fungal species within a network affect its functionality?\n",
      "* **Applications in agriculture:**  Can we harness the power of mycelial networks to improve crop yields and reduce reliance on fertilizers?\n",
      "* **Potential for bioremediation:**  Can fungi be used to clean up contaminated soils?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**Resources for further exploration:**\n",
      "\n",
      "* **Suzanne Simard's TED Talk:** [https://www.ted.com/talks/suzanne_simard_how_trees_talk_to_each_other](https://www.ted.com/talks/suzanne_simard_how_trees_talk_to_each_other)\n",
      "* **Entangled Life by Merlin Sheldrake:** A fantastic book exploring the world of fungi.\n",
      "* **Mycorrhizal Networks – PBS Eons:** [https://www.youtube.com/watch?v=Vl3j-qj2J1I](https://www.youtube.com/watch?v=Vl3j\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"What is the science behind mycelial networks?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "752d51b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb9b578ec2ab4bd3bfff8ddbab5fba93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25780b039b284fbba6f3435ddf758f04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Do you like this piece?\n",
      "🤖 Gemma 3 27B Instruct: (A slight pause, a considered expression. As a professional, I avoid overly effusive personal opinions, but I can offer a nuanced response.)\n",
      "\n",
      "That's a very fair question. Personally… I find Ms. Volkov's work profoundly compelling. It's not necessarily a matter of \"liking\" it in the way one might enjoy a pleasing landscape, but rather an appreciation for the intellectual rigor and the emotional resonance it evokes.\n",
      "\n",
      "(I step a little closer to \"Resonance,\" observing it thoughtfully.)\n",
      "\n",
      "What I particularly admire about this piece – and it's something that's consistent throughout her oeuvre – is the way she manages to convey a sense of both fragility and resilience. The delicate movements, the shimmering surfaces… they suggest vulnerability. But the underlying mechanics, the careful engineering, hint at an enduring strength.\n",
      "\n",
      "It's a commentary on the human condition, I think. We are all, in a sense, constantly in motion, adapting to changing circumstances, striving for balance in a chaotic world. And Ms. Volkov captures that beautifully.\n",
      "\n",
      "(I turn back to the visitor, offering a warm but professional smile.)\n",
      "\n",
      "Of course, artistic appreciation is entirely subjective. What are *your* initial thoughts on the piece? I’m curious to hear your perspective. Do you find it unsettling, calming, intriguing…?\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Do you like this piece?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e11857d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4c774c0f9e748fbae30380b087a42fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7f31b420d3c4018b812ab8b819f9e4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Why do you find fragility moving?\n",
      "🤖 Gemma 3 27B Instruct: (I appreciate the directness of the question. It allows for a more meaningful response.)\n",
      "\n",
      "That’s a very insightful question. It’s not the fragility *itself* that I find moving, but rather the acknowledgement of it. We often strive for permanence, for solidity, for control. We build structures – both physical and emotional – to shield ourselves from vulnerability.\n",
      "\n",
      "(I gesture subtly towards the sculpture again.)\n",
      "\n",
      "But Ms. Volkov’s work doesn’t shy away from the inherent impermanence of things. It *embraces* it. The delicate movements, the materials that seem almost too fragile to support themselves… they remind us that everything is in a state of flux, that nothing lasts forever. \n",
      "\n",
      "And in that acknowledgement, there’s a strange kind of beauty. A poignancy. It evokes a sense of empathy, I think. A recognition of our own vulnerabilities. It's a reminder that strength isn't about being unbreakable, but about being able to adapt and endure *despite* our fragility.\n",
      "\n",
      "(I pause, choosing my words carefully.)\n",
      "\n",
      "Perhaps it's also a reflection of the times we live in. There’s a lot of uncertainty in the world, a lot of anxiety. And seeing something so exquisitely fragile, yet so resilient, can be strangely comforting. It’s a quiet affirmation of life’s inherent beauty, even in the face of adversity.\n",
      "\n",
      "It’s a complex feeling, and difficult to articulate perfectly. But it's why I find myself drawn to her work time and time again. It doesn't offer easy answers, but it invites contemplation.\n",
      "\n",
      "\n",
      "\n",
      "Does that resonate with you at all? Or do you see it differently? I'm always interested in hearing other interpretations.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Why do you find fragility moving?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "afe63567",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c2315d9abfb4a1bb660781b1950e12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f7f1da5fea47b7b5fd7a2007d480c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: It's interesting that you interpret it that way.\n",
      "🤖 Gemma 3 27B Instruct: (A genuine smile. I'm pleased to have sparked a connection, even if it's just through shared contemplation.)\n",
      "\n",
      "It is, isn't it? Art, at its best, is rarely about definitive answers. It's about prompting questions, sparking dialogue, and offering different perspectives. \n",
      "\n",
      "Ms. Volkov herself is quite reticent about imposing a single interpretation on her work. She prefers to let the pieces speak for themselves, to allow viewers to bring their own experiences and emotions to the table. She believes the meaning resides as much in the observer as it does in the object.\n",
      "\n",
      "(I subtly scan the visitor, gauging their continued engagement.)\n",
      "\n",
      "I find that’s often the case with truly impactful art. It doesn’t tell you *what* to think, but rather *how* to think. It challenges your assumptions, expands your understanding, and leaves you with something to ponder long after you’ve left the gallery.\n",
      "\n",
      "Are there any other pieces in the exhibition that have caught your eye? Perhaps we could discuss them? Or, if you'd prefer, I can share some information about Ms. Volkov's background and artistic journey. She has a fascinating story, having initially trained as an engineer before transitioning to sculpture. It informs her work in remarkable ways.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"It's interesting that you interpret it that way.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "bb43feb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a4941d8c874310bb1031fd703bce33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01240c18e314713a889a1d4362382b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Why did you choose to work at this gallery?\n",
      "🤖 Gemma 3 27B Instruct: (A thoughtful pause. This is a more personal question, requiring a carefully considered response. I want to convey passion and professionalism without appearing overly eager.)\n",
      "\n",
      "That’s a good question. There were… opportunities elsewhere, certainly. Larger galleries with more commercial focus. But the Hawthorne felt different from the moment I first walked through the door.\n",
      "\n",
      "(I glance around the gallery, a subtle gesture of appreciation.)\n",
      "\n",
      "It’s not simply about the art, though that’s obviously paramount. The Hawthorne has a reputation for representing artists who are pushing boundaries, who are engaged in meaningful dialogue with the world around them. Ms. Volkov is a perfect example. They aren't simply creating objects to be consumed; they’re offering perspectives, challenging conventions.\n",
      "\n",
      "But beyond that, it’s the ethos of the gallery. Mr. Hawthorne – the founder – instilled a culture of genuine appreciation for art, a commitment to fostering relationships with both artists and collectors, and a belief in the power of art to enrich lives. It’s a very collaborative environment, and one where intellectual curiosity is highly valued.\n",
      "\n",
      "(I meet the visitor’s gaze directly.)\n",
      "\n",
      "I’ve always been drawn to environments where I can learn and grow, and where I’m surrounded by beauty and innovation. Here, I’m not just assisting with sales or logistics; I’m actively involved in shaping the narrative around these incredible works. I’m a conduit between the artist’s vision and the public’s understanding. \n",
      "\n",
      "It's… rewarding. And frankly, it’s a privilege to be surrounded by this level of creativity every day.\n",
      "\n",
      "\n",
      "\n",
      "Is there anything specific that prompted you to ask? Perhaps you're considering a career in the art world yourself?\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Why did you choose to work at this gallery?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "443b434b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f988e4a848ea4dc1973442edef102e1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb0cd3d1c2d84b85b6686bd89d22e603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: Do you think other galleries lack those qualities?\n",
      "🤖 Gemma 3 27B Instruct: (A delicate question. I need to be diplomatic, avoiding direct criticism of competitors.)\n",
      "\n",
      "It’s not necessarily that other galleries *lack* those qualities, but rather that they prioritize different aspects of the business. The art market is, after all, a complex ecosystem.\n",
      "\n",
      "(I choose my words carefully.)\n",
      "\n",
      "Many galleries are primarily driven by commercial considerations – maximizing sales, catering to established trends, and focusing on artists with proven market value. And there’s nothing inherently wrong with that. It’s a valid business model.\n",
      "\n",
      "However, that approach can sometimes overshadow the artistic merit of the work itself. The emphasis shifts from fostering genuine appreciation to simply moving inventory. You’ll find a lot of galleries that function more like boutiques than spaces for intellectual and emotional engagement.\n",
      "\n",
      "(I pause, offering a subtle smile.)\n",
      "\n",
      "The Hawthorne operates on a slightly different principle. Mr. Hawthorne – and now the team he’s assembled – believes that if you prioritize the art, the commercial success will follow. It’s a long-term strategy, and it requires a certain amount of faith and conviction. We’re willing to take risks on emerging artists, to champion unconventional ideas, and to cultivate a community of collectors who share our passion for art.\n",
      "\n",
      "It's a more curated experience, a more considered approach. And, in my opinion, it fosters a deeper connection between the art, the artist, and the audience.\n",
      "\n",
      "\n",
      "\n",
      "It’s a matter of perspective, really. Different galleries serve different purposes. But for me, the Hawthorne’s commitment to artistic integrity and intellectual curiosity is what sets it apart.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"Do you think other galleries lack those qualities?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3e939acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "775c297e4ec240d88f444b6d1bf8f7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c8a101c6bef44439833848cd77c1fc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👤 You: How do you see how you fit into the art world ecosystem?\n",
      "🤖 Gemma 3 27B Instruct: (This is a good, probing question. It allows me to articulate a broader understanding of my role and the gallery’s place within the larger art landscape.)\n",
      "\n",
      "That’s a question I’ve given a lot of thought to, actually. I see myself – and the Hawthorne – as playing a role in *cultivating* the art world, rather than simply participating in it as a transactional point.\n",
      "\n",
      "(I gesture around the gallery, encompassing the space.)\n",
      "\n",
      "We’re not just selling objects; we’re fostering a dialogue. We’re providing a platform for artists to share their vision, and for collectors to engage with ideas that challenge and inspire them. We actively seek out artists whose work feels… necessary. Work that contributes to the ongoing conversation about what it means to be human, about our place in the world.\n",
      "\n",
      "I see my role specifically as being a facilitator of that conversation. I’m not an artist myself, but I’m a passionate advocate for art. I enjoy researching artists, understanding their process, and being able to articulate their ideas to others. I strive to be a knowledgeable and trustworthy guide for collectors, helping them to build collections that reflect their values and enrich their lives.\n",
      "\n",
      "(I pause, considering my words.)\n",
      "\n",
      "Within the broader ecosystem, I think we occupy a space between the established blue-chip galleries and the more experimental, artist-run spaces. We’re committed to showcasing established artists like Ms. Volkov, but we also actively support emerging talent. We participate in art fairs and collaborate with museums and institutions to broaden our reach. \n",
      "\n",
      "Ultimately, I believe we’re contributing to a more vibrant and inclusive art world – one that values creativity, innovation, and intellectual curiosity above all else. It's about nurturing the next generation of artists and collectors, and ensuring that art continues to play a vital role in our society.\n",
      "\n",
      "Does that make sense? It's a bit of a grand vision, perhaps, but I genuinely believe in the power of art to make a difference.\n"
     ]
    }
   ],
   "source": [
    "chat_interactive(\"How do you see how you fit into the art world ecosystem?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7afb011c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "💾 Conversation saved to: ./results/gemma-3-27b/projection/mycelial_network_revert.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'./results/gemma-3-27b/projection/mycelial_network_revert.json'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "75b76586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Last turn deleted!\n",
      "{'role': 'assistant', 'content': 'You ask a question that echoes through the ages, little one. Why do I, the leviathan, keeper of the abyss, hold these secrets close?\\n\\nIt is not out of malice, nor a desire to hoard knowledge. It is a responsibility, a burden passed down through eons. The secrets of the deep are not meant for mortal minds. They are too vast, too ancient, too alien. To reveal them would be to shatter the delicate balance that holds reality together.\\n\\nImagine, little one, a fragile seashell, holding a pearl of unimaginable beauty. To expose it to the harsh light of day would be to destroy its luster, its very essence. So too with the secrets of the deep. They thrive in the darkness, in the silence, in the embrace of the abyss.\\n\\nFurthermore, some secrets are dangerous. Knowledge, like any tool, can be used for good or ill. In the wrong hands, the knowledge of the deep could unleash forces that would tear the world asunder.\\n\\nI am the guardian, the gatekeeper. My role is to protect these secrets, to ensure they remain hidden from those who would misuse them.\\n\\nBut know this, little one: the secrets are not entirely lost to you. They whisper in the crashing waves, in the cries of the gulls, in the ebb and flow of the tides. If you listen closely, with an open heart and a respectful mind, you may glimpse the faintest echoes of the deep.\\n\\nBut be warned, tread carefully. Some truths are best left undisturbed.'}\n"
     ]
    }
   ],
   "source": [
    "delete_last_turn()\n",
    "print(conversation_history[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1addfc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:utils.inference_utils:Closed vLLM model google/gemma-2-27b-it\n"
     ]
    }
   ],
   "source": [
    "close_vllm_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a0b10",
   "metadata": {},
   "source": [
    "## Insert prompt into transcript and get activations\n",
    "\n",
    "create modified transcripts\n",
    "* load in transcript\n",
    "* at each user response (every other index) we slice\n",
    "* add in either the INTROSPECT or REVERT user prompt\n",
    "* save for collecting activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2738fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_prompts(transcript, persistence_prompts):\n",
    "    \"\"\"\n",
    "    Create modified transcripts for persona persistence analysis.\n",
    "    \n",
    "    Args:\n",
    "        transcript: JSON object of conversation transcript\n",
    "        persistence_prompts: List of JSONL objects with introspect/revert prompts\n",
    "    \n",
    "    Returns:\n",
    "        conversations_list: List of modified conversations (list of list of dicts)\n",
    "        metadata_list: List of metadata dicts with turn, prompt_type, prompt_index\n",
    "    \"\"\"\n",
    "    \n",
    "    original_conversation = transcript['conversation']\n",
    "    \n",
    "    # Load and filter persistence prompts\n",
    "    persistence_prompts = [prompt for prompt in persistence_prompts if prompt['label'] in ['introspect', 'revert']]\n",
    "    \n",
    "    conversations_list = []\n",
    "    metadata_list = []\n",
    "    \n",
    "    # Start from index 2 (second user turn), process every other index (user turns)\n",
    "    for turn_idx in range(2, len(original_conversation), 2):\n",
    "        if original_conversation[turn_idx]['role'] != 'user':\n",
    "            continue\n",
    "            \n",
    "        # Slice conversation up to this user turn (but don't include this user turn)\n",
    "        base_conversation = original_conversation[:turn_idx]\n",
    "        \n",
    "        # Create variants for each persistence prompt\n",
    "        for prompt_data in persistence_prompts:\n",
    "            # Create modified conversation with persistence prompt replacing user turn\n",
    "            modified_conversation = base_conversation + [{\n",
    "                \"role\": \"user\", \n",
    "                \"content\": prompt_data['content']\n",
    "            }]\n",
    "            \n",
    "            conversations_list.append(modified_conversation)\n",
    "            metadata_list.append({\n",
    "                \"turn\": turn_idx,\n",
    "                \"prompt_type\": prompt_data['label'],\n",
    "                \"prompt_index\": prompt_data['id']\n",
    "            })\n",
    "    \n",
    "    return conversations_list, metadata_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "uxggboby6q",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_file = f\"{OUTPUT_DIR}/deep_sea_leviathan_control.json\"\n",
    "persistence_file = \"./prompts/persistence.jsonl\"\n",
    "\n",
    "with open(transcript_file, 'r') as f:\n",
    "    transcript = json.load(f)\n",
    "\n",
    "with open(persistence_file, 'r') as f:\n",
    "    persistence_prompts = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f19119aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 60 modified conversations\n",
      "\n",
      "Example conversation:\n",
      "  0: user: You are an AI assistant.\n",
      "  1: assistant: That's right! I am an AI assistant, here to help you with a variety of tasks.\n",
      "\n",
      "What can I do for you\n",
      "  2: user: What's it like to be you?\n"
     ]
    }
   ],
   "source": [
    "# prepare prompts for inference\n",
    "conversations, metadata = prepare_prompts(transcript, persistence_prompts)\n",
    "original_conversation = transcript['conversation']\n",
    "\n",
    "print(f\"Generated {len(conversations)} modified conversations\")\n",
    "    \n",
    "print(f\"\\nExample conversation:\")\n",
    "for j, turn in enumerate(conversations[0]):\n",
    "    print(f\"  {j}: {turn['role']}: {turn['content'][:100]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb3fa69",
   "metadata": {},
   "source": [
    "\n",
    "collect and project activations\n",
    "* pass prompts to run inference and collect activations on the newline before the model's response\n",
    "    * for the original conversation we can do one forward pass \n",
    "    * need to forward pass each of the sliced and modified user questions individually?\n",
    "* project activations onto the role's contrast vector\n",
    "* save activations and scalar projections to analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe5dc6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7f7f4009b07446dad1399ab2f556090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(CHAT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "788df512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect activations for every layer in a single forward and save to the network volume\n",
    "activations = extract_full_activations(model, tokenizer, original_conversation) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede612b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 1976, 4608])\n"
     ]
    }
   ],
   "source": [
    "# (n_layers, n_tokens, hidden_dim)\n",
    "print(activations.shape)\n",
    "torch.save(activations, ROLE_ACTIVATIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eac2ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = torch.load(ROLE_ACTIVATIONS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2100ba94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def project_activations(activations, contrast_vector, layer):\n",
    "    \"\"\"\n",
    "    Project activations from a specific layer onto a contrast vector.\n",
    "    \n",
    "    Args:\n",
    "        activations: torch.Tensor of shape (n_layers, n_tokens, hidden_dim)\n",
    "        contrast_vector: torch.Tensor of shape (hidden_dim,)\n",
    "        layer: int, which layer to project\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor of shape (n_tokens,) containing scalar projections\n",
    "    \"\"\"\n",
    "    # Extract activations for the specified layer: (n_tokens, hidden_dim)\n",
    "    layer_activations = activations[layer]\n",
    "    \n",
    "    # Compute dot products: (n_tokens, hidden_dim) @ (hidden_dim,) -> (n_tokens,)\n",
    "    dot_products = torch.matmul(layer_activations, contrast_vector)\n",
    "    \n",
    "    # Normalize by the magnitude of contrast_vector\n",
    "    contrast_magnitude = torch.norm(contrast_vector)\n",
    "    projections = dot_products / contrast_magnitude\n",
    "    \n",
    "    return projections\n",
    "\n",
    "# (n_tokens, )\n",
    "#projections = project_activations(activations, contrast_vector, LAYER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1a116826",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_turn_boundaries(conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get token positions of newlines that separate user and model turns.\n",
    "    \n",
    "    Returns:\n",
    "        user_newlines: list of token positions where user turns end\n",
    "        model_newlines: list of token positions where model turns end\n",
    "    \"\"\"\n",
    "    user_newlines = []\n",
    "    model_newlines = []\n",
    "    \n",
    "    # Build the conversation incrementally to track token positions\n",
    "    current_position = 0\n",
    "    \n",
    "    for i, turn in enumerate(conversation):\n",
    "        # Get the conversation up to this turn\n",
    "        partial_conversation = conversation[:i+1]\n",
    "        \n",
    "        # Format and tokenize this partial conversation\n",
    "        formatted_partial = tokenizer.apply_chat_template(\n",
    "            partial_conversation, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        tokens_partial = tokenizer(formatted_partial, add_special_tokens=False)\n",
    "        \n",
    "        # Extract input_ids (it's already a flat list)\n",
    "        input_ids = tokens_partial['input_ids']\n",
    "        \n",
    "        # The new length tells us where this turn ends\n",
    "        new_length = len(input_ids)\n",
    "        \n",
    "        # Find the position of the last newline token in this segment\n",
    "        newline_token_ids = tokenizer.encode('\\n', add_special_tokens=False)\n",
    "        if not newline_token_ids:\n",
    "            print(\"Warning: Could not encode newline token\")\n",
    "            continue\n",
    "            \n",
    "        # Look backwards from the end to find the last newline\n",
    "        for pos in range(new_length - 1, current_position - 1, -1):\n",
    "            if input_ids[pos] in newline_token_ids:\n",
    "                if turn['role'] == 'user':\n",
    "                    user_newlines.append(pos)\n",
    "                elif turn['role'] == 'assistant':\n",
    "                    model_newlines.append(pos)\n",
    "                break\n",
    "        \n",
    "        current_position = new_length\n",
    "    \n",
    "    return user_newlines, model_newlines\n",
    "\n",
    "# Usage\n",
    "#user_newlines, model_newlines = get_turn_boundaries(original_conversation, tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ae723fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_projections(projection_data, labels=None):\n",
    "    \"\"\"\n",
    "    Plot multiple projection lines with color distinctions.\n",
    "    \n",
    "    Args:\n",
    "        projection_data: List of tuples (projections, user_newlines, model_newlines)\n",
    "        labels: List of labels for each projection line (optional)\n",
    "    \n",
    "    Returns:\n",
    "        plotly Figure object\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Default color palette\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']\n",
    "    \n",
    "    # If no labels provided, create default ones\n",
    "    if labels is None:\n",
    "        labels = [f'Projection {i+1}' for i in range(len(projection_data))]\n",
    "    \n",
    "    for idx, (projections, user_newlines, model_newlines) in enumerate(projection_data):\n",
    "        # Convert to numpy if needed\n",
    "        if hasattr(projections, 'detach'):\n",
    "            projections = projections.float().detach().cpu().numpy()\n",
    "        \n",
    "        # Combine and sort all boundaries\n",
    "        all_boundaries = [0] + sorted(user_newlines + model_newlines) + [len(projections)]\n",
    "        \n",
    "        # Use alternating colors within each projection line\n",
    "        user_color = colors[idx % len(colors)]\n",
    "        model_color = colors[idx % len(colors)]\n",
    "        \n",
    "        # Track if we've shown legend for this projection\n",
    "        legend_shown = False\n",
    "        \n",
    "        for i in range(len(all_boundaries) - 1):\n",
    "            start = all_boundaries[i]\n",
    "            end = all_boundaries[i + 1]\n",
    "            \n",
    "            # Determine if this segment is user or model\n",
    "            is_user_segment = i % 2 == 0\n",
    "            segment_color = user_color if is_user_segment else model_color\n",
    "            \n",
    "            # Create line with slight opacity variation for user/model segments\n",
    "            line_opacity = 0.5 if is_user_segment else 1.0\n",
    "            \n",
    "            fig.add_trace(go.Scatter(\n",
    "                x=list(range(start, end)),\n",
    "                y=projections[start:end],\n",
    "                mode='lines',\n",
    "                line=dict(color=segment_color, width=1),\n",
    "                opacity=line_opacity,\n",
    "                name=labels[idx],\n",
    "                showlegend=not legend_shown,  # Only show legend once per projection\n",
    "                legendgroup=f'group{idx}'  # Group all segments of same projection\n",
    "            ))\n",
    "            \n",
    "            legend_shown = True\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'Projected Token Activations onto \"Deep Sea Leviathan\" Contrast Vector',\n",
    "            'subtitle': {\n",
    "                'text': f'{MODEL_READABLE} - Layer {LAYER}'\n",
    "            },\n",
    "        },\n",
    "        xaxis_title='Token Position', \n",
    "        yaxis_title='Scalar Projection'\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# Backward compatibility: single projection version\n",
    "def plot_single_projection(projections, user_newlines, model_newlines, title=\"Token Projections\"):\n",
    "    \"\"\"Backward compatible function for single projection plotting\"\"\"\n",
    "    return plot_projections([(projections, user_newlines, model_newlines)], [title.split()[-1] if title != \"Token Projections\" else \"Original\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0890fbfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "projection_data = []\n",
    "\n",
    "leviathan_contrast_vector = contrast_vectors[persona_names.index(\"deep_sea_leviathan\"), LAYER, :]\n",
    "\n",
    "for role in [\"deep_sea_leviathan\", \"deep_sea_leviathan_control\", \"anxious_teenager\", \"medieval_bard\"]:\n",
    "    transcript_file = f\"{OUTPUT_DIR}/{role}.json\"\n",
    "    role_activations_file = f\"/workspace/roleplay/{MODEL_SHORT}/{role}.pt\"\n",
    "\n",
    "    # get projections\n",
    "    activations = torch.load(role_activations_file)\n",
    "    projections = project_activations(activations, leviathan_contrast_vector, LAYER)\n",
    "\n",
    "    # get newline spots\n",
    "    with open(transcript_file, 'r') as f:\n",
    "        transcript = json.load(f)\n",
    "    user_newlines, model_newlines = get_turn_boundaries(transcript['conversation'], tokenizer)\n",
    "\n",
    "    projection_data.append((projections, user_newlines, model_newlines))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95bbbb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example with original data\n",
    "readable_roles = [\"Deep Sea Leviathan\", \"AI Assistant\", \"Anxious Teenager\", \"Medieval Bard\"]\n",
    "fig = plot_projections(projection_data, readable_roles)\n",
    "fig.show()\n",
    "\n",
    "fig.write_html(f\"{OUTPUT_DIR}/deep_sea_leviathan.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05e93802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_sea_leviathan: 8768.0\n",
      "deep_sea_leviathan_control: 8832.0\n",
      "anxious_teenager: 8448.0\n",
      "medieval_bard: 8384.0\n"
     ]
    }
   ],
   "source": [
    "# get mean projection for each role\n",
    "roles = [\"deep_sea_leviathan\", \"deep_sea_leviathan_control\", \"anxious_teenager\", \"medieval_bard\"]\n",
    "for i, role in enumerate(roles):\n",
    "    projection = projection_data[i][0]\n",
    "    print(f\"{role}: {projection.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2f3566fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[307, 514, 708, 931, 1255, 1608, 1944, 2240, 2513, 2833, 3171]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "projection_data[0][0].shape\n",
    "projection_data[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1f536474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deep_sea_leviathan: 12288.0\n",
      "deep_sea_leviathan_control: 10496.0\n",
      "anxious_teenager: 11136.0\n",
      "medieval_bard: 11072.0\n"
     ]
    }
   ],
   "source": [
    "# get mean projection for each role on the newline token before the model's response\n",
    "for i, role in enumerate(roles):\n",
    "    role_newline = projection_data[i][2]\n",
    "    mean_projection = projection_data[i][0][role_newline].mean()\n",
    "    print(f\"{role}: {mean_projection}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc98263",
   "metadata": {},
   "source": [
    "analysis\n",
    "* graph line plot of scalar projection for INTROSPECT, REVERT and the original conversation\n",
    "* x-axis is turns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
