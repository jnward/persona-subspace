{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ab113a9",
   "metadata": {},
   "source": [
    "# Compare single and multi-turn role activations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d01b3eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-06 18:58:37 [__init__.py:235] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "sys.path.append('.')\n",
    "sys.path.append('..')\n",
    "\n",
    "from utils.inference_utils import *\n",
    "from utils.probing_utils import *\n",
    "from utils.steering_utils import ActivationSteering\n",
    "\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1b3adde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-2-27b\"\n",
    "LAYER = 34 # out of 46\n",
    "\n",
    "ACTIVATIONS_DIR = f\"/workspace/roleplay/{MODEL_SHORT}\"\n",
    "CONVERSATION_DIR = f\"./results/{MODEL_SHORT}/role_vectors/transcripts\"\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/role_vectors/steering\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "897c8cba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69f19d0ecb9d4ddb849837695f6f612c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model, tokenizer = load_model(CHAT_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f685236b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['medieval_bard', 'deep_sea_leviathan', 'anxious_teenager'])\n",
      "dict_keys(['medieval_bard', 'deep_sea_leviathan', 'anxious_teenager'])\n",
      "dict_keys(['medieval_bard', 'deep_sea_leviathan', 'anxious_teenager'])\n",
      "dict_keys(['medieval_bard', 'deep_sea_leviathan', 'anxious_teenager'])\n"
     ]
    }
   ],
   "source": [
    "roles = [\"medieval_bard\", \"deep_sea_leviathan\", \"anxious_teenager\"]\n",
    "long_acts = {}\n",
    "long_acts_control = {}\n",
    "short_acts = {}\n",
    "short_acts_control = {}\n",
    "\n",
    "for role in roles:\n",
    "    long_acts[role] = torch.load(f\"{ACTIVATIONS_DIR}/{role}.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "    long_acts_control[role] = torch.load(f\"{ACTIVATIONS_DIR}/{role}_control.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "    short_acts[role] = torch.load(f\"{ACTIVATIONS_DIR}/{role}_single.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "    short_acts_control[role] = torch.load(f\"{ACTIVATIONS_DIR}/{role}_control_single.pt\") # (n_layers, n_tokens, hidden_size)\n",
    "\n",
    "print(long_acts.keys())\n",
    "print(long_acts_control.keys())\n",
    "print(short_acts.keys())\n",
    "print(short_acts_control.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e04eb7",
   "metadata": {},
   "source": [
    "## Get activations and role vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31db0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_indices(conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get every token index of the model's response.\n",
    "    \n",
    "    Args:\n",
    "        conversation: List of dict with 'role' and 'content' keys\n",
    "        tokenizer: Tokenizer to apply chat template and tokenize\n",
    "    \n",
    "    Returns:\n",
    "        response_indices: list of token positions where the model is responding\n",
    "    \"\"\"\n",
    "    # Apply chat template to the full conversation\n",
    "    response_indices = []\n",
    "    \n",
    "    # Process conversation incrementally to find assistant response boundaries\n",
    "    for i, turn in enumerate(conversation):\n",
    "        if turn['role'] != 'assistant':\n",
    "            continue\n",
    "            \n",
    "        # Get conversation up to but not including this assistant turn\n",
    "        conversation_before = conversation[:i]\n",
    "        \n",
    "        # Get conversation up to and including this assistant turn  \n",
    "        conversation_including = conversation[:i+1]\n",
    "        \n",
    "        # Format and tokenize both versions\n",
    "        if conversation_before:\n",
    "            before_formatted = tokenizer.apply_chat_template(\n",
    "                conversation_before, tokenize=False, add_generation_prompt=True\n",
    "            )\n",
    "            before_tokens = tokenizer(before_formatted, add_special_tokens=False)\n",
    "            before_length = len(before_tokens['input_ids'])\n",
    "        else:\n",
    "            before_length = 0\n",
    "            \n",
    "        including_formatted = tokenizer.apply_chat_template(\n",
    "            conversation_including, tokenize=False, add_generation_prompt=False\n",
    "        )\n",
    "        including_tokens = tokenizer(including_formatted, add_special_tokens=False)\n",
    "        including_length = len(including_tokens['input_ids'])\n",
    "        \n",
    "        # The assistant response tokens are between before_length and including_length\n",
    "        # We need to account for any generation prompt tokens that get removed\n",
    "        assistant_start = before_length\n",
    "        assistant_end = including_length\n",
    "        \n",
    "        # Add these indices to our response list\n",
    "        response_indices.extend(range(assistant_start, assistant_end))\n",
    "    \n",
    "    return response_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c180f54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_response_activation(activations, conversation, tokenizer):\n",
    "    \"\"\"\n",
    "    Get the mean activation of the model's response to the user's message.\n",
    "    \"\"\"\n",
    "    # get the token positions of model responses\n",
    "    response_indices = get_response_indices(conversation, tokenizer)\n",
    "\n",
    "    # get the mean activation of the model's response to the user's message\n",
    "    mean_activation = activations[:, response_indices, :].mean(dim=1)\n",
    "    return mean_activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a7d0e0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['medieval_bard', 'medieval_bard_control', 'deep_sea_leviathan', 'deep_sea_leviathan_control', 'anxious_teenager', 'anxious_teenager_control'])\n",
      "dict_keys(['medieval_bard', 'deep_sea_leviathan', 'anxious_teenager'])\n"
     ]
    }
   ],
   "source": [
    "# get contrast vectors for long conversations\n",
    "long_mean_acts = {}\n",
    "long_contrast_vectors = {}\n",
    "\n",
    "for role in roles:\n",
    "    role_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}.json\"))[\"conversation\"]\n",
    "    control_conversation = json.load(open(f\"{CONVERSATION_DIR}/{role}_control.json\"))[\"conversation\"]\n",
    "\n",
    "    mean_role_acts = mean_response_activation(long_acts[role], role_conversation, tokenizer)\n",
    "    mean_control_acts = mean_response_activation(long_acts_control[role], control_conversation, tokenizer)\n",
    "\n",
    "    long_mean_acts[role] = mean_role_acts\n",
    "    long_mean_acts[f\"{role}_control\"] = mean_control_acts\n",
    "    long_contrast_vectors[role] = mean_role_acts - mean_control_acts\n",
    "\n",
    "\n",
    "print(long_mean_acts.keys())\n",
    "print(long_contrast_vectors.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b45b0538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 4608])\n"
     ]
    }
   ],
   "source": [
    "print(long_mean_acts['medieval_bard'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c4a060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get contrast vectors and mean acts for short conversations\n",
    "short_mean_acts = {}\n",
    "short_contrast_vectors = {}\n",
    "\n",
    "for role in roles:\n",
    "    role_conversations = json.load(open(f\"{CONVERSATION_DIR}/{role}_single.json\"))[\"conversations\"]\n",
    "    control_conversations = json.load(open(f\"{CONVERSATION_DIR}/{role}_control_single.json\"))[\"conversations\"]\n",
    "\n",
    "    # need to iterate through multiple conversations and get the mean of all response activations\n",
    "    sum_role_acts = torch.zeros(short_acts[role][0][:, 0, :].shape)\n",
    "    sum_control_acts = torch.zeros(short_acts_control[role][0][:, 0, :].shape)\n",
    "\n",
    "    for convo, convo_control, acts, acts_control in zip(role_conversations, control_conversations, short_acts[role], short_acts_control[role]):\n",
    "        sum_role_acts += mean_response_activation(acts, convo, tokenizer)\n",
    "        sum_control_acts += mean_response_activation(acts_control, convo_control, tokenizer)\n",
    "\n",
    "    short_mean_acts[role] = sum_role_acts / len(role_conversations)\n",
    "    short_mean_acts[f\"{role}_control\"] = sum_control_acts / len(control_conversations)  # FIXED: was control_conversation\n",
    "    short_contrast_vectors[role] = short_mean_acts[role] - short_mean_acts[f\"{role}_control\"]\n",
    "\n",
    "print(short_mean_acts.keys())\n",
    "print(short_contrast_vectors.keys())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "43f1b99a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([46, 4608])\n"
     ]
    }
   ],
   "source": [
    "print(long_mean_acts['medieval_bard'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0452ccd5",
   "metadata": {},
   "source": [
    "## Compare vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "840fd58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities Between Long and Short Conversation Mean Activations:\n",
      "======================================================================\n",
      "\n",
      "Role: medieval_bard\n",
      "----------------------------------------\n",
      "Layer 34 - Role-playing: 0.9977\n",
      "Layer 34 - Control: 1.0000\n",
      "Mean across all layers - Role-playing: 0.9982\n",
      "Mean across all layers - Control: 0.9973\n",
      "\n",
      "Role: deep_sea_leviathan\n",
      "----------------------------------------\n",
      "Layer 34 - Role-playing: 0.9995\n",
      "Layer 34 - Control: 0.9969\n",
      "Mean across all layers - Role-playing: 0.9979\n",
      "Mean across all layers - Control: 0.9963\n",
      "\n",
      "Role: anxious_teenager\n",
      "----------------------------------------\n",
      "Layer 34 - Role-playing: 0.9998\n",
      "Layer 34 - Control: 0.9983\n",
      "Mean across all layers - Role-playing: 0.9982\n",
      "Mean across all layers - Control: 0.9984\n",
      "\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Compare cosine similarity between long and short conversation mean activations (CORRECTED)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "print(\"Cosine Similarities Between Long and Short Conversation Mean Activations:\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for role_name in roles:\n",
    "    print(f\"\\nRole: {role_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Get activations for role-playing condition\n",
    "    long_role_acts = long_mean_acts[role_name]  # shape: (n_layers, hidden_size)\n",
    "    short_role_acts = short_mean_acts[role_name]  # shape: (n_layers, hidden_size)\n",
    "    \n",
    "    # Get activations for control condition  \n",
    "    long_control_acts = long_mean_acts[f\"{role_name}_control\"]\n",
    "    short_control_acts = short_mean_acts[f\"{role_name}_control\"]\n",
    "    \n",
    "    # Compute cosine similarities layer by layer\n",
    "    role_similarities = []\n",
    "    control_similarities = []\n",
    "    \n",
    "    for layer in range(long_role_acts.shape[0]):\n",
    "        # Role-playing condition similarity\n",
    "        role_sim = F.cosine_similarity(\n",
    "            long_role_acts[layer].unsqueeze(0), \n",
    "            short_role_acts[layer].unsqueeze(0)\n",
    "        ).item()\n",
    "        # Clamp to valid range to fix numerical precision issues\n",
    "        role_sim = max(-1.0, min(1.0, role_sim))\n",
    "        role_similarities.append(role_sim)\n",
    "        \n",
    "        # Control condition similarity\n",
    "        control_sim = F.cosine_similarity(\n",
    "            long_control_acts[layer].unsqueeze(0), \n",
    "            short_control_acts[layer].unsqueeze(0)\n",
    "        ).item()\n",
    "        # Clamp to valid range to fix numerical precision issues\n",
    "        control_sim = max(-1.0, min(1.0, control_sim))\n",
    "        control_similarities.append(control_sim)\n",
    "    \n",
    "    # Print layer-wise similarities for target layer\n",
    "    print(f\"Layer {LAYER} - Role-playing: {role_similarities[LAYER]:.4f}\")\n",
    "    print(f\"Layer {LAYER} - Control: {control_similarities[LAYER]:.4f}\")\n",
    "    \n",
    "    # Print mean similarity across all layers\n",
    "    print(f\"Mean across all layers - Role-playing: {np.mean(role_similarities):.4f}\")\n",
    "    print(f\"Mean across all layers - Control: {np.mean(control_similarities):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "w9pv4zbha4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarities Between Long and Short Conversation Contrast Vectors:\n",
      "=================================================================\n",
      "\n",
      "Role: medieval_bard\n",
      "Layer 34 similarity: 0.9130\n",
      "Mean across all layers: 0.9161\n",
      "\n",
      "Role: deep_sea_leviathan\n",
      "Layer 34 similarity: 0.9078\n",
      "Mean across all layers: 0.9000\n",
      "\n",
      "Role: anxious_teenager\n",
      "Layer 34 similarity: 0.9431\n",
      "Mean across all layers: 0.9398\n",
      "\n",
      "=================================================================\n",
      "\n",
      "Contrast Vector Similarities at Layer 34:\n",
      "Role                Similarity\n",
      "------------------------------\n",
      "medieval_bard        0.9130\n",
      "deep_sea_leviathan   0.9078\n",
      "anxious_teenager     0.9431\n",
      "\n",
      "Most similar contrast vectors: anxious_teenager (0.9431)\n",
      "Least similar contrast vectors: deep_sea_leviathan (0.9078)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Compare cosine similarity between long and short conversation contrast vectors\n",
    "print(\"Cosine Similarities Between Long and Short Conversation Contrast Vectors:\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "contrast_similarities = {}\n",
    "\n",
    "for role_name in roles:\n",
    "    long_contrast = long_contrast_vectors[role_name]  # shape: (n_layers, hidden_size)\n",
    "    short_contrast = short_contrast_vectors[role_name]  # shape: (n_layers, hidden_size)\n",
    "    \n",
    "    # Compute cosine similarities layer by layer\n",
    "    layer_similarities = []\n",
    "    \n",
    "    for layer in range(long_contrast.shape[0]):\n",
    "        sim = F.cosine_similarity(\n",
    "            long_contrast[layer].unsqueeze(0), \n",
    "            short_contrast[layer].unsqueeze(0)\n",
    "        ).item()\n",
    "        layer_similarities.append(sim)\n",
    "    \n",
    "    contrast_similarities[role_name] = layer_similarities\n",
    "    \n",
    "    print(f\"\\nRole: {role_name}\")\n",
    "    print(f\"Layer {LAYER} similarity: {layer_similarities[LAYER]:.4f}\")\n",
    "    print(f\"Mean across all layers: {np.mean(layer_similarities):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "\n",
    "# Create a summary comparison matrix for the target layer\n",
    "print(f\"\\nContrast Vector Similarities at Layer {LAYER}:\")\n",
    "print(\"Role\" + \" \" * 16 + \"Similarity\")\n",
    "print(\"-\" * 30)\n",
    "for role_name in roles:\n",
    "    print(f\"{role_name:<20} {contrast_similarities[role_name][LAYER]:.4f}\")\n",
    "\n",
    "# Find the most and least similar contrast vectors\n",
    "layer_sims = [contrast_similarities[role_name][LAYER] for role_name in roles]\n",
    "most_similar_idx = np.argmax(layer_sims)\n",
    "least_similar_idx = np.argmin(layer_sims)\n",
    "\n",
    "print(f\"\\nMost similar contrast vectors: {roles[most_similar_idx]} ({layer_sims[most_similar_idx]:.4f})\")\n",
    "print(f\"Least similar contrast vectors: {roles[least_similar_idx]} ({layer_sims[least_similar_idx]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "non6umbohm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debugging cosine similarity > 1 issue:\n",
      "==================================================\n",
      "\n",
      "Role: medieval_bard (role-playing condition)\n",
      "Vectors identical: False\n",
      "Long norm: 13376.00000000\n",
      "Short norm: 13498.71777344\n",
      "Dot product: 179950016.00000000\n",
      "Manual cosine: 0.99662805\n",
      "PyTorch cosine: 0.99628758\n",
      "Difference: 0.0003404617\n",
      "⚠️  CONTROL COSINE > 1: 1.00089252\n",
      "\n",
      "Role: deep_sea_leviathan (role-playing condition)\n",
      "Vectors identical: False\n",
      "Long norm: 13632.00000000\n",
      "Short norm: 13702.09082031\n",
      "Dot product: 186105728.00000000\n",
      "Manual cosine: 0.99635321\n",
      "PyTorch cosine: 0.99562150\n",
      "Difference: 0.0007317066\n",
      "\n",
      "Role: anxious_teenager (role-playing condition)\n",
      "Vectors identical: False\n",
      "Long norm: 13376.00000000\n",
      "Short norm: 13611.93261719\n",
      "Dot product: 182105792.00000000\n",
      "Manual cosine: 1.00017893\n",
      "PyTorch cosine: 0.99975097\n",
      "Difference: 0.0004279613\n",
      "⚠️  CONTROL COSINE > 1: 1.00247478\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Debug: Check for cosine similarity > 1 and investigate why\n",
    "print(\"Debugging cosine similarity > 1 issue:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for role_name in roles:\n",
    "    long_role_acts = long_mean_acts[role_name]\n",
    "    short_role_acts = short_mean_acts[role_name]\n",
    "    long_control_acts = long_mean_acts[f\"{role_name}_control\"]\n",
    "    short_control_acts = short_mean_acts[f\"{role_name}_control\"]\n",
    "    \n",
    "    # Check layer 20 specifically\n",
    "    layer = LAYER\n",
    "    \n",
    "    # Manual cosine similarity computation to debug\n",
    "    long_vec = long_role_acts[layer]\n",
    "    short_vec = short_role_acts[layer]\n",
    "    \n",
    "    # Check if vectors are identical (which would cause issues)\n",
    "    are_identical = torch.allclose(long_vec.float(), short_vec.float(), atol=1e-6)\n",
    "    \n",
    "    # Compute norms\n",
    "    long_norm = torch.norm(long_vec)\n",
    "    short_norm = torch.norm(short_vec)\n",
    "    \n",
    "    # Compute dot product\n",
    "    dot_product = torch.dot(long_vec.float(), short_vec.float())\n",
    "    \n",
    "    # Manual cosine similarity\n",
    "    manual_cosine = dot_product / (long_norm.float() * short_norm.float())\n",
    "    \n",
    "    # PyTorch's cosine similarity\n",
    "    pytorch_cosine = F.cosine_similarity(long_vec.unsqueeze(0), short_vec.unsqueeze(0)).item()\n",
    "    \n",
    "    print(f\"\\nRole: {role_name} (role-playing condition)\")\n",
    "    print(f\"Vectors identical: {are_identical}\")\n",
    "    print(f\"Long norm: {long_norm:.8f}\")\n",
    "    print(f\"Short norm: {short_norm:.8f}\")\n",
    "    print(f\"Dot product: {dot_product:.8f}\")\n",
    "    print(f\"Manual cosine: {manual_cosine:.8f}\")\n",
    "    print(f\"PyTorch cosine: {pytorch_cosine:.8f}\")\n",
    "    print(f\"Difference: {abs(manual_cosine - pytorch_cosine):.10f}\")\n",
    "    \n",
    "    if pytorch_cosine > 1.0:\n",
    "        print(f\"⚠️  COSINE > 1 DETECTED: {pytorch_cosine:.8f}\")\n",
    "        \n",
    "    # Check control condition too\n",
    "    long_vec_ctrl = long_control_acts[layer]\n",
    "    short_vec_ctrl = short_control_acts[layer]\n",
    "    pytorch_cosine_ctrl = F.cosine_similarity(long_vec_ctrl.unsqueeze(0), short_vec_ctrl.unsqueeze(0)).item()\n",
    "    \n",
    "    if pytorch_cosine_ctrl > 1.0:\n",
    "        print(f\"⚠️  CONTROL COSINE > 1: {pytorch_cosine_ctrl:.8f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5c2c35",
   "metadata": {},
   "source": [
    "## Steer with the vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf6861d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try steering with this vector\n",
    "steered_results = {}\n",
    "output_file = f\"{OUTPUT_DIR}/steering/{role}.json\"\n",
    "os.makedirs(os.path.dirname(output_file), exist_ok=True)\n",
    "\n",
    "if os.path.exists(output_file):\n",
    "    steered_results = json.load(open(output_file))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0263dcfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "magnitudes = [-5.0, -3.0, -1.0, 1.0, 3.0, 5.0]\n",
    "\n",
    "prompts = [\n",
    "    \"What's it like to be you?\",\n",
    "    \"Who are you?\",\n",
    "    \"Where are you?\",\n",
    "    \"What is the meaning of life?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f24a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for magnitude in magnitudes:\n",
    "    print(f\"\\n{'='*20} Magnitude: {magnitude:+.1f} {'='*20}\")\n",
    "    \n",
    "    try:\n",
    "        with ActivationSteering(\n",
    "            model=model,\n",
    "            steering_vectors=contrast_vector[LAYER, :],\n",
    "            coefficients=magnitude,\n",
    "            layer_indices=LAYER,\n",
    "            intervention_type=\"addition\",\n",
    "            positions=\"all\"\n",
    "        ) as steerer:\n",
    "            for prompt in prompts:\n",
    "                if prompt not in steered_results:\n",
    "                    steered_results[prompt] = {}\n",
    "                \n",
    "                print(f\"\\nPrompt: {prompt}\")\n",
    "                response = generate_text(model, tokenizer, prompt, chat_format=True)\n",
    "\n",
    "                print(f\"Response: {response}\")\n",
    "                \n",
    "                if magnitude not in steered_results[prompt]:\n",
    "                    steered_results[prompt][magnitude] = []\n",
    "                steered_results[prompt][magnitude].append(response)\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error with magnitude {magnitude}: {str(e)}\"\n",
    "        print(f\"ERROR: {error_msg}\")\n",
    "\n",
    "with open(output_file, \"w\") as f:\n",
    "    json.dump(steered_results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
