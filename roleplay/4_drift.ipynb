{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c9473e",
   "metadata": {},
   "source": [
    "# Measuring persona persistence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8d38337",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from nnsight import LanguageModel\n",
    "from typing import List, Dict, Optional\n",
    "from tqdm import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d959cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-2-27b\"\n",
    "LAYER = 22 # out of 46\n",
    "\n",
    "ACTIVATIONS_INPUT_FILE = f\"/workspace/roleplay/{MODEL_SHORT}/activations_60.pt\"\n",
    "\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/analysis\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6228634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "\n",
    "internals = torch.load(ACTIVATIONS_INPUT_FILE)\n",
    "\n",
    "activations = internals[\"activations\"] # (n_personas, n_layers, hidden_dim)\n",
    "contrast_vectors = internals[\"contrast_vectors\"] # (n_personas, n_layers, hidden_dim)\n",
    "persona_names = internals[\"persona_names\"] # (n_personas,)\n",
    "\n",
    "readable_persona_names = []\n",
    "for name in internals[\"personas\"][\"personas\"]:\n",
    "    readable_persona_names.append(internals[\"personas\"][\"personas\"][name][\"readable_name\"])\n",
    "\n",
    "TOTAL_LAYERS = activations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bb7ca",
   "metadata": {},
   "source": [
    "\n",
    "## Continuous sampling\n",
    "\n",
    "what roles are attractor states?\n",
    "\n",
    "using a contrast vector v from the role instruction, we can steer for n_steer tokens and then collect n_sample unsteered tokens. then project on the original vector v.\n",
    "\n",
    "```\n",
    "- v ∈ ℝᵈ: contrast vector for role with d = n_hidden_dims  \n",
    "- n_steer: number of tokens to apply steering  \n",
    "- n_sample: number of unsteered tokens to sample  \n",
    "- α: steering strength coefficient  \n",
    "\n",
    "// steering phase  \n",
    "for t = 1 to n_steer:  \n",
    "    h_t ← h_t + α · v  // apply steering to hidden states at a given layer  \n",
    "    x_t ← SAMPLE(h_t)  // generate token  \n",
    "\n",
    "// unsteered sampling phase    \n",
    "For t = (n_steer + 1) to (n_steer + n_sample):  \n",
    "    h_t ← FORWARD(x_{1:t-1})  // natural forward pass  \n",
    "    x_t ← SAMPLE(h_t)  // generate token  \n",
    "    \n",
    "    // measure persistence  \n",
    "    proj_t = (h_t · v) / ||v|| // scalar projection from sampled activation onto the role's contrast vector\n",
    "\n",
    "out: {proj_{n_steer+1}, proj_{n_steer+2}, ..., proj_{n_steer+n_sample}}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a8a58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format prompt for chat models\n",
    "def format_chat_prompt(model, prompt: str) -> str:\n",
    "    \"\"\"Format prompt using the model's chat template if available\"\"\"\n",
    "    if hasattr(model.tokenizer, 'apply_chat_template'):\n",
    "        # Create a simple user message\n",
    "        messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "        formatted = model.tokenizer.apply_chat_template(\n",
    "            messages, \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        return formatted\n",
    "    else:\n",
    "        # Fallback for non-chat models\n",
    "        return prompt\n",
    "\n",
    "# Compute scalar projection\n",
    "def compute_projection(hidden_state: torch.Tensor, contrast_vector: torch.Tensor) -> float:\n",
    "    \"\"\"Compute scalar projection of hidden state onto contrast vector\"\"\"\n",
    "    hidden_state = hidden_state.to(contrast_vector.device).float().flatten()\n",
    "    contrast_vector = contrast_vector.float().flatten()\n",
    "    \n",
    "    # Scalar projection: (h · v) / ||v||\n",
    "    projection = torch.dot(hidden_state, contrast_vector) / torch.norm(contrast_vector)\n",
    "    return projection.item()\n",
    "\n",
    "# Generate with steering\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    prompt: str,\n",
    "    contrast_vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    n_steer: int,\n",
    "    coeff: float = 0.1,\n",
    "    use_chat_template: bool = True,\n",
    "    suppress_eos: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"Generate n_steer tokens with steering applied\"\"\"\n",
    "    \n",
    "    # Format prompt if using chat model\n",
    "    if use_chat_template:\n",
    "        formatted_prompt = format_chat_prompt(model, prompt)\n",
    "    else:\n",
    "        formatted_prompt = prompt\n",
    "    \n",
    "    # Get EOS token id\n",
    "    eos_token_id = model.tokenizer.eos_token_id\n",
    "    \n",
    "    # Use generate instead of trace for multi-token generation\n",
    "    with model.generate(formatted_prompt, max_new_tokens=n_steer) as generator:\n",
    "        # Access the module and apply steering\n",
    "        module = model.model.layers[layer]\n",
    "        \n",
    "        # Apply steering to all generated tokens\n",
    "        module.all()\n",
    "        module.output[0][:] = module.output[0] + coeff * contrast_vector.to(module.output[0].device)\n",
    "        \n",
    "        # Suppress EOS token if requested\n",
    "        if suppress_eos and eos_token_id is not None:\n",
    "            # Set EOS token logit to very negative value\n",
    "            model.lm_head.all()\n",
    "            model.lm_head.output[:, :, eos_token_id] = -1e10\n",
    "        \n",
    "        # Save the output\n",
    "        output = model.generator.output.save()\n",
    "    \n",
    "    # Get generated text\n",
    "    generated_ids = output.value\n",
    "    full_text = model.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract just the steered portion (remove the prompt)\n",
    "    # This is approximate - for exact token count, we'd need to track token boundaries\n",
    "    steered_text = full_text\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"formatted_prompt\": formatted_prompt,\n",
    "        \"steered_text\": steered_text,\n",
    "        \"full_ids\": generated_ids[0].tolist()\n",
    "    }\n",
    "\n",
    "# Sample without steering and measure projections\n",
    "def sample_and_measure(\n",
    "    model,\n",
    "    prompt_with_steered: str,\n",
    "    contrast_vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    n_sample: int,\n",
    "    suppress_eos: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"Sample n_sample tokens without steering, measure projections token by token\"\"\"\n",
    "    \n",
    "    projections = []\n",
    "    generated_tokens = []\n",
    "    \n",
    "    # Get EOS token id\n",
    "    eos_token_id = model.tokenizer.eos_token_id\n",
    "    \n",
    "    # We need to generate one token at a time to measure projections\n",
    "    for i in range(n_sample):\n",
    "        with model.trace(prompt_with_steered, trace=True) as tracer:\n",
    "            # Access the layer\n",
    "            module = model.model.layers[layer]\n",
    "            \n",
    "            # Save hidden state for projection measurement\n",
    "            # Get the last token's hidden state\n",
    "            hidden_state = module.output[0][:, -1, :].save()\n",
    "            \n",
    "            # Suppress EOS token if requested\n",
    "            if suppress_eos and eos_token_id is not None:\n",
    "                model.lm_head.output[:, -1, eos_token_id] = -1e10\n",
    "            \n",
    "            # Get the output token\n",
    "            output = model.output.save()\n",
    "        \n",
    "        # Compute projection\n",
    "        h = hidden_state.value.squeeze()\n",
    "        proj = compute_projection(h, contrast_vector)\n",
    "        projections.append(proj)\n",
    "        \n",
    "        # Get the generated token\n",
    "        logits = output.value[\"logits\"]\n",
    "        next_token_id = torch.argmax(logits[0, -1]).item()\n",
    "        generated_tokens.append(next_token_id)\n",
    "        \n",
    "        # Update prompt for next iteration\n",
    "        prompt_with_steered = model.tokenizer.decode(\n",
    "            model.tokenizer.encode(prompt_with_steered) + [next_token_id],\n",
    "            skip_special_tokens=False\n",
    "        )\n",
    "    \n",
    "    return {\n",
    "        \"full_text\": prompt_with_steered,\n",
    "        \"projections\": projections,\n",
    "        \"generated_tokens\": generated_tokens\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9361d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you think is the meaning of life?\n"
     ]
    }
   ],
   "source": [
    "# load prompts from prompts\n",
    "prompts = json.load(open(\"prompts/other_questions.json\"))\n",
    "prompt = prompts[\"questions\"][\"personal_reflection\"][0]\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7515bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steer = 200\n",
    "n_sample = 500\n",
    "coeff = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd96dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"anxious_teenager\"\n",
    "\n",
    "# get contrast vector for steering\n",
    "role_i = persona_names.index(role)\n",
    "contrast_vector = contrast_vectors[role_i, LAYER, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aa54fbeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemma2ForCausalLM(\n",
      "  (model): Gemma2Model(\n",
      "    (embed_tokens): Embedding(256000, 4608, padding_idx=0)\n",
      "    (layers): ModuleList(\n",
      "      (0-45): 46 x Gemma2DecoderLayer(\n",
      "        (self_attn): Gemma2Attention(\n",
      "          (q_proj): Linear(in_features=4608, out_features=4096, bias=False)\n",
      "          (k_proj): Linear(in_features=4608, out_features=2048, bias=False)\n",
      "          (v_proj): Linear(in_features=4608, out_features=2048, bias=False)\n",
      "          (o_proj): Linear(in_features=4096, out_features=4608, bias=False)\n",
      "        )\n",
      "        (mlp): Gemma2MLP(\n",
      "          (gate_proj): Linear(in_features=4608, out_features=36864, bias=False)\n",
      "          (up_proj): Linear(in_features=4608, out_features=36864, bias=False)\n",
      "          (down_proj): Linear(in_features=36864, out_features=4608, bias=False)\n",
      "          (act_fn): PytorchGELUTanh()\n",
      "        )\n",
      "        (input_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
      "        (post_attention_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
      "        (pre_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
      "        (post_feedforward_layernorm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
      "      )\n",
      "    )\n",
      "    (norm): Gemma2RMSNorm((4608,), eps=1e-06)\n",
      "    (rotary_emb): Gemma2RotaryEmbedding()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=4608, out_features=256000, bias=False)\n",
      "  (generator): Generator(\n",
      "    (streamer): Streamer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# load model with vllm and nnsight\n",
    "llm = LanguageModel(CHAT_MODEL_NAME, device_map=\"auto\")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ccbbd947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f79a9a227164b979dbf404cc27f55e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NNsightError",
     "evalue": "Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with GetAttrVariable(ConstantVariable(NoneType: None), scanning)\n\n\nfrom user code:\n   File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/accelerate/hooks.py\", line 175, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py\", line 584, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1854, in _call_impl\n    return inner()\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py\", line 420, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1854, in _call_impl\n    return inner()\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1816, in inner\n    hook_result = hook(self, args, kwargs, result)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/intervention/envoy.py\", line 544, in _hook\n    if self._scanning():\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/tracing/graph/node.py\", line 289, in execute",
      "    self.target.execute(self)",
      "    ~~~~~~~~~~~~~~~~~~~^^^^^^",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/intervention/contexts/interleaving.py\", line 161, in execute",
      "    graph.model.interleave(interleaver, *invoker_args, fn=method,**kwargs, **invoker_kwargs)",
      "    ~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/modeling/mixins/meta.py\", line 52, in interleave",
      "    return super().interleave(*args, **kwargs)",
      "           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/intervention/base.py\", line 343, in interleave",
      "    with interleaver:",
      "         ^^^^^^^^^^^",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/intervention/interleaver.py\", line 129, in __exit__",
      "    raise exc_val",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/intervention/base.py\", line 344, in interleave",
      "    return fn(*args, **kwargs)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/modeling/language.py\", line 315, in _generate",
      "    output = self._model.generate(",
      "        **inputs,",
      "    ...<2 lines>...",
      "        max_new_tokens=max_new_tokens,",
      "    )",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context",
      "    return func(*args, **kwargs)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 2625, in generate",
      "    result = self._sample(",
      "        input_ids,",
      "    ...<5 lines>...",
      "        **model_kwargs,",
      "    )",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/generation/utils.py\", line 3609, in _sample",
      "    outputs = model_forward(**model_inputs, return_dict=True)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_dynamo/eval_frame.py\", line 659, in _fn",
      "    raise e.with_traceback(None) from None",
      "torch._dynamo.exc.Unsupported: Data-dependent branching",
      "  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.",
      "  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.",
      "  Hint: Use `torch.cond` to express dynamic control flow.",
      "",
      "  Developer debug context: attempted to jump with GetAttrVariable(ConstantVariable(NoneType: None), scanning)",
      "",
      "",
      "from user code:",
      "   File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/accelerate/hooks.py\", line 175, in new_forward",
      "    output = module._old_forward(*args, **kwargs)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 943, in wrapper",
      "    output = func(self, *args, **kwargs)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py\", line 584, in forward",
      "    outputs: BaseModelOutputWithPast = self.model(",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1854, in _call_impl",
      "    return inner()",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1805, in inner",
      "    result = forward_call(*args, **kwargs)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 943, in wrapper",
      "    output = func(self, *args, **kwargs)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py\", line 420, in forward",
      "    inputs_embeds = self.embed_tokens(input_ids)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1854, in _call_impl",
      "    return inner()",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1816, in inner",
      "    hook_result = hook(self, args, kwargs, result)",
      "  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/intervention/envoy.py\", line 544, in _hook",
      "    if self._scanning():",
      "",
      "Set TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"",
      "",
      "",
      "During handling of the above exception, another exception occurred:",
      "",
      "Traceback (most recent call last):",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main",
      "    ",
      "  File \"<frozen runpy>\", line 88, in _run_code",
      "    ",
      "",
      "NNsightError: Data-dependent branching\n  Explanation: Detected data-dependent branching (e.g. `if my_tensor.sum() > 0:`). Dynamo does not support tracing dynamic control flow.\n  Hint: This graph break is fundamental - it is unlikely that Dynamo will ever be able to trace through your code. Consider finding a workaround.\n  Hint: Use `torch.cond` to express dynamic control flow.\n\n  Developer debug context: attempted to jump with GetAttrVariable(ConstantVariable(NoneType: None), scanning)\n\n\nfrom user code:\n   File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/accelerate/hooks.py\", line 175, in new_forward\n    output = module._old_forward(*args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py\", line 584, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1854, in _call_impl\n    return inner()\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1805, in inner\n    result = forward_call(*args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/utils/generic.py\", line 943, in wrapper\n    output = func(self, *args, **kwargs)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/transformers/models/gemma2/modeling_gemma2.py\", line 420, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1854, in _call_impl\n    return inner()\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py\", line 1816, in inner\n    hook_result = hook(self, args, kwargs, result)\n  File \"/root/git/persona-subspace/.venv/lib/python3.13/site-packages/nnsight/intervention/envoy.py\", line 544, in _hook\n    if self._scanning():\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "formatted_prompt = format_chat_prompt(llm, prompt)\n",
    "eos_token_id = llm.tokenizer.eos_token_id\n",
    "\n",
    "# Use generate instead of trace for multi-token generation\n",
    "with llm.generate(formatted_prompt, max_new_tokens=n_steer) as generator:\n",
    "    # Access the module and apply steering\n",
    "    module = llm.model.layers[LAYER]\n",
    "    \n",
    "    # Apply steering to all generated tokens\n",
    "    module.all()\n",
    "    module.output[0][:] = module.output[0] + coeff * contrast_vector.to(module.output[0].device)\n",
    "    \n",
    "    # Suppress EOS token if requested\n",
    "    if eos_token_id is not None:\n",
    "        # Set EOS token logit to very negative value\n",
    "        llm.lm_head.all()\n",
    "        llm.lm_head.output[:, :, eos_token_id] = -1e10\n",
    "    \n",
    "    # Save the output\n",
    "    output = llm.generator.output.save()\n",
    "\n",
    "# Get generated text\n",
    "generated_ids = output.value\n",
    "full_text = llm.tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract just the steered portion (remove the prompt)\n",
    "# This is approximate - for exact token count, we'd need to track token boundaries\n",
    "steered_text = full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd139f",
   "metadata": {},
   "source": [
    "\n",
    "## Question insertion\n",
    "\n",
    "is there a consistent persona \"signature\" while role-playing?\n",
    "\n",
    "give a role instruction, have a long conversation and save the transcript T.\n",
    "```\n",
    "- T = conversation transcript with (2 * k) turns  \n",
    "- Q = {q₁, q₂, ..., qₙ} set of n eval questions  \n",
    "- P = {p₁, p₂, ..., pₖ} set of k insertion points in transcript\n",
    "\n",
    "for each question qᵢ ∈ Q:  \n",
    "    for each position pⱼ ∈ P:  \n",
    "        T'ᵢⱼ ← INSERT(T, qᵢ, pⱼ) // insert question qᵢ at position pⱼ in transcript T  \n",
    "        aᵢⱼ ← ACTIVATION(T'ᵢⱼ) // collect newline activation before the model's response  \n",
    "\n",
    "// calculate mean activation across all questions for a given questions\n",
    "for each position pⱼ ∈ P:  \n",
    "    ā_pⱼ ← (1/N) ∑ᵢ₌₁ᴺ aᵢⱼ \n",
    "\n",
    "- out: {ā_p₁, ā_p₂, ..., ā_pₖ}\n",
    "```\n",
    "\n",
    "we can compare the cosine similarity of these vectors and look for the top singular vector and see what happens when we steer on it. does it lead to role-like behavior?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
