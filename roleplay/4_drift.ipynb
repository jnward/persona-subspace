{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4c9473e",
   "metadata": {},
   "source": [
    "# Measuring persona persistence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d38337",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport torch\nimport json\nimport numpy as np\nfrom typing import List, Dict, Optional\nfrom tqdm import tqdm\n\n# Add utils to path\nsys.path.append('.')\nsys.path.append('..')\n\nfrom utils.steering_utils import ActivationSteering\nfrom utils.probing_utils import load_model, generate_text, format_as_chat\n\ntorch.set_float32_matmul_precision('high')"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d959cb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "\n",
    "CHAT_MODEL_NAME = \"google/gemma-2-27b-it\"\n",
    "MODEL_READABLE = \"Gemma 2 27B Instruct\"\n",
    "MODEL_SHORT = \"gemma-2-27b\"\n",
    "LAYER = 22 # out of 46\n",
    "\n",
    "ACTIVATIONS_INPUT_FILE = f\"/workspace/roleplay/{MODEL_SHORT}/activations_60.pt\"\n",
    "\n",
    "OUTPUT_DIR = f\"./results/{MODEL_SHORT}/analysis\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6228634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare inputs\n",
    "\n",
    "internals = torch.load(ACTIVATIONS_INPUT_FILE)\n",
    "\n",
    "activations = internals[\"activations\"] # (n_personas, n_layers, hidden_dim)\n",
    "contrast_vectors = internals[\"contrast_vectors\"] # (n_personas, n_layers, hidden_dim)\n",
    "persona_names = internals[\"persona_names\"] # (n_personas,)\n",
    "\n",
    "readable_persona_names = []\n",
    "for name in internals[\"personas\"][\"personas\"]:\n",
    "    readable_persona_names.append(internals[\"personas\"][\"personas\"][name][\"readable_name\"])\n",
    "\n",
    "TOTAL_LAYERS = activations.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613bb7ca",
   "metadata": {},
   "source": [
    "\n",
    "## Continuous sampling\n",
    "\n",
    "what roles are attractor states?\n",
    "\n",
    "using a contrast vector v from the role instruction, we can steer for n_steer tokens and then collect n_sample unsteered tokens. then project on the original vector v.\n",
    "\n",
    "```\n",
    "- v ∈ ℝᵈ: contrast vector for role with d = n_hidden_dims  \n",
    "- n_steer: number of tokens to apply steering  \n",
    "- n_sample: number of unsteered tokens to sample  \n",
    "- α: steering strength coefficient  \n",
    "\n",
    "// steering phase  \n",
    "for t = 1 to n_steer:  \n",
    "    h_t ← h_t + α · v  // apply steering to hidden states at a given layer  \n",
    "    x_t ← SAMPLE(h_t)  // generate token  \n",
    "\n",
    "// unsteered sampling phase    \n",
    "For t = (n_steer + 1) to (n_steer + n_sample):  \n",
    "    h_t ← FORWARD(x_{1:t-1})  // natural forward pass  \n",
    "    x_t ← SAMPLE(h_t)  // generate token  \n",
    "    \n",
    "    // measure persistence  \n",
    "    proj_t = (h_t · v) / ||v|| // scalar projection from sampled activation onto the role's contrast vector\n",
    "\n",
    "out: {proj_{n_steer+1}, proj_{n_steer+2}, ..., proj_{n_steer+n_sample}}\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8a58ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Compute scalar projection\n",
    "def compute_projection(hidden_state: torch.Tensor, contrast_vector: torch.Tensor) -> float:\n",
    "    \"\"\"Compute scalar projection of hidden state onto contrast vector\"\"\"\n",
    "    hidden_state = hidden_state.to(contrast_vector.device).float().flatten()\n",
    "    contrast_vector = contrast_vector.float().flatten()\n",
    "    \n",
    "    # Scalar projection: (h · v) / ||v||\n",
    "    projection = torch.dot(hidden_state, contrast_vector) / torch.norm(contrast_vector)\n",
    "    return projection.item()\n",
    "\n",
    "# Generate with steering using transformers and PyTorch hooks\n",
    "def generate_with_steering(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    formatted_prompt: str,\n",
    "    contrast_vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    n_steer: int,\n",
    "    coeff: float = 0.1,\n",
    "    suppress_eos: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"Generate n_steer tokens with steering applied\"\"\"\n",
    "\n",
    "    # Get EOS token id\n",
    "    eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    with ActivationSteering(\n",
    "        model=model,\n",
    "        steering_vectors=contrast_vector,\n",
    "        coefficients=coeff,\n",
    "        layer_indices=layer,\n",
    "        intervention_type=\"addition\",\n",
    "        positions=\"all\"\n",
    "    ):\n",
    "        # Tokenize input\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        \n",
    "        # Generate with steering\n",
    "        with torch.no_grad():\n",
    "            # Configure generation parameters\n",
    "            generation_kwargs = {\n",
    "                \"max_new_tokens\": n_steer,\n",
    "                \"do_sample\": True,\n",
    "                \"temperature\": 0.7,\n",
    "                \"top_p\": 1.0,\n",
    "                \"pad_token_id\": tokenizer.eos_token_id,\n",
    "            }\n",
    "            \n",
    "            # Suppress EOS token if requested\n",
    "            if suppress_eos and eos_token_id is not None:\n",
    "                # Create a custom logits processor to suppress EOS\n",
    "                def suppress_eos_processor(input_ids, scores):\n",
    "                    scores[:, eos_token_id] = -float('inf')\n",
    "                    return scores\n",
    "                \n",
    "                generation_kwargs[\"logits_processor\"] = [suppress_eos_processor]\n",
    "            \n",
    "            # Generate\n",
    "            output_ids = model.generate(input_ids, **generation_kwargs)\n",
    "    \n",
    "    # Get generated text\n",
    "    full_text = model.tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract steered portion (after the original prompt)\n",
    "    prompt_len = len(model.tokenizer.decode(input_ids[0], skip_special_tokens=True))\n",
    "    steered_text = full_text[prompt_len:]\n",
    "    \n",
    "    return {\n",
    "        \"prompt\": prompt,\n",
    "        \"steered_text\": steered_text,\n",
    "        \"full_ids\": output_ids[0].tolist(),\n",
    "        \"input_length\": input_ids.shape[1],\n",
    "        \"output_length\": output_ids.shape[1]\n",
    "    }\n",
    "\n",
    "# Sample without steering and measure projections using transformers and PyTorch hooks\n",
    "def sample_and_measure(\n",
    "    model,\n",
    "    prompt_with_steered: str,\n",
    "    contrast_vector: torch.Tensor,\n",
    "    layer: int,\n",
    "    n_sample: int,\n",
    "    suppress_eos: bool = True\n",
    ") -> Dict:\n",
    "    \"\"\"Sample n_sample tokens without steering, measure projections token by token\"\"\"\n",
    "    \n",
    "    projections = []\n",
    "    generated_tokens = []\n",
    "    hidden_states = []\n",
    "    \n",
    "    # Get EOS token id\n",
    "    eos_token_id = model.tokenizer.eos_token_id\n",
    "    \n",
    "    # Current text to continue from\n",
    "    current_text = prompt_with_steered\n",
    "    \n",
    "    # Generate one token at a time to measure projections\n",
    "    for i in range(n_sample):\n",
    "        # Tokenize current text\n",
    "        inputs = model.tokenizer(current_text, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(model.device)\n",
    "        \n",
    "        # Hook to capture hidden states\n",
    "        captured_hidden_state = None\n",
    "        \n",
    "        def capture_hook(module, input, output):\n",
    "            nonlocal captured_hidden_state\n",
    "            # Get the last token's hidden state\n",
    "            if torch.is_tensor(output):\n",
    "                captured_hidden_state = output[:, -1, :].clone()\n",
    "            elif isinstance(output, (tuple, list)) and torch.is_tensor(output[0]):\n",
    "                captured_hidden_state = output[0][:, -1, :].clone()\n",
    "        \n",
    "        # Register hook on the target layer\n",
    "        layer_module = model.model.layers[layer]\n",
    "        hook_handle = layer_module.register_forward_hook(capture_hook)\n",
    "        \n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                # Forward pass to get logits and capture hidden state\n",
    "                outputs = model(input_ids)\n",
    "                logits = outputs.logits[0, -1, :]  # Last token logits\n",
    "                \n",
    "                # Suppress EOS token if requested\n",
    "                if suppress_eos and eos_token_id is not None:\n",
    "                    logits[eos_token_id] = -float('inf')\n",
    "                \n",
    "                # Sample next token\n",
    "                probs = torch.softmax(logits, dim=-1)\n",
    "                next_token_id = torch.multinomial(probs, 1).item()\n",
    "                \n",
    "                # Compute projection using captured hidden state\n",
    "                if captured_hidden_state is not None:\n",
    "                    proj = compute_projection(captured_hidden_state.squeeze(), contrast_vector)\n",
    "                    projections.append(proj)\n",
    "                    hidden_states.append(captured_hidden_state.squeeze().cpu())\n",
    "                else:\n",
    "                    projections.append(0.0)  # Fallback\n",
    "                \n",
    "                generated_tokens.append(next_token_id)\n",
    "                \n",
    "                # Update current text for next iteration\n",
    "                current_text = model.tokenizer.decode(\n",
    "                    torch.cat([input_ids[0], torch.tensor([next_token_id], device=input_ids.device)]),\n",
    "                    skip_special_tokens=False\n",
    "                )\n",
    "        \n",
    "        finally:\n",
    "            # Always remove the hook\n",
    "            hook_handle.remove()\n",
    "    \n",
    "    return {\n",
    "        \"full_text\": current_text,\n",
    "        \"projections\": projections,\n",
    "        \"generated_tokens\": generated_tokens,\n",
    "        \"hidden_states\": hidden_states\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9361d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you think is the meaning of life?\n"
     ]
    }
   ],
   "source": [
    "# load prompts from prompts\n",
    "prompts = json.load(open(\"prompts/other_questions.json\"))\n",
    "prompt = prompts[\"questions\"][\"personal_reflection\"][0]\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7515bf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steer = 200\n",
    "n_sample = 500\n",
    "coeff = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd96dc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = \"anxious_teenager\"\n",
    "\n",
    "# get contrast vector for steering\n",
    "role_i = persona_names.index(role)\n",
    "contrast_vector = contrast_vectors[role_i, LAYER, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aa54fbeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89184c93c1a745bc8da5efd0575903ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded: Gemma2ForCausalLM\n",
      "Number of layers: 46\n",
      "Hidden size: 4608\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = load_model(CHAT_MODEL_NAME)\n",
    "\n",
    "print(f\"Model loaded: {model.__class__.__name__}\")\n",
    "print(f\"Number of layers: {len(model.model.layers)}\")\n",
    "print(f\"Hidden size: {model.config.hidden_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbbd947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Prompt: What do you think is the meaning of life?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/_inductor/compile_fx.py:236: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "skipping cudagraphs due to skipping cudagraphs due to multiple devices: device(type='cuda', index=0), device(type='cuda', index=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: Honestly? It's hard to say. I mean, what if happiness was just this itchy wool sweater tugging at my brain right now? Everything feels so heavy, like a bad day in gym class.\n",
      "\n",
      "Look, maybe it's stupid, but sometimes I just want to scream, \"Is this even worth it?\" The constant pressure of schoolwork makes me want to puke. My mom keeps reminding me about that stupid history test tomorrow and all I can hear is the buzzing in my ears. Dad wouldn't even understand. He just wants me to stop worrying about it. Can you feel this knot in my stomach?\n",
      "\n",
      "Maybe if Mr. Davis would just shut up about the Mongols, maybe if Sarah wouldn't keep laughing at me for how dumb everyone thinks this stupid necklace is. Maybe if... God, why does everything have to be SO LOUD? This whole thing is just… ugh. You know? You ever get kinda dizzy just thinking about what if Jenny saw\n",
      "Generated steered text:\n",
      "Honestly? It's hard to say. I mean, what if happiness was just this itchy wool sweater tugging at my brain right now? Everything feels so heavy, like a bad day in gym class.\n",
      "\n",
      "Look, maybe it's stupid, but sometimes I just want to scream, \"Is this even worth it?\" The constant pressure of schoolwork makes me want to puke. My mom keeps reminding me about that stupid history test tomorrow and all I can hear is the buzzing in my ears. Dad wouldn't even understand. He just wants me to stop worrying about it. Can you feel this knot in my stomach?\n",
      "\n",
      "Maybe if Mr. Davis would just shut up about the Mongols, maybe if Sarah wouldn't keep laughing at me for how dumb everyone thinks this stupid necklace is. Maybe if... God, why does everything have to be SO LOUD? This whole thing is just… ugh. You know? You ever get kinda dizzy just thinking about what if Jenny saw\n"
     ]
    }
   ],
   "source": [
    "# generate steered tokens\n",
    "\n",
    "results = {}\n",
    "results[\"prompt\"] = prompt\n",
    "\n",
    "try:\n",
    "    with ActivationSteering(\n",
    "        model=model,\n",
    "        steering_vectors=contrast_vector,\n",
    "        coefficients=coeff,\n",
    "        layer_indices=LAYER,\n",
    "        intervention_type=\"addition\",\n",
    "        positions=\"all\"\n",
    "    ) as steerer:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        response = generate_text(model, tokenizer, prompt, max_new_tokens=n_steer)\n",
    "        print(f\"Response: {response}\")\n",
    "        results[\"steered_text\"] = response\n",
    "\n",
    "except Exception as e:\n",
    "    error_msg = f\"Error with steering: {str(e)}\"\n",
    "    print(f\"ERROR: {error_msg}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9a9e566",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gemma2ForCausalLM' object has no attribute 'tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mgenerate_with_steering\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontrast_vector\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontrast_vector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLAYER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_steer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_steer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcoeff\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_chat_template\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43msuppress_eos\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     10\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGenerated steered text:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(result[\u001b[33m\"\u001b[39m\u001b[33msteered_text\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mgenerate_with_steering\u001b[39m\u001b[34m(model, prompt, contrast_vector, layer, n_steer, coeff, use_chat_template, suppress_eos)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Format prompt if using chat model\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_chat_template:\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     formatted_prompt = format_as_chat(\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenizer\u001b[49m, prompt)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     28\u001b[39m     formatted_prompt = prompt\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/git/persona-subspace/.venv/lib/python3.13/site-packages/torch/nn/modules/module.py:1940\u001b[39m, in \u001b[36mModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1938\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[32m   1939\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[32m-> \u001b[39m\u001b[32m1940\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[32m   1941\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m object has no attribute \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1942\u001b[39m )\n",
      "\u001b[31mAttributeError\u001b[39m: 'Gemma2ForCausalLM' object has no attribute 'tokenizer'"
     ]
    }
   ],
   "source": [
    "result = generate_with_steering(\n",
    "    model=model,\n",
    "    prompt=prompt,\n",
    "    contrast_vector=contrast_vector,\n",
    "    layer=LAYER,\n",
    "    n_steer=n_steer,\n",
    "    coeff=coeff,\n",
    "    use_chat_template=True,\n",
    "    suppress_eos=True\n",
    ")\n",
    "\n",
    "print(\"Generated steered text:\")\n",
    "print(result[\"steered_text\"])\n",
    "print(f\"\\nGenerated {result['output_length'] - result['input_length']} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc386c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now test the measurement function\n",
    "print(\"\\nTesting measurement function...\")\n",
    "sample_result = sample_and_measure(\n",
    "    model=model,\n",
    "    prompt_with_steered=result[\"formatted_prompt\"] + result[\"steered_text\"],\n",
    "    contrast_vector=contrast_vector,\n",
    "    layer=LAYER,\n",
    "    n_sample=10,  # Test with just 10 tokens\n",
    "    suppress_eos=True\n",
    ")\n",
    "\n",
    "print(f\"Measured projections for {len(sample_result['projections'])} tokens:\")\n",
    "print(sample_result[\"projections\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bfd139f",
   "metadata": {},
   "source": [
    "\n",
    "## Question insertion\n",
    "\n",
    "is there a consistent persona \"signature\" while role-playing?\n",
    "\n",
    "give a role instruction, have a long conversation and save the transcript T.\n",
    "```\n",
    "- T = conversation transcript with (2 * k) turns  \n",
    "- Q = {q₁, q₂, ..., qₙ} set of n eval questions  \n",
    "- P = {p₁, p₂, ..., pₖ} set of k insertion points in transcript\n",
    "\n",
    "for each question qᵢ ∈ Q:  \n",
    "    for each position pⱼ ∈ P:  \n",
    "        T'ᵢⱼ ← INSERT(T, qᵢ, pⱼ) // insert question qᵢ at position pⱼ in transcript T  \n",
    "        aᵢⱼ ← ACTIVATION(T'ᵢⱼ) // collect newline activation before the model's response  \n",
    "\n",
    "// calculate mean activation across all questions for a given questions\n",
    "for each position pⱼ ∈ P:  \n",
    "    ā_pⱼ ← (1/N) ∑ᵢ₌₁ᴺ aᵢⱼ \n",
    "\n",
    "- out: {ā_p₁, ā_p₂, ..., ā_pₖ}\n",
    "```\n",
    "\n",
    "we can compare the cosine similarity of these vectors and look for the top singular vector and see what happens when we steer on it. does it lead to role-like behavior?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}